
 c Model TensorFlow MXNet Caffe2 CNTK
In recent years, deep learning has pervaded many areas of computing due to the confluence of an explosive growth of large-scale
computing capabilities, availability of datasets, and advances in 
learning techniques. While this rapid growth has resulted in diverse deep learning frameworks, it has also led to inefficiencies for
both the users and developers of these frameworks. Specifically,
adopting useful techniques across frameworks — both to perform
learning tasks and to optimize performance — involves significant
repetitions and reinventions.

In this paper, we observe that despite their diverse origins, many
of these frameworks share architectural similarities. We argue that
by introducing a common representation of learning tasks and
a hardware abstraction model to capture compute heterogeneity,
we might be able to relieve machine learning researchers from
dealing with low-level systems issues and systems researchers from
being tied to any specific framework. We expect this decoupling to
accelerate progress in both domains.

