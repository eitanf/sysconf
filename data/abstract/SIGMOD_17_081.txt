
As the use of machine learning (ML) permeates into diverse
application domains, there is an urgent need to support a
declarative framework for ML. Ideally, a user will specify an
ML task in a high-level and easy-to-use language and the
framework will invoke the appropriate algorithms and system configurations to execute it. An important observation
towards designing such a framework is that many ML tasks
can be expressed as mathematical optimization problems,
which take a specific form. Furthermore, these optimization problems can be efficiently solved using variations of
the gradient descent (GD) algorithm. Thus, to decouple a
user specification of an ML task from its execution, a key
component is a GD optimizer. We propose a cost-based GD
optimizer that selects the best GD plan for a given ML task.
To build our optimizer, we introduce a set of abstract operators for expressing GD algorithms and propose a novel approach to estimate the number of iterations a GD algorithm
requires to converge. Extensive experiments on real and synthetic datasets show that our optimizer not only chooses the
best GD plan but also allows for optimizations that achieve
orders of magnitude performance speed-up.
