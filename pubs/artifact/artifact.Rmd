---
title: The role of software availability in computer systems papers
preprint: false
author:
  - name: Eitan Frachtenberg
    corresponding: true
    email: eitan@reed.edu
affiliation:
    address: Department of Computer Science, Reed College
bibliography: ../sysconf.bib
abstract: > 
  Research in computer systems typically involves the engineering, implementation, and measurement of complex systems software. The availability of these software artifacts is crucial for reproducibility and replicability of the research's results, because system software often embodies numerous implicit assumptions and parameters that are not fully documented in the research article itself. Regrettably, it is still not a common practice for systems researchers to make their software artifacts fully available, which may be a factor in the low rate of studies that replicate or reproduce other systems studies.


  In this paper we  explore the effects of including the original software artifacts with computer systems papers. We analyze a cross-sectional data set of papers from 56 systems conferences from 2017. We collected extensive data on the conferences, papers, and authors.
  Exploratory paper... observations... main ones:
  After controlling for paper-, author-, and conference-related factors, the release of an artifact appears to increase the citations of a systems paper by some 34%.
output:
  bookdown::pdf_book:
    base_format: rticles::peerj_article
    keep_tex: true
    citation_package: natbib
  rticles::peerj_article: default
---

```{r code = readLines("../load_data.R"), echo = F, message = F}
```


```{r setup, echo=F, message=F, warning=F, cache=F}
library('tidyverse')
library('kableExtra')
library('MASS')
library('lmerTest')

knitr::opts_chunk$set(echo = FALSE, fig.align = "center")

artifacts <- read.csv(paste0(toplevel, "features/artifacts.csv"), na.strings = "",
                      colClasses = c("factor", rep("logical", 5), "factor"))

tags <- read.csv(paste0(toplevel, "features/content_tags.csv"), na.strings = "",
                      colClasses = c("factor", "factor"))

dummy_tags <- tags %>%
  group_by(key) %>%
  summarize(system= any(tag == "system"))

cbp1 <- c("#999999", "#E69F00", "#56B4E9", "#009E73",
          "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

# The palette with black:
cbp2 <- c("#000000", "#E69F00", "#56B4E9", "#009E73",
          "#F0E442", "#0072B2", "#D55E00", "#CC79A7")


how_long <- 42  # Citations 3.5 years since publiaation
tolerance <- 7


all_confs$org <- as.factor(ifelse(all_confs$is_org_ACM, "ACM", ifelse(all_confs$is_org_IEEE, "IEEE", ifelse(all_confs$is_org_USENIX, "USENIX", "other"))))
all_confs$key = gsub("_\\d+", "", as.character(all_confs$key))

ppl <- roles %>%
  filter(role == "author") %>%
  left_join(persons) %>%
  group_by(key) %>%
  summarize(nauthors = n(),
            total_npubs = sum(npubs, na.rm = T),
            max_h = max(hindex, na.rm = T),
            all_com = all(sector == "COM", na.rm = T),
            same_country = length(unique(country)) == 1,
            is_usa = first(country) == "US",
            is_woman = first(gender) == "F",
            top_university = any(top_university),
            top_company = any(top_company)
           ) %>%
  mutate(max_h = ifelse(is.finite(max_h), max_h, NA))


by_paper <- papers %>%
  left_join(artifacts) %>%
  left_join(filter(citations, near(months, how_long, tolerance))) %>%  # Pick anything within 3 months of how_long
  group_by(key) %>%     # And narrow down to the closest date
  filter(abs(months - how_long) == min(abs(months - how_long))) %>%
  mutate(conf = as.factor(gsub("_\\d+", "", as.character(key)))) %>%
  mutate(artifact_released = !is.na(unreleased) & !unreleased) %>%
  left_join(all_confs, by = c("conf" = "key")) %>%
  left_join(dummy_tags) %>%
  left_join(ppl)


with_ar <- filter(by_paper, artifact_released)
without_ar <- filter(by_paper, !artifact_released)
with_ar_l <- filter(with_ar, linked)
with_ar_nl <- filter(with_ar, !linked)
with_ar_b <- filter(with_ar, badge)
with_ar_nb <- filter(with_ar, !badge)
with_ar_e <- filter(with_ar, evaluated)
with_ar_ne <- filter(with_ar, !evaluated)
unreleased <- filter(by_paper, unreleased)
released <- filter(by_paper, !unreleased)
unexpired <- filter(by_paper, !expired)
expired <- filter(by_paper, expired)

## Function to pretty-format a means comparison across papers with and without artifacts released
report_means <- function(col)
{
  paste0(round(mean(pull(with_ar, col), na.rm = T), 2),
         " vs. ",
         round(mean(pull(without_ar, col), na.rm = T), 2),
         "; ",
         report_test(t.test(pull(with_ar, col), pull(without_ar, col))))
}
```


# Motivation and Background {#sec:intro}


Many scientific experimental results have failed to be repeated or reproduced, leading to the so-called "reproducibility crisis" [@baker16:reproducibility].
An experimental result is not fully established unless it can be independently reproduced, and an intermediate step towards this goal is to release and possibly audit artifacts associated with the work [@ACM20:artifact].
Artifacts are also a critical aspect of open-science initiatives, that have gained substantial momentum in computer science [@heumuller20:publish].

Unfortunately, the sharing and auditing of artifacts is still not as commonplace as warranted by its importance.
In particular, in the field of computer systems, where we might expect that at least software-based experiments should be easily reproducible, we find instead that many artifacts are not readily available or buildable [@collberg16:repeatability; @heumuller20:publish; @krishnamurthi15:real]
Researchers struggle to reproduce experimental results and reuse research code from scientific papers due to continuously changing software and hardware, lack of common APIs, stochastic behavior of computer systems and a lack of a common experimental methodology [@fursin21:collective].

The Association of Computing Machinery (ACM) defines a paper's artifact as follows [@ACM20:artifact]:

> By "artifact" we mean a digital object that was either created by the authors to be used as part of the study or generated by the experiment itself. For example, artifacts can be software systems, scripts used to run experiments, input datasets, raw data collected in the experiment, or scripts used to analyze results.

The quality, standards, and expectations for artifacts still vary greatly [@hermann20:community], and many artifacts submissions do not meet even the lowest expectation bar [@dahlgren19:getting].
In an attempt to standardize expectations for computer-science artifacts, the ACM describes its initiative to specifically evaluate artifact availability and functionality in some of its journals and conferences, and award papers with badges accordingly [@ACM20:artifact].
 One goal of this standardized artifact evaluation process is to send authors and readers the message that artifacts are valued an are an important contribution.
It also provides valuable author feedback [@krishnamurthi15:real].

There are some encouraging signs that such initiatives are leading to increased adoption.
For example, since 2015, almost all major programming language conferences allow the submission of research artifacts [@steuwer18:badges]
Other conferences in computer systems are also increasingly encouraging artifact sharing, including many conferences in our dataset [@childers17:artifact; @dahlgren19:getting; @heumuller20:publish; @saucez19:evaluating]


In addition to their benefits for open and reproducible science, papers with linked artifacts may be associated with higher citation counts [@childers17:artifact; @heumuller20:publish].
Citations are not only a widely used metric of influence for papers and researchers, but also an indirect measure of the work's quality and usefulness, which presumably are both helped by the availability of artifacts.
The primary goal of this paper is therefore to evaluate the association between artifacts availability and citations in the research field of computer systems.

This paper is primarily exploratory in nature, trying to uncover the relationships between artifact properties and other paper properties in a large cross-sectional field study.
Nevertheless, a few questions, inspired by similar studies, could be formulated before this data exploration and then be answered by it, we are offered as secondary contributions:


As a final contribution, this study provides a varied dataset of papers, tagged with rich metadata, including their artifact status.
Since comprehensive data on papers with software artifacts is not always readily available, because of the significant manual labor involved, this dataset can serve as the basis of additional studies.

The rest of this paper is organized as follows. The next section presents in detail the dataset, methodology, and limitations of our study.
An extensive set of descriptive and explanatory statistics is presented in the results section.
The discussion section that follows then elaborates on the secondary effects of artifact accessibility and citation covariates.
Finally, the concluding section summarizes the main findings of this study and enumerates some future research directions.






# Data and Methods {#sec:data}

The main challenge in this study was not actually the analysis itself but rather the data collection and cleaning.
It is therefore worthwhile to describe the data in detail to support replication and reproductions of this study.

```{r sys-confs, echo=F, message=F, warning=F, cache=T}
tmp <- all_confs %>%
  mutate(Conference = gsub("_\\d*", "", conference)) %>%
  rename(Date = postdate, Papers = npapers, Authors = authors_num, Country = country) %>%
  mutate(Acceptance = round(acceptance_rate, 2)) %>%
  dplyr::select(Conference, Date, Papers, Authors, Acceptance) %>%
  arrange(Conference)

cbind(tmp[1:(nrow(tmp)/2),], tmp[(1+nrow(tmp)/2):nrow(tmp),]) %>%
  knitr::kable(booktabs = T,
               align = c("l", "c", "r", "r", "r"),
               caption = "System conferences, including start date, number of published papers, total number of named authors, acceptance rate, and country code.") %>%
  kable_styling(font_size = 8, latex_options = "hold_position")
```

The primary dataset we analyze comes from a hand-curated collection of `r nrow(all_confs)` peer-reviewed systems conferences from a single publication year (2017).
In CS, and in particular in its more applied fields such as systems, original scientific results are typically first published in peer-reviewed conferences \cite{patterson99:evaluating,patterson04:health}, and then possibly in archival journals, sometimes years later \cite{vrettas15:conferences}.
The conferences we selected include some of the most prestigious systems conferences (based on indirect measurements such as Google Scholar's metrics), as well as several smaller or less-competitive conferences for contrast (Table \@ref(tab:sys-confs)).
To reduce time-related variance, we chose to focus on a large cross-sectional set of conferences from a single publication year.
Our choice of which conferences belong to "systems" is necessarily subjective.
Not all systems papers from 2017 are included in our set, and some papers that are in our set may not be universally considered part of systems (for example, if they lean more towards algorithms or theory).
Nevertheless, we believe that our cross-sectional set is both wide enough to represent the field well and focused enough to distinguish it from the rest of CS.
In total, our sample includes `r sum(all_confs$npapers)` peer-reviewed papers.

In addition to artifact information, we gathered various statistics on each conference, either from its web page, proceedings, or directly from its chairs.
We collected data about review policies, important dates, the composition of its technical PC, and the number of submitted papers, among others.
We also collected historical metrics from the Institute of Electrical and Electronics Engineers (IEEE), Association for Computing Machinery (ACM), and Google Scholar (GS) websites, including past citations, age, and total publications, and downloaded all papers in PDF format.


We are mainly interested in the effect of software artifacts on citations of their associated papers, and citations typically take a few months or years, allowing for the original papers to be discovered, read, cited, and then the citations themselves published and recorded.
The time duration since these papers had been published, approximately four years, permits the analysis of their short-to-medium term impact in terms of citations.
Only
`r top_cites <- group_by(citations, key) %>% summarize(max = max(citations)); sum(top_cites$max == 0)` papers
(`r pct(sum(top_cites$max == 0), nrow(top_cites), 2)`%)
had not been cited at all at the time of this writing.

For this study, the most critical piece of information on these papers is their artifacts.
Unfortunately, they included no standardized metadata to include artifact information, so it had to be collected manually from various sources, as detailed next.

The only form of standardized artifact metadata we found was for a subset of conferences organized by the ACM.
In the proceedings page in the ACM's digital library of these conferences, special badges denote which papers made artifacts available, and which ones had artifacts evaluated (for conferences that supported either badge).
In addition, the ACM digital library also serves as a repository for the artifacts (as supplementary information), and all of these ACM papers included a DOI link back to the appropriate web page with the artifact.

Unfortunately, most papers in this dataset were not published by the ACM or had no artifact badges.
These papers required a manual scanning of the PDF text of each paper in order to identify such links.
Several search terms were used to assist in this search, such as "github",  "gitlab", "bitbucket", "sourceforge", and "zenodo" for repositories; variants of "available", "open source", and "download" for links; and  variants of "artifact", "reproducibility", and "will release" for indirect references.
Moreover, the artifacts for numerous papers could actually be discovered online despite no mention in the original paper, by searching github.com for author names, paper titles, or system monikers.

In all, `r nrow(artifacts)` papers in our dataset
(`r pct(nrow(artifacts), nrow(papers), 2)`%)
had an identifiable artifact, primarily source code but occasionally data, configuration, or benchmarking files.
Artifacts that had been included in previous papers or written by someone other than the paper's authors were excluded from this count.
Anecdotally, most of the source-code repositories in these artifacts showed no commits, forks, or issues after the publication of their paper.

For each one of these papers/artifacts, the following fields were added to the paper's record in our dataset:

 * Whether the paper had an "artifact available" badge.
 * Whether it had an "artifact evaluated" badge.
 * Whether a link to the artifact was clearly available in the paper.
 * The URL for the artifact, whether linked to in the paper, or found elsewhere.
 * The last date that this artifact was still found intact online.

All of the searches for these artifacts are recent, so from the last field above we can denote the current status of an artifact as either "extant" or "expired".
From the availability of a URL we can classify an artifact as "released" or "unreleased" (the latter denoting papers that promised an artifact but no link or repository was found).
And from the host of the URL we can classify the location of the artifact as either an "Academic" web page, the "ACM" digital library", A "Filesharing" service such as Dropbox or Google, a specialized "Repository", such as Github.com, "Other" (including .com and .org web sites), and "NA".
The data for all these artifacts is summarized in the file `features/artifacts.csv` in the supplementary information.

The final relevant piece of the dataset is the conferences' attitude toward artifacts.
A few conferences specifically encouraged artifacts in the call-for-papers or web-sites, such as MobiCom and SLE.
Four conferences---SC, OOPSLA, PLDI, SLE---archived their artifacts in the ACM's digital library.
In addition to general encouragement and archival, seven conferences specifically offered to evaluated artifacts by a program committee: OOPSLA, PACT, PLDI, PPoPP, SC, and SLE.
Within these conferences,
`r evald <- filter(by_paper, conf %in% c("OOPSLA", "PACT", "PLDI", "PPoPP", "SC", "SLE")); pct(sum(evald$artifact_released), nrow(evald), 2)`%
of papers released an artifact, a very similar rate to the $\approx{40}$% rate found in a smaller study of systems conference with an artifact evaluation process [@childers17:artifact].


## Statistics {-}

For statistical testing, group means were compared pairwise using Welch's two-sample t-test; differences between distributions of two categorical variables were tested with $\chi^{2}$ test; and comparisons between two numeric properties of the same population were evaluated with Pearson's product-moment correlation. All statistical tests are reported with their p-values.

## Limitations {-}

Our methodology is primarily constrained by the manual curation of data, especially artifact data.
The effort involved in compiling all the necessary data limits the scalability of our approach to additional conferences or years.
Furthermore, the manual search for artifacts in the text and in repositories is a laborious process and prone to human error.
Although a large enough number of artifacts was identified for statistical analysis,
there likely remain untagged papers in the dataset that did actually release an artifact (false negatives).
Nevertheless, there is no evidence to suggest that their number is large or that their distribution is skewed in some way as to bias statistical analyses.
That said, since the complete dataset is (naturally) also released as an artifact of this paper, it can be enhanced and corrected over time.

## Ethics statement {-}

All of the data for this study was collected from public online sources and therefore did not require the informed consent of the papers' authors.
The cleaned and summarized data itself is also made fully available to the research community.

# Results {#sec:results}



## Descriptive statistics

We start with simple characterization of the statistical distributions of artifacts in our dataset.
of the `r nrow(artifacts)` papers with artifacts, we find that about
`r pct(sum(artifacts$linked), nrow(artifacts))`%
included an actual link to the artifact in the text.
The ACM digital library marked
`r sum(artifacts$badge)` papers (`r pct(sum(artifacts$badge), nrow(artifacts))`%)
with an "Artifact available" badge, and
`r sum(artifacts$evaluated)` papers (`r pct(sum(artifacts$evaluated), nrow(artifacts))`%)
with an "Artifact evaluated" badge.
The majority of artifact papers
(`r pct(sum(!artifacts$expired), nrow(artifacts))`%)
still had their artifacts available for download at the time of this writing.
This ratio is somewhat similar to a comparable study that found that 73% of URLs in five open-access journals after five years [@saberi12:accessibility].
Of the `r nrow(artifacts)` papers that promised artifacts, `r sum(artifacts$unreleased)` appear to have never released them.
The distribution of the location of the accessible artifacts is shown in Table \@ref(tab:location-dist), and is dominated by Github repositories.

```{r location-dist, echo=F}
artifacts %>%
  mutate(Location = factor(location, levels = c("Repository", "Academic", "Other", "ACM", "Filesharing", NA))) %>%
  group_by(Location) %>%
  summarize(Count = n()) %>%
  knitr::kable(booktabs = T,
               align = "lr",
               caption = "Class of artifact URLs. `NA' locations indicate expired or unreleased URLs.")
```

```{r artifact-ratios, warning=F, message=F, fig.height = 7, out.width="95%", fig.cap="Papers with artifact by conference."}
by_conf <- by_paper %>%
  group_by(conf) %>%
  summarize(ratio = sum(!is.na(expired)) / n(), n = n()) %>%
  left_join(all_confs, by = c("conf" = "key")) %>%
  arrange(desc(ratio)) %>%
  mutate(org = factor(org, levels = c("ACM", "IEEE", "USENIX", "other")))

ggplot(by_conf, aes(x = reorder (conf, ratio), y = 100 * ratio, fill = org)) +
  geom_bar(stat = "identity") +
  geom_text(aes(y = 95, label = paste0("n=", n), color = org), hjust = 0, size = 3, show.legend = F) +
  coord_flip() +
  theme_minimal() +
  ylim(0, 100) +
  xlab("Conference") +
  ylab("Percent of paper with artifacts") +
  guides(fill = guide_legend("Organization")) +
  scale_color_manual(values = c(cbp1[2:4], cbp1[1])) +
  scale_fill_manual(values = c(cbp1[2:4], cbp1[1])) +
  theme(legend.position = "bottom",
        axis.text.y = element_text(angle = 0, hjust = 1),
        panel.grid.major.y = element_blank())
```

Looking at the differences across conferences, Fig. \@ref(fig:artifact-ratios) shows the percentage of papers with artifacts per conference, ranging from 0% to OOPSLA's
`r round(100 * by_conf[1,]$ratio, 2)`%
(mean: `r round(100 * mean(by_conf$ratio), 2)`%,
SD: `r round(100 * sd(by_conf$ratio), 2)`%).
Unsurprisingly, nearly all of the conferences where artifacts were evaluated are prominent in their relatively high artifact rates.
Only MobiCom stands out as a conference that evaluated artifacts for the "best community paper award" but had a low overall ratio of papers with artifacts.

It should be noted, however, that many papers in MobiCom are hardware-related, where artifacts are typically unfeasible.
The same is true for a number of other conferences with low artifact ratios, such as ISCA, HPCA, and MICRO.
Also worth noting is the fact ACM conferences appear to attract many more artifacts than IEEE conferences, although the reasons likely vary on a conference-by-conference basis.

Without directly comparable information on artifact availability rates in other fields, it is impossible to tell whether the rate in our dataset,
`r pct(nrow(with_ar), nrow(by_paper))`%,
is high or low.
It appears to be on the higher end, however, when compared indirectly to the figures in some recent studies [@CITE].
In general, skimming the papers in our dataset revealed that many "systems" papers do indeed describe the implementation of a new computer system, mostly in software.
It is plausible that the abundance of software systems in these papers and the relative ease of releasing them as software artifacts has contributed substantially to this rate.



## Relationships to citations

The main research question of this paper is, does the open availability of an artifact affect the citations of a paper in computer systems?
To answer this question, we look at the distribution of citations for each paper exactly `r how_long` months after it was published, to normalize the comparison.
Figure \@ref(fig:cite-hist) shows the overall paper distribution as a histogram, while Fig. \@ref(fig:cite-dist) breaks down the distributions of artifact and non-artifact papers, as density plots.

```{r cite-hist, echo = F, warning = F, message = F, fig.asp = 0.6, fig.cap = paste("Distribution of paper citations", how_long, "months after publication (log-scale)")}
ggplot(by_paper, aes(x = citations)) +
  geom_histogram(color = "black", fill = "blue", alpha = 0.3) +
#  stat_density(aes(y = ..count..), color = "black", fill = "blue", alpha = 0.3) +
  scale_x_continuous(breaks = c(0, 1, 2, 3, 5, 10, 20, 30, 50, 100, 200, 300, 500, 1000, 2000), trans = "log1p", expand = c(0, 0)) +
#  geom_histogram(stat = "count") +
  theme_minimal()
#  scale_x_log10() #breaks = c(1, 2, 11, 101, 1001, 2001), labels = c(0, 1, 10, 100, 1000, 2000))
```

```{r cite-dist, echo = F, warning = F, message = F, fig.asp = 0.6, fig.cap = paste("Density plot of paper citations", how_long, "months after publication (log-scale)")}
ggplot(by_paper, aes(x = citations, color = artifact_released)) +
  geom_density() +
  theme_minimal() +
  theme(legend.position = "bottom") +
  guides(color = guide_legend(title = "Artifact released")) +
  scale_color_manual(values = c("darkgreen", "orange")) +
  scale_x_log10()
```


Citations range from none at all
(`r nrow(filter(by_paper, citations == 0))` papers) to nearly 1,000, with two outlier papers exceeding 2,000 citations [@carlini17:towards; @jouppi17:datacenter].
The distributions appear roughly log-normal.
The mean citations per paper with artifacts released was
`r round(mean(with_ar$citations), 1)`,
compared to
`r round(mean(without_ar$citations), 1)` with none
(`r report_test(t.test(with_ar$citations, without_ar$citations))`).
Since the citation distribution is so right-skewed, it makes sense to also compare the median citations with and without artifacts
(`r median(with_ar$citations)` vs. `r median(without_ar$citations)`, `r report_test(wilcox.test(with_ar$citations, without_ar$citations))`).
Both statistics suggest a clear and statistically significant advantage in citations for papers that released an artifact.
Likewise, the `r nrow(released)` papers that released an artifact garnered more citations than the `r nrow(unreleased)` papers that did promised an artifact that could not be found
(`r report_test(t.test(released$citations, unreleased$citations))`),
and extant artifacts fared better than expired ones
(`r report_test(t.test(unexpired$citations, expired$citations))`).

In contradistinction, some positive attributes of artifacts made them actually less cited.
For example, the mean citations of the
`r nrow(with_ar_l)`
papers with a linked artifact,
`r round(mean(with_ar_l$citations), 1)`,
was much lower than the
`r round(mean(with_ar_nl$citations), 1)`
mean for the `r nrow(with_ar_nl)` papers with artifacts we found using a Web search
(`r report_test(t.test(with_ar_l$citations, with_ar_nl$citations))`;
`r report_test(wilcox.test(with_ar_l$citations, with_ar_nl$citations))`).
Curiously, the inclusion of a link in the paper, presumably making the artifact more accessible, was associated with fewer citations.

Similarly counter-intuitive, papers who received an "Artifact evaluated" badge fared worse in citations than artifact papers who did not
(`r report_test(t.test(with_ar_e$citations, with_ar_ne$citations))`;
`r report_test(wilcox.test(with_ar_e$citations, with_ar_ne$citations))`).
Papers who received an "Artifact available" badge did not fare significantly worse than artifact papers who did not
(`r report_test(t.test(with_ar_b$citations, with_ar_nb$citations))`;
`r report_test(wilcox.test(with_ar_b$citations, with_ar_nb$citations))`).

Finally, we can also break down the citations per paper grouped by the type of location for the artifact and by organization, looking at medians instead because of the outsized effects of outliers (Table \@ref(tab:location-cites)).
The three major location categories do not show significant differences in median citations, and the last two categories may be too small to ascribe statistical significance to their differences.


```{r location-cites, echo=F}
with_ar %>%
  drop_na(location) %>%
  mutate(Location = factor(location, levels = c("Repository", "Academic", "Other", "ACM", "Filesharing"))) %>%
  group_by(Location) %>%
  summarize("Median citations" = round(median(citations), 0)) %>%
  knitr::kable(booktabs = T,
               align = "lr",
               caption = "Median citations by class of artifact URLs for extant artifacts")
```


```{r org-cites, echo=F, eval=F}
with_ar %>%
  drop_na(org) %>%
  mutate(Organization = factor(org, levels = c("ACM", "IEEE", "USENIX", "other"))) %>%
  group_by(Organization) %>%
  summarize("Median citations" = round(median(citations), 0)) %>%
  knitr::kable(booktabs = T,
               align = "lr",
               caption = "Median citations by class of artifact URLs for extant artifacts")
```

A related question is how often do systems papers cite other artifacts.
Unfortunately, there is no simple automated way to answer this question, and a careful reading of all `r nrow(papers)` papers is impractical.
As a crude approximation, a simple search for the string "github" in the full-text of all the papers yielded 900 distinct results.
Keep in mind, however, that perhaps half of those could be referring to their own artifact rather another paper's, and that not all cited github repositories do indeed represent paper artifacts.
Incidentally, papers with released artifacts also tend to average significantly more references of their own
(`r report_means("references")`),
but there is no reason to suspect a causal relationship rather than some confounding cause.
We dive deeper into questions of association and causality with citations in the discussion section.

## Covariate analysis

Having addressed the relationships between artifacts and citations, we can now explore relationships with additional variables from this expansive dataset.



### Accessibility

One of the most important aspects of software artifacts, per the FAIR principles [@CITE], is that they be easily accessible.
As mentioned previously,
`r pct(nrow(filter(by_paper, expired)), nrow(filter(by_paper, !is.na(expired))))`%
of artifacts are already inaccessible, a mere $\\aprox{3.5}$ years after publication.
Most of the artifacts in our dataset were published in code repositories, predominantly Github, that do not guarantee persistent access or even universal access protocols such as DOI.
However, only
`r pct(nrow(filter(by_paper, expired, location == "Repository")), nrow(filter(by_paper, !is.na(expired), location == "Repository")))`%
of the "Repository" artifacts were inaccessible.
In contrast,
`r pct(nrow(filter(by_paper, expired, location == "Academic")), nrow(filter(by_paper, !is.na(expired), location == "Academic")))`%
of the artifacts in university pages have already expired, likely because they had been hosted by students or faculty that have since moved elsewhere.
Also, a full
`r pct(nrow(filter(by_paper, expired, location == "Filesharing")), nrow(filter(by_paper, !is.na(expired), location == "Filesharing")))`%
of the artifacts on file-sharing sites such as Dropbox or Google Drive are no longer there, possibly because these are paid services or free to a limited capacity, and can get expensive to maintain over time.

Another important FAIR principle is the findability of the artifact, which in the absence of artifact DOIs in our dataset, we estimate by looking at the number of papers that explicitly link to their artifacts.
The missing artifacts consisted of a full
`r pct(nrow(filter(by_paper, expired, !linked)), nrow(filter(by_paper, !is.na(expired), !linked)))`%
of the papers with no artifact link, compared to only
`r pct(nrow(filter(by_paper, expired, linked)), nrow(filter(by_paper, !is.na(expired), linked)))`%
for papers that linked to them
(`r report_test(chisq.test(table(by_paper$expired, by_paper$linked)))`).

A related question to artifact accessibility is how accessible is the actual paper that introduced the artifact, which may itself be associated with higher citations [@gargouri10:self; @mccabe15:availability; @mckiernan16:point; @tahamtan16:factors].
A substantial proportion of the papers
(`r pct(sum(by_paper$open_access), nrow(by_paper))`%)
were published in `r sum(all_confs$open_access)` open-access conferences.
Other papers have also been released openly as preprints or via other means.
One way to gauge the availability of the paper's text is to look it up on GS and see if an accessible version is linked, which is recorded in our dataset.
Of the `r nrow(papers)` papers,
`r pct(sum(!is.na(by_paper$months_to_eprint)), nrow(by_paper))`%
Displayed an accessible link to the full text on GS.
Specifically, of the papers that released artifacts,
`r pct(nrow(filter(with_ar, !is.na(months_to_eprint))), nrow(with_ar))`%
were associated with an accessible paper as well, compared to
`r pct(nrow(filter(without_ar, !is.na(months_to_eprint))), nrow(without_ar))`%
of the papers with no artifacts
(`r report_test(chisq.test(table(by_paper$artifact_released, is.na(by_paper$months_to_eprint))))`).


Moreover, our dataset includes not only the availability of a fulltext link on GS, but also the approximate duration since publication (in months) that it took GS to display this link, offering a quantitative measure of accessibility speed as well.
It shows that for papers with artifacts, GS averaged approximately
`r round(mean(with_ar$months_to_eprint, na.rm = T), 1)`
months to display a link to an eprint, compared to
`r round(mean(without_ar$months_to_eprint, na.rm = T), 1)`
months for papers with no artifacts
(`r report_test(t.test(with_ar$months_to_eprint, without_ar$months_to_eprint))`).
Both accessibility differences are statistically significant, but keep in mind that these accessibility of papers and artifacts are not independent: some conferences that encouraged artifacts were also open-access, particularly with the ACM.
Accessibility of the paper may also play a role in its citability; several studies suggested that accessible papers are better cited [@bernius09:open; @niyazov16:open; @snijder16:revisiting], although others disagree [@calver10:patterns; @davis11:impact].


### Awards

Many conferences present competitive awards, such as "best paper," "best student paper," "community award," etc.
Of the `r nrow(by_paper)` total papers,
`r pct(sum(by_paper$award), nrow(by_paper))`% received at least one such award.
Papers with artifacts are disproportionately represented in this exclusive subset
(`r pct(nrow(filter(with_ar, award)), nrow(filter(by_paper, award)), 1)`% vs.
`r pct(nrow(filter(with_ar, !award)), nrow(filter(by_paper, !award)), 1)`% in non-award papers;
`r report_test(chisq.test(table(by_paper$award, by_paper$artifact_released)))`).

Again, it is unclear whether this relationship is causal, since the two covariates are not entirely independent.
A handful of awards specifically evaluated the contribution of the paper's artifact.
Even if the relationship is indeed causal, its direction is also unclear, since
`r pct(nrow(filter(with_ar, award, !linked)), nrow(filter(with_ar, award)), 1)`%
of award papers with artifact did not link to it in the paper.
It is possible these papers released their artifacts after winning the award or because of it.

### Textual properties

Some of the textual properties can be estimated from the full text of the papers using simple command-line tools.
Our dataset includes three such properties: the length of each paper in words, the number of references it cites, and the existence of a system's name in the paper's title.

The approximate paper length in words and the number of references turn out to be positively associated with the release of an artifact.
Papers with artifacts average **more pages** than papers without
(`r report_means("mean_pages")`),
**more words**
(`r report_means("words")`),
and **more references**
(`r report_means("references")`).
Keep in mind, however, that longer papers also correspond to more references
(`r report_test(cor.test(by_paper$words, by_paper$references))`),
and are further confounded with specific conference factors such as page limits.

As previously mentioned, many systems papers introduce a new computer system, often as software.
Sometimes, these papers name their system by a moniker, and their title starts with the moniker, followed by a colon and a short description (e.g., "Widget: An Even Faster Key-Value Store").
This feature is easy to extract automatically for all paper titles.

We could hypothesize that a paper that introduces a new system, especially a named system, would be more likely to include an artifact with the code for this system, quite likely with the same repository name.
Our data supports this hypothesis.
The ratio of artifacts released in papers with a labeled title,
`r pct(nrow(filter(with_ar, labeled_title)), nrow(filter(by_paper, labeled_title)))`%,
is nearly double that of papers without a labeled title,
`r pct(nrow(filter(with_ar, !labeled_title)), nrow(filter(by_paper, !labeled_title)))`%
(`r report_test(chisq.test(table(by_paper$artifact_released, by_paper$labeled_title)))`).

These textual relationships may not be very insightful, because of the difficulty to ascribe any causality to them, but they can clue the paper's reader to the possibility of an artifact, even if one is not linked in the paper.
Indeed, they accelerated the search for such unlinked artifacts during the curation of the data for this paper.


### Conference prestige

Finally, we look at conference-specific covariates that could represent how well-known or competitive a conference is.
In addition to textual conference factors, these conference metrics may also be associated with higher rates of artifact release.

Several proxy metrics for prestige appear to support this hypothesis.
Papers with released artifacts tend to be appear in conferences that average a **lower acceptance rate**
(`r report_means("acceptance_rate")`),
**more paper submissions**
(`r report_means("submissions")`),
**Higher historical mean citations per paper**
(`r report_means("mean_historical_citations")`),
and a **higher h5-index** from GS metrics^[\url{https://scholar.google.com/citations?view_op=top_venues}]
(`r report_means("h5_index")`).
Also note that papers in conferences that offered some option for author response to peer review (often in the form of a rebuttal) were slightly more likely to include artifacts, perhaps as a response to peer review
(`r report_test(chisq.test(table(by_paper$artifact_released, by_paper$rebuttal)))`).

To explain these relationships, we might hypothesize that a higher rate of artifact submission would be associated with more reputable conferences, either because artifact contribute to prestige, or because more rigorous conferences are also more likely to expect the artifacts.
Observe, however, that some of the conferences that encourage or require artifacts are not as competitive as the others.
For example, OOPSLA, with the highest artifact rate, had an acceptance rate of 0.3, and SLE, with the fourth-highest artifact rate, had an acceptance rate of 0.42.
The implication here is that it may not enough for a conference to actively encourage artifacts for it to be competitive, but a conference that already is competitive may also attract more artifacts.


<!--
Research questions:

  * OK was an artifact available and evaluated?
  * OK how well was the paper cited in 42 months since publication?
  * OK How do different properties of the artifact affect citations, including factors such as: continuing artifact availability; continuing development past publication date; Total number of new and ongoing contributors; and type of software repository,
  * NOPE How do these factors also affect indexing and discoverability of the artifacts?
  * NOPE Was the software artifact cited independently of the article at any point?
  * OK, useless: Estimate % of papers with a "system" in them, based on the tags we have (compute confidence intervals for a boolean property / proportion)
  * OK Compare citation rates for artifact vs. no artifact; available artifact vs. expired; linked vs. not linked; badge vs. not badge; evaluated badge or not; award vs. no award.

Additional questions:

  * Authors from only industry? [@collberg16:repeatability]
  * did artifact availability affect the paper's scores during peer review?
  * OK Did badge availability increase the amount of artifact sharing?
  * OK how many of these artifact survived to date, and what mechanisms were used to preserve or to cite them?
  * OK: What type of URL is most likely to survive? DL? github? .edu/.ac? .com?
  * OK: how quickly was the paper discovered by a database such as Google Scholar?
  * Look at the difference between posting on github and ACM/dl (for persistence/DOI).
  * OK, useless: How many artifact (URLs) were reused?
  * what other paper or conference characteristics correlate with discoverable and citeable software artifact?
  * OK: How many papers cite a github reference? How many?
  * TODO: Look at topic tags or paper entities.
  * TODO: Look at months to gs, months to e-print
  * Graph of percentage of avail-artifact by org and by conference and by topic. SC in particular has an artifact evaluation appendix.
  * Assess the [FAIR](https://www.go-fair.org/fair-principles/) aspects of the data as features (based on the [updated critera](https://www.cell.com/patterns/fulltext/S2666-3899(21)00036-2):
    - Findability:  linked in paper or have to google?; persistent identifier/DOI?
    - Accessibility: artifact still accessible; the protocol is open, free, and universal; metadata outlives the data.
    - Interoperable: (Meta)data use a formal, accessible, shared, and broadly applicable language for knowledge representation.
    - Reusable: accessible data license; detailed provenance; meets domain-relevant community standards.


When comparing artifact to citations or word, control for specific conference or at least conference characteristic.

-->



# Discussion {#sec:discussion}


 * Conference organizers have a large impact on the availability of artifacts.
 * Address the F and A of FAIR.

## Controlling for confounders

We observed some strong associations between artifact release and paper attributes such as: citations, length, etc.
Comparing citations across groups with and without artifacts as computed in the previous section is informative, but not necessarily indicative of a causal relationship.
As cautioned throughout this study, these associations are insufficient to draw strong causal conclusion, primarily because there are many confounding variables, most of which related to the publishing conference.
These confounding factors could provide provide partial or complete explanation to differences in citations beyond artifact availability.

In other words, papers published in the same conference might exhibit strong correlations that interact or interfere with the attribute we are trying to compare to artifact release.
We can attempt to control for these confounding variables by controlling for conference when evaluating associations using a multilevel model.

One obvious factor affecting citations is time since publication, which we control for by measuring all citations at exactly the same interval, `r how_long` months since the conference's official start.
Another crucial factor is the field of study, which we normalize for by focusing on a single field, while provide a wide cross-section of the field to limit the effect of statistical variability.

There are also numerous paper-related factors that have shown positive association with citations, such as: review papers, fewer equations, number of references, statistically significant positive results, length of papers, number of figures and images, and even more obscure features such as the presence of punctuation marks in the title.
We can control for the paper-related features we collected by building a simple linear regression model of citations as a function of artifact availability, and then add control variables as predictor variables and observe the effect on the main predictor.
Because of the right skew of the citation distribution, the response variable we examine is instead `r epsilon = 0` $ln(citations+)$, omitting the `r sum(by_paper$citations == 0)` papers with zero citations.

In the simplest form,
`r clean <- filter(by_paper, citations > 0); model0 <- lm(data = clean, log(citations + epsilon) ~ artifact_released)`
fitting a linear model of the log-transformed citations as a function of artifact released yields an intercept (base citations) of
`r round(coef(model0)[1], 2)`
and slope of
`r round(coef(model0)[2], 2)`,
meaning that releasing an artifact adds approximately
`r round(100 * (exp(coef(model0)[2]) - 1), 0)`%
more citations to the paper, after exponentiation.
The p-value for this predictor is exceedingly low (less than $2\times10^{-16}$)
but the simplistic model only explains
`r round(100 * summary(model0)$adj.r.squared, 2)`%
of the variance in citations
(Adjusted $R^2$=`r round(summary(model0)$adj.r.squared, 3)`).
The Bayesian Information Criterion (BIC) for this model is
`r format(BIC(model0), scientific=F)`,
with
`r summary(model0)$df[2]` degrees of freedom.

We can now add various paper covariates to the linear model in an attempt to get more precise estimates for the artifact released predictor, by iteratively experimenting with different predictor combinations to minimize BIC using stepwise model selection [@garcia-portugues21:modeling, Ch. 3].
The per-paper factors considered were: **paper length** (words), **number of coauthors**, **accessibility speed** (months to eprint^[Papers with no eprint available at the time of this writing were given a time to eprint of 1,000 months, which turned out to be insignificant for the regression analysis.)]), **number of references**, **colon in the title**, and **award** given.

```{r modelBIC1, cache = T, echo = F, message = F, results = 'hide'}
cleaner <- clean %>%
  mutate(months_to_eprint = ifelse(is.na(months_to_eprint), 1000, months_to_eprint))

model1 <- lm(data = cleaner, log(citations + epsilon) ~ artifact_released + words + months_to_eprint + references + nauthors + award + subtitle)
modelBIC1 <- stepAIC(model1, k = log(nrow(cleaner)))
```

It turns out that all but the last factor have a statistically significant effect on citations, which brings the model to an increased adjusted $R^2$ value of
`r round(summary(modelBIC1)$adj.r.squared, 3)`
and a BIC of
`r round(BIC(modelBIC1), 2)` (df = `r summary(modelBIC1)$df[2]`).
However, the coefficient for artifact released went down to
`r round(coef(summary(modelBIC1))[2,1], 2)`
(`r round(100 * (exp(coef(modelBIC1)[2]) - 1), 0)`% relative citation increase)
with an associated p-value of
`r round(coef(summary(modelBIC1))[2,4], 14)`.

```{r modelBIC2, cache = T, echo = F, message = F, results = 'hide'}
cleanest <- cleaner %>%
  drop_na(total_npubs, max_h, all_com, same_country, is_usa, is_woman, top_university, top_company)
model2 <- lm(data = cleanest, log(citations + epsilon) ~ artifact_released + words + months_to_eprint + references + nauthors + subtitle + total_npubs + max_h + all_com + same_country + is_usa + is_woman + top_university + top_company)
modelBIC2 <- stepAIC(model2, k = log(nrow(cleanest)))
```

Similar to paper variables, some author-related factors such as their own reputation, author country, and their gender has been associated with citation count [@tahamtan16:factors].
We enhance our linear model with the following metrics (omitting `r nrow(cleaner) - nrow(cleanest)` papers with NA values):

 * Whether all the coauthors with a known affiliation came from the same country, as international collaborations can be associated with higher citations [@puuska14:international].
 * Is the lead author affiliated with the United States, which may also lead to higher citations [@gargouri10:self; @peng12:where].
 * Whether any of the coauthors was affiliated with one of the top 50 universities per [www.topuniversities.com](www.topuniversities.com)  (27% of papers) or a top company (if any author was affiliated with either (Google, Microsoft, Yahoo!, or Facebook: 18% of papers), based on the definitions of a similar study [@tomkins17:reviewer].
 * The sum of the total past publications of all coauthors of the paper [@bjarnason02:nordic].
 * The maximum h-index of all coauthors [@hurley13:deconstructing].
 * Whether all the coauthors with a known affiliation came from industry.
 * The gender of the first author [@frachtenberg21:whpc].

Only the maximum h-index and top-university affiliation had statistically significant coefficients, but hardly affected the overall model.
The minimal changes likely do not justify the cost in increased complexity and reduced dataset (because of missing data), so for the remainder of the analysis we ignore author-related factors and proceed with the previous model.

Finally, we add the last level: conference factors.
Conference (or journal) factors, such as the conference's own prestige and competitiveness, can have large effects on citations, as discussed in the previous section.
Although we can approximate some of these factors with metrics, there may also be other unknown or qualitative conference factors we cannot model.
To account for conference effects, we instead build a mixed-methods model, where all the previously mentioned factors become fixed effects and the conference becomes a random effect [@roback21:beyond, Ch. 8].

```{r modelLMER, cache = T, echo = F, message = F, results = 'hide', warning = F}
modelLMER <- lmer(data = cleaner, log(citations + epsilon) ~ artifact_released + words + months_to_eprint + references + nauthors + subtitle + (1|conference))
```

This last model does indeed reduce the relative effect of artifact release on citations to a coefficient of
`r round(coef(summary(modelLMER))[2, 1], 2)`.
But this coefficient still represents a relative citation increase of about a third for papers with released artifacts
(`r round(100 * (exp(coef(summary(modelLMER))[2, 1]) - 1), 0)`%),
which is significant.
We can approximate a p-value for this coefficient via Satterthwaiteâ€™s degrees of freedom method using R's `lmerTest` package [@kuznetsova17:lmertest], which is also statistically significant at
`r round(coef(summary(modelLMER))[2, 5], 14)`.


Even with all these controls, it is still important to emphasize that this strong observational association is still insufficient to claim causation [@lewis18:open].

* TODO: Manually add [89] reference counts cleanly to src/gather_papers.py

# Conclusion {#sec:conclusion}

After controlling for various paper-related and conference-related factors, we still find a positive association between artifact availability and paper citations in computer systems.
Number of citations is a contentions metric,  and it is associated with many other factors, primarily among those--we can hope--is the subjective quality of the work.
Some of these factors may better explain the increase in citations for papers with artifacts.
Although this association does not necessarily imply causation, it is possible and perhaps even plausible that that artifact availability does contribute to the citation count of a paper.
Certainly, there are many other benefits to open and reproducible science that make this effort worthwhile regardless of citation.
It is therefore encouraging to see an increasing number of systems conferences actively encouraging and investing effort in the artifact evaluation and dissemination process.
For example, The automated archiving of artifacts by ACM and the inclusion of badges may be Conducive to higher artifact availability, but not necessarily to higher citations for these paper.

 * Many computer systems papers include artifacts, primarily in the form of the software that implements the new system that is at the center of the paper.
 * The majority of these artifacts are hosted on Github. Although we found that very few of these expired, Github currently offers no guarantees for persistence or indexing. Conference organizers could request artifacts to have their own persistent, citable links (as SC currently does). Alternatively, Github could offer easier tools or automated integration with other tools (such as Zenodo) to offer this service.
