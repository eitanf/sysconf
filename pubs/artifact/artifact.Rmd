---
title: Software artifacts and citations in computer systems papers
preprint: false
author:
  - name: Eitan Frachtenberg
    corresponding: true
    email: eitan@reed.edu
affiliation:
    address: Department of Computer Science, Reed College
bibliography: ../sysconf.bib
abstract: >
  Research in computer systems often involves the engineering, implementation, and measurement of complex systems software. The availability of these software artifacts is critical for the reproducibility and replicability of the research's results, because system software often embodies numerous implicit assumptions and parameters that are not fully documented in the research article itself. Artifact sharing has also been associated with higher citations counts. And yet, the sharing of research artifacts is still not as common as warranted by its importance.
  
  
  The primary goal of this study is to provide a statistical analysis of the artifact-sharing rates and associated factors and effects in the field.
  To this end, we explore a cross-sectional dataset of papers in 56 systems conferences from 2017. In addition to extensive data on the conferences, papers, and authors, this dataset includes up-to-date data on the release, ongoing availability, badging, and locations of research artifacts. We combine this manually curated data with recent citation counts to evaluate the relationships between different artifact properties and citation metrics. Additionally, we revisit previous observations from other fields on the relationships between artifact properties and various other characteristics of papers, authors, and venue and apply them to this field.


  The overall rate of artifact sharing we find in this dataset is approximately 30%, although it varies significantly with paper, author, and conference factors, and it is closer to 43% for conferences that actively evaluated artifact sharing. Approximately 20% of all shared artifacts are no longer accessible four years after publications, predominately when hosted on personal and academic websites. In terms of citations, papers with shared artifacts averaged approximately 75% more citations than papers with none. Even after controlling for numerous confounding covariates, the release of an artifact appears to increase the citations of a systems paper by some 34%. This metric is further boosted by the open availability of the paper's text.
output:
  bookdown::pdf_book:
    base_format: rticles::peerj_article
    keep_tex: true
    citation_package: natbib
  rticles::peerj_article: default
---

```{r code = readLines("../load_data.R"), echo = F, message = F}
```


```{r setup, echo=F, message=F, warning=F, cache=F}
library('tidyverse')
library('kableExtra')
library('MASS')
library('lmerTest')

knitr::opts_chunk$set(echo = FALSE, fig.align = "center")

artifacts <- read.csv(paste0(toplevel, "features/artifacts.csv"), na.strings = "",
                      colClasses = c("factor", rep("logical", 5), "factor"))

tags <- read.csv(paste0(toplevel, "features/content_tags.csv"), na.strings = "",
                      colClasses = c("factor", "factor"))

dummy_tags <- tags %>%
  group_by(key) %>%
  summarize(system= any(tag == "system"))

cbp1 <- c("#999999", "#E69F00", "#56B4E9", "#009E73",
          "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

# The palette with black:
cbp2 <- c("#000000", "#E69F00", "#56B4E9", "#009E73",
          "#F0E442", "#0072B2", "#D55E00", "#CC79A7")


how_long <- 42  # Citations 3.5 years since publication
tolerance <- 7


all_confs$org <- as.factor(ifelse(all_confs$is_org_ACM, "ACM", ifelse(all_confs$is_org_IEEE, "IEEE", ifelse(all_confs$is_org_USENIX, "USENIX", "other"))))
all_confs$key = gsub("_\\d+", "", as.character(all_confs$key))

ppl <- roles %>%
  filter(role == "author") %>%
  left_join(persons) %>%
  group_by(key) %>%
  summarize(nauthors = n(),
            total_npubs = sum(npubs, na.rm = T),
            max_h = max(hindex, na.rm = T),
            all_com = all(sector == "COM", na.rm = T),
            same_country = length(unique(country)) == 1,
            is_usa = first(country) == "US",
            is_woman = first(gender) == "F",
            top_university = any(top_university),
            top_company = any(top_company)
           ) %>%
  mutate(max_h = ifelse(is.finite(max_h), max_h, NA))


by_paper <- papers %>%
  left_join(artifacts) %>%
  left_join(filter(citations, near(months, how_long, tolerance))) %>%  # Pick anything within 3 months of how_long
  group_by(key) %>%     # And narrow down to the closest date
  filter(abs(months - how_long) == min(abs(months - how_long))) %>%
  mutate(conf = as.factor(gsub("_\\d+", "", as.character(key)))) %>%
  mutate(artifact_released = !is.na(unreleased) & !unreleased) %>%
  left_join(all_confs, by = c("conf" = "key")) %>%
  left_join(dummy_tags) %>%
  left_join(ppl)


with_ar <- filter(by_paper, artifact_released)
without_ar <- filter(by_paper, !artifact_released)
with_ar_l <- filter(with_ar, linked)
with_ar_nl <- filter(with_ar, !linked)
with_ar_b <- filter(with_ar, badge)
with_ar_nb <- filter(with_ar, !badge)
with_ar_e <- filter(with_ar, evaluated)
with_ar_ne <- filter(with_ar, !evaluated)
unreleased <- filter(by_paper, unreleased)
released <- filter(by_paper, !unreleased)
unexpired <- filter(by_paper, !expired)
expired <- filter(by_paper, expired)

## Function to pretty-format a means comparison across papers with and without artifacts released
report_means <- function(col)
{
  paste0(round(mean(pull(with_ar, col), na.rm = T), 2),
         " vs. ",
         round(mean(pull(without_ar, col), na.rm = T), 2),
         "; ",
         report_test(t.test(pull(with_ar, col), pull(without_ar, col))))
}
```


# Motivation and Background {#sec:intro}


Many scientific experimental results cannot be successfully repeated or reproduced, leading to the so-called "reproducibility crisis" [@baker16:reproducibility; @van15:sluggish].
An experimental result is not fully established unless it can be independently reproduced [@stodden08:legal], and an important step towards this goal is the release and possibly audit of artifacts associated with the work, including computer code [@ACM20:artifact; @fehr16:best].
The availability of experimental artifacts is not only crucial for reproducibility, but also contributes to the transparency, reusability, and credibility of the work [@feitelson15:repeatability].
Artifacts additionally play an important role in open-science initiatives, which have gained substantial momentum in computer science [@heumuller20:publish].

Unfortunately, the sharing and auditing of artifacts is still not as commonplace as warranted by their importance.
In particular, in the field of computer systems, where we might expect that many software-based experiments should be both easy and imperative to share and reproduce [@ince12:case], we find instead that many artifacts are not readily available or buildable [@collberg16:repeatability; @freire12:computational; @heumuller20:publish; @krishnamurthi15:real].
Some of the reasons that researchers struggle to reproduce experimental results and reuse research code from scientific papers are the continuously changing software and hardware, lack of common APIs, stochastic behavior of computer systems and a lack of a common experimental methodology [@fursin21:collective], as well as copyright restrictions [@stodden08:legal].

One challenge in addressing this topic is that definitions and expectations for research artifacts are not always clear.
The Association of Computing Machinery (ACM) defines a paper's artifact as follows:

> By "artifact" we mean a digital object that was either created by the authors to be used as part of the study or generated by the experiment itself. For example, artifacts can be software systems, scripts used to run experiments, input datasets, raw data collected in the experiment, or scripts used to analyze results.  [@ACM20:artifact]

The quality, standards, and expectations for artifacts still vary greatly [@hermann20:community], and many artifacts submissions do not meet even the lowest expectation bar [@dahlgren19:getting].
In an attempt to standardize expectations for computer-science artifacts, the ACM describes its initiative to specifically evaluate artifact availability and functionality in some of its journals and conferences, and decorate papers with badges accordingly [@ACM20:artifact].
One goal of this standardized artifact evaluation process is to send authors and readers the message that artifacts are valued and are an important contribution.
It also provides valuable author feedback [@krishnamurthi15:real].

There are some encouraging signs that such initiatives are leading to increased adoption.
Other conferences in computer systems are also increasingly encouraging artifact sharing, including many conferences in the dataset we analyze here [@childers17:artifact; @dahlgren19:getting; @fursin19:artifact; @heumuller20:publish; @saucez19:evaluating].
An interesting question then emerges, what are some of the effects of artifact sharing in computer systems that we can observe and measure?

The primary effect we are interested in is citations.
In addition to their benefits for open and reproducible science, papers with linked artifacts have been sometimes associated with higher citation counts [@childers17:artifact; @heumuller20:publish].
Citations are not only a widely used metric of influence for papers and researchers, but also an indirect measure of the work's quality and usefulness, which presumably are both helped by the availability of artifacts.
Citations may also stand in as a proxy metrics for the transparency, reusability, reproducibility, and credibility of papers, if we make the assumption that any these qualities encourage subsequent researcher to cite the original work.
The main goal of this paper is therefore to evaluate the association between artifacts availability as a predictor variable and citations as an outcome variable in the research field of computer systems.

With this expansive dataset, we can broaden the scope of our exploration to uncover more relationships between artifact properties and other paper properties.
Despite being primarily exploratory in nature, this paper still poses and answers  as secondary contributions a few concrete research questions, inspired by similar studies:

1. What is the ratio of papers in computer systems for which artifacts are available?
2. How many of these artifacts are actually linked from the paper?
3. How many of these artifacts have expired since publication? What characterizes these artifacts?
4. What are the per-conference factors and differences that affect the ratio of artifact sharing?
5. In particular, does conference prestige affect artifact availability?
5. What is the relationship between artifact accessibility and paper accessibility?
6. What is the relationship between artifact accessibility and paper awards?
7. Are there any textual properties of the paper that can predict artifact availability?

As a final, but not insignificant contribution, this study provides a rich dataset of papers, tagged with varied metadata from multiple sources, including for the first time artifact properties.
Since comprehensive data on papers with software artifacts is not always readily available, because of the significant manual labor involved, this dataset can serve as the basis of additional studies.

The rest of this paper is organized as follows. The next section presents in detail the dataset, methodology, and limitations of our study.
An extensive set of descriptive and explanatory statistics is presented in the results section.
The discussion section that follows dives deeper into aspects of accessibility and the confounding factors that also affect paper citations.
Finally, the concluding section summarizes the main findings of this study and enumerates some future research directions.


# Data and Methods {#sec:data}

The main challenge in this study was the collection and cleaning of the data, especially artifact data.
It is therefore worthwhile to describe the data in detail to support replication and reproductions of this study.

```{r sys-confs, echo=F, message=F, warning=F, cache=T}
tmp <- all_confs %>%
  mutate(Conference = gsub("_\\d*", "", conference)) %>%
  rename(Date = postdate, Papers = npapers, Authors = authors_num, Country = country) %>%
  mutate(Acceptance = round(acceptance_rate, 2)) %>%
  dplyr::select(Conference, Date, Papers, Authors, Acceptance) %>%
  arrange(Conference)

cbind(tmp[1:(nrow(tmp)/2),], tmp[(1+nrow(tmp)/2):nrow(tmp),]) %>%
  knitr::kable(booktabs = T,
               align = c("l", "c", "r", "r", "r"),
               caption = "System conferences, including start date, number of published papers, total number of named authors, acceptance rate, and country code.") %>%
  kable_styling(font_size = 8, latex_options = "hold_position")
```

The primary dataset we analyze comes from a hand-curated collection of `r nrow(all_confs)` peer-reviewed systems conferences from a single publication year (2017).
In CS, and in particular in its more applied fields such as systems, original scientific results are typically first published in peer-reviewed conferences [@patterson99:evaluating; @patterson04:health], and then possibly in archival journals, sometimes years later [@vrettas15:conferences].
The conferences we selected include some of the most prestigious systems conferences (based on indirect measurements such as Google Scholar's metrics), as well as several smaller or less-competitive conferences for contrast (Table \@ref(tab:sys-confs)).
To reduce time-related variance, we chose to focus on a large cross-sectional set of conferences from a single publication year.
Our choice of which conferences belong to "systems" is necessarily subjective.
Not all systems papers from 2017 are included in our set, and some papers that are in our set may not be universally considered part of systems (for example, if they lean more towards algorithms or theory).
Nevertheless, we believe that our cross-sectional set is both wide enough to represent the field well and focused enough to distinguish it from the rest of CS.
In total, our sample includes `r sum(all_confs$npapers)` peer-reviewed papers.

In addition to artifact information, we gathered various statistics on each conference, either from its web page, proceedings, or directly from its chairs.^[The complete descriptions of the full dataset can be found in the various README files in the data directory of the repository and supplementary materials.]
We collected data about review policies, important dates, the composition of its technical PC, and the number of submitted papers, among others.
We also collected historical conference metrics from the Institute of Electrical and Electronics Engineers (IEEE), Association for Computing Machinery (ACM), and Google Scholar (GS) websites, including past citations, age, and total publications, and downloaded all papers in PDF format.
The dataset also includes extensive data on the authors and the textual properties of the papers, and the relevant features are discussed in the next section.

We are mainly interested in the effect of software artifacts on citation of their associated papers.
Citation metrics typically lag publication by a few months or years, allowing for the original papers to be discovered, read, cited, and then the citations themselves published and recorded.
The time duration since these papers had been published, approximately four years, permits the analysis of their short-to-medium term impact in terms of citations.
In practice, this duration is long enough that only
`r top_cites <- group_by(citations, key) %>% summarize(.groups = "drop", max = max(citations)); sum(top_cites$max == 0)` papers
(`r pct(sum(top_cites$max == 0), nrow(top_cites), 2)`%)
had not been cited at all at the time of this writing, while several others already garnered hundreds of citations.
(This paper does not address the question of how citations decay over the longer term, especially as artifacts expire over time, but this dataset could be clearly expanded to this end in the future.)

For this study, the most critical piece of information on these papers is their artifacts.
Unfortunately, the papers included no standardized metadata with artifact information, so it had to be collected manually from various sources, as detailed next.

The only form of standardized artifact metadata was found for the subset of conferences organized by the ACM with artifact badge initiatives.
In the proceedings page in the ACM's digital library of these conferences, special badges denote which papers made artifacts available, and which papers had artifacts evaluated (for conferences that supported either badge).
In addition, the ACM digital library also serves as a repository for the artifacts (as supplementary information), and all of these ACM papers included a link back to the appropriate web page with the artifact.

Unfortunately, most papers in this dataset were not published by the ACM or had no artifact badges.
These papers required a manual scanning of the PDF text of each paper in order to identify such links.
Several search terms were used to assist in this search, such as "github,"  "gitlab," "bitbucket," "sourceforge," and "zenodo" for repositories; variants of "available," "open source," and "download" for links; and  variations of "artifact," "reproducibility," and "will release" for indirect references.
Moreover, the artifacts for numerous papers could actually be discovered online despite no mention in the original paper, by searching github.com for author names, paper titles, or system monikers.

In all, `r nrow(artifacts)` papers in our dataset
(`r pct(nrow(artifacts), nrow(papers), 2)`%)
had an identifiable or promised artifact, primarily source code but occasionally data, configuration, or benchmarking files.
Artifacts that had been included in previous papers or written by someone other than the paper's authors were excluded from this count.
Anecdotally, most of the source-code repositories in these artifacts showed no development activity---commits, forks, or issues---after the publication of their paper.

For each one of these papers/artifacts, the following fields were recorded in the paper's record in the dataset:

 * Whether the paper had an "artifact available" badge.
 * Whether it had an "artifact evaluated" badge.
 * Whether a link to the artifact was clearly available in the paper.
 * The URL for the artifact, whether linked to in the paper, or found elsewhere.
 * The last date that this artifact was still found intact online.

All of the searches for these artifacts are recent, so from the last field above we can denote the current status of an artifact as either "extant" or "expired".
From the availability of a URL we can classify an artifact as "released" or "unreleased" (the latter denoting papers that promised an artifact but no link or repository was found).
And from the host domain of the URL we can classify the location of the artifact as either an "Academic" web page, the "ACM" digital library, a "Filesharing" service such as Dropbox or Google, a specialized "Repository" such as Github.com, "Other" (including .com and .org web sites), or "NA".
The records for all these artifacts are summarized in the file `features/artifacts.csv` in the supplementary information.

The final relevant piece of the dataset is the conferences' attitude toward artifacts.
A few conferences, such as MobiCom and SLE, specifically encouraged artifacts in the call-for-papers or web sites.
Four conferences---SC, OOPSLA, PLDI, SLE---archived their artifacts in the ACM's digital library.
In addition to general encouragement and archival, six conferences specifically offered to evaluated artifacts by a program committee: OOPSLA, PACT, PLDI, PPoPP, SC, and SLE.

## Limitations {-}

The methodology described here is primarily constrained by the manual curation of data, especially artifact data.
The effort involved in compiling all the necessary data limits the scalability of this approach to additional conferences or years.
Furthermore, the manual search for artifacts in the text and in repositories is a laborious process and prone to human error.
Although a large-enough number of artifacts was identified for statistical analysis,
there likely remain untagged papers in the dataset that did actually release an artifact (false negatives).
Nevertheless, there is no evidence to suggest that their number is large or that their distribution is skewed in some way as to bias statistical analyses.
That said, since the complete dataset is (naturally) released as an artifact of this paper, it can be enhanced and corrected over time.

## Ethics statement {-}

All of the data for this study was collected from public online sources and therefore did not require the informed consent of the papers' authors.
The cleaned and summarized data itself is also made fully available to the research community.

## Statistics {-}

For statistical testing, group means were compared pairwise using Welch's two-sample t-test; differences between distributions of two categorical variables were tested with $\chi^{2}$ test; and comparisons between two numeric properties of the same population were evaluated with Pearson's product-moment correlation. All statistical tests are reported with their p-values.


# Results {#sec:results}



## Descriptive statistics

We start with simple characterization of the statistical distributions of artifacts in our dataset.
Of the `r nrow(artifacts)` papers with artifacts, we find that about
`r pct(sum(artifacts$linked), nrow(artifacts))`%
included an actual link to the artifact in the text.
The ACM digital library marked
`r sum(artifacts$badge)` papers (`r pct(sum(artifacts$badge), nrow(artifacts))`%)
with an "Artifact available" badge, and
`r sum(artifacts$evaluated)` papers (`r pct(sum(artifacts$evaluated), nrow(artifacts))`%)
with an "Artifact evaluated" badge.
The majority of artifact papers
(`r pct(sum(!artifacts$expired), nrow(artifacts))`%)
still had their artifacts available for download at the time of this writing.
This ratio is somewhat similar to a comparable study that found that 73% of URLs in five open-access journals after five years [@saberi12:accessibility].
Of the `r nrow(artifacts)` papers that promised artifacts, `r sum(artifacts$unreleased)` appear to have never released them.
The distribution of the location of the accessible artifacts is shown in Table \@ref(tab:location-dist), and is dominated by Github repositories.

```{r location-dist, echo=F}
artifacts %>%
  mutate(Location = factor(location, levels = c("Repository", "Academic", "Other", "ACM", "Filesharing", NA))) %>%
  group_by(Location) %>%
  summarize(Count = n()) %>%
  knitr::kable(booktabs = T,
               align = "lr",
               caption = "Class of artifact URLs. `NA' locations indicate expired or unreleased URLs.")
```

```{r artifact-ratios, warning=F, message=F, fig.height = 7, out.width="95%", fig.cap="Papers with artifact by conference."}
by_conf <- by_paper %>%
  group_by(conf) %>%
  summarize(ratio = sum(!is.na(expired)) / n(), n = n()) %>%
  left_join(all_confs, by = c("conf" = "key")) %>%
  arrange(desc(ratio)) %>%
  mutate(org = factor(org, levels = c("ACM", "IEEE", "USENIX", "other")))

ggplot(by_conf, aes(x = reorder (conf, ratio), y = 100 * ratio, fill = org)) +
  geom_bar(stat = "identity") +
  geom_text(aes(y = 95, label = paste0("n=", n), color = org), hjust = 0, size = 3, show.legend = F) +
  coord_flip() +
  theme_minimal() +
  ylim(0, 100) +
  xlab("Conference") +
  ylab("Percent of paper with artifacts") +
  guides(fill = guide_legend("Organization")) +
  scale_color_manual(values = c(cbp1[2:4], cbp1[1])) +
  scale_fill_manual(values = c(cbp1[2:4], cbp1[1])) +
  theme(legend.position = "bottom",
        axis.text.y = element_text(angle = 0, hjust = 1),
        panel.grid.major.y = element_blank())
```

Looking at the differences across conferences, Fig. \@ref(fig:artifact-ratios) shows the percentage of papers with artifacts per conference, ranging from 0% to OOPSLA's
`r round(100 * by_conf[1,]$ratio, 2)`%
(mean: `r round(100 * mean(by_conf$ratio), 2)`%,
SD: `r round(100 * sd(by_conf$ratio), 2)`%).
Unsurprisingly, nearly all of the conferences where artifacts were evaluated are prominent in their relatively high artifact rates.
Only MobiCom stands out as a conference that evaluated artifacts for the "best community paper award" but had a low overall ratio of papers with artifacts.

It should be noted, however, that many papers in MobiCom are hardware-related, where artifacts are typically unfeasible.
The same is true for a number of other conferences with low artifact ratios, such as ISCA, HPCA, and MICRO.
Also worth noting is the fact that ACM conferences appear to attract many more artifacts than IEEE conferences, although the reasons likely vary on a conference-by-conference basis.

Another indicator for artifact availability is author affiliation.
As observed in other systems papers, industry-affiliated authors typically face more restrictions for sharing artifacts [@collberg16:repeatability], likely because the artifacts hold a commercial potential or competitive advantage [@ince12:case].
In our dataset, only
`r pct(nrow(filter(by_paper, artifact_released, all_com)), nrow(filter(by_paper, all_com)))`%
of the
`r nrow(filter(by_paper, all_com))`
papers where all authors had an industry affiliation also released an artifact, compared to
`r pct(nrow(filter(by_paper, artifact_released, !all_com)), nrow(filter(by_paper, !all_com)))`%
for the other papers
(`r report_test(chisq.test(table(by_paper$all_com, by_paper$artifact_released)))`).


Without directly comparable information on artifact availability rates in other fields, it is impossible to tell whether the overall rate in our dataset,
`r pct(nrow(with_ar), nrow(by_paper))`%,
is high or low.
However, within the six conferences which evaluated artifacts,
`r evald <- filter(by_paper, conf %in% c("OOPSLA", "PACT", "PLDI", "PPoPP", "SC", "SLE")); pct(sum(evald$artifact_released), nrow(evald), 2)`%
of papers released an artifact, a very similar rate to the $\approx{40}$% rate found in a smaller study of systems conferences with an artifact evaluation process [@childers17:artifact].

In general, skimming the papers in our dataset revealed that many "systems" papers do in fact describe the implementation of a new computer system, mostly in software.
It is plausible that the abundance of software systems in these papers and the relative ease of releasing them as software artifacts contributes directly to this sharing rate, in addition to conference-level factors.



## Relationships to citations

The main research question of this paper is, does the open availability of an artifact affect the citations of a paper in computer systems?
To answer this question, we look at the distribution of citations for each paper exactly `r how_long` months after it was published, to normalize the comparison.
Figure \@ref(fig:cite-hist) shows the overall paper distribution as a histogram, while Fig. \@ref(fig:cite-dist) breaks down the distributions of artifact and non-artifact papers, as density plots.

```{r cite-hist, echo = F, warning = F, message = F, fig.asp = 0.6, fig.cap = paste("Distribution of paper citations", how_long, "months after publication (log-scale)")}
ggplot(by_paper, aes(x = citations)) +
  geom_histogram(color = "black", fill = "blue", alpha = 0.3) +
#  stat_density(aes(y = ..count..), color = "black", fill = "blue", alpha = 0.3) +
  scale_x_continuous(breaks = c(0, 1, 2, 3, 5, 10, 20, 30, 50, 100, 200, 300, 500, 1000, 2000), trans = "log1p", expand = c(0, 0)) +
#  geom_histogram(stat = "count") +
  theme_minimal()
#  scale_x_log10() #breaks = c(1, 2, 11, 101, 1001, 2001), labels = c(0, 1, 10, 100, 1000, 2000))
```

```{r cite-dist, echo = F, warning = F, message = F, fig.asp = 0.6, fig.cap = paste("Density plot of paper citations", how_long, "months after publication (log-scale)")}
ggplot(by_paper, aes(x = citations, color = artifact_released)) +
  geom_density() +
  theme_minimal() +
  theme(legend.position = "bottom") +
  guides(color = guide_legend(title = "Artifact released")) +
  scale_color_manual(values = c("darkgreen", "orange")) +
  scale_x_log10()
```


Citations range from none at all
(`r nrow(filter(by_paper, citations == 0))` papers) to about 1000, with two outlier papers exceeding 2000 citations [@carlini17:towards; @jouppi17:datacenter].
The distributions appear roughly log-normal.
The mean citations per paper with artifacts released was
`r round(mean(with_ar$citations), 1)`,
compared to
`r round(mean(without_ar$citations), 1)` with none
(`r report_test(t.test(with_ar$citations, without_ar$citations))`).
Since the citation distribution is so right-skewed, it makes sense to also compare the median citations with and without artifacts
(`r median(with_ar$citations)` vs. `r median(without_ar$citations)`, `r report_test(wilcox.test(with_ar$citations, without_ar$citations))`).
Both statistics suggest a clear and statistically significant advantage in citations for papers that released an artifact.
Likewise, the `r nrow(released)` papers that actually released an artifact garnered more citations than the `r nrow(unreleased)` papers that did promise an artifact that could later not be found
(`r report_test(t.test(released$citations, unreleased$citations))`),
and extant artifacts fared better than expired ones
(`r report_test(t.test(unexpired$citations, expired$citations))`).

In contradistinction, some positive attributes of artifacts made them actually less cited.
For example, the mean citations of the
`r nrow(with_ar_l)`
papers with a linked artifact,
`r round(mean(with_ar_l$citations), 1)`,
was much lower than the
`r round(mean(with_ar_nl$citations), 1)`
mean for the `r nrow(with_ar_nl)` papers with artifacts we found using a Web search
(`r report_test(t.test(with_ar_l$citations, with_ar_nl$citations))`;
`r report_test(wilcox.test(with_ar_l$citations, with_ar_nl$citations))`).
Curiously, the inclusion of a link in the paper, presumably making the artifact more accessible, was associated with fewer citations.

Similarly counter-intuitive, papers who received an "Artifact evaluated" badge fared worse in citations than artifact papers who did not
(`r report_test(t.test(with_ar_e$citations, with_ar_ne$citations))`;
`r report_test(wilcox.test(with_ar_e$citations, with_ar_ne$citations))`).
Papers who received an "Artifact available" badge did not fare significantly worse than artifact papers who did not
(`r report_test(t.test(with_ar_b$citations, with_ar_nb$citations))`;
`r report_test(wilcox.test(with_ar_b$citations, with_ar_nb$citations))`).
These findings appear to contradict the premise that such badges are associated with increased artifact sharing, as has been found in other fields [@baker16:digital].

Finally, we can also break down the citations per paper grouped by the type of location for the artifact and by organization, looking at medians instead because of the outsized effects of outliers (Table \@ref(tab:location-cites)).
The three major location categories do not show significant differences in median citations, and the last two categories may be too small to ascribe statistical significance to their differences.


```{r location-cites, echo=F}
with_ar %>%
  drop_na(location) %>%
  mutate(Location = factor(location, levels = c("Repository", "Academic", "Other", "ACM", "Filesharing"))) %>%
  group_by(Location) %>%
  summarize("Median citations" = round(median(citations), 0)) %>%
  knitr::kable(booktabs = T,
               align = "lr",
               caption = "Median citations by class of artifact URLs for extant artifacts")
```


```{r org-cites, echo=F, eval=F}
with_ar %>%
  drop_na(org) %>%
  mutate(Organization = factor(org, levels = c("ACM", "IEEE", "USENIX", "other"))) %>%
  group_by(Organization) %>%
  summarize("Median citations" = round(median(citations), 0)) %>%
  knitr::kable(booktabs = T,
               align = "lr",
               caption = "Median citations by class of artifact URLs for extant artifacts")
```

A related question is how often do systems papers cite other artifacts.
Unfortunately, there is no simple automated way to answer this question, and a careful reading of all `r nrow(papers)` papers is impractical.
As a crude approximation, a simple search for the string "github" in the full-text of all the papers yielded 900 distinct results.
Keep in mind, however, that perhaps half of those could be referring to their own artifact rather than another paper's, and that not all cited github repositories do indeed represent paper artifacts.
Incidentally, papers with released artifacts also tend to average significantly more references of their own
(`r report_means("references")`),
but there is no reason to suspect a causal relationship to artifacts, as opposed to some other confounding cause.
We dive deeper into questions of association and causality with citations in the discussion section.

## Covariate analysis

Having addressed the relationships between artifacts and citations, we can now explore relationships with additional variables from this expansive dataset.



### Awards

Many conferences present competitive awards, such as "best paper," "best student paper," "community award," etc.
Of the `r nrow(by_paper)` total papers,
`r pct(sum(by_paper$award), nrow(by_paper))`% received at least one such award.
Papers with artifacts are disproportionately represented in this exclusive subset
(`r pct(nrow(filter(with_ar, award)), nrow(filter(by_paper, award)), 1)`% vs.
`r pct(nrow(filter(with_ar, !award)), nrow(filter(by_paper, !award)), 1)`% in non-award papers;
`r report_test(chisq.test(table(by_paper$award, by_paper$artifact_released)))`).

Again, it is unclear whether this relationship is causal, since the two covariates are not entirely independent.
A handful of awards specifically evaluated the contribution of the paper's artifact.
Even if the relationship is indeed causal, its direction is also unclear, since
`r pct(nrow(filter(with_ar, award, !linked)), nrow(filter(with_ar, award)), 1)`%
of award papers with artifact did not link to it in the paper.
It is possible these papers released their artifacts after winning the award or because of it.

### Textual properties

Some of the textual properties of papers can be estimated from their full text using simple command-line tools.
Our dataset includes three such properties: the length of each paper in words, the number of references it cites, and the existence of a system's name in the paper's title.

The approximate paper length in words and the number of references turn out to be positively associated with the release of an artifact.
Papers with artifacts average **more pages** than papers without
(`r report_means("mean_pages")`),
**more words**
(`r report_means("words")`),
and **more references**
(`r report_means("references")`).
Keep in mind, however, that longer papers also correspond to more references
(`r report_test(cor.test(by_paper$words, by_paper$references))`),
and are further confounded with specific conference factors such as page limits.

As previously mentioned, many systems papers introduce a new computer system, often as software.
Sometimes, these papers name their system by a moniker, and their title starts with the moniker, followed by a colon and a short description (e.g., "Widget: An Even Faster Key-Value Store").
This feature is easy to extract automatically for all paper titles.

We could hypothesize that a paper that introduces a new system, especially a named system, would be more likely to include an artifact with the code for this system, quite likely with the same repository name.
Our data supports this hypothesis.
The ratio of artifacts released in papers with a labeled title,
`r pct(nrow(filter(with_ar, labeled_title)), nrow(filter(by_paper, labeled_title)))`%,
is nearly double that of papers without a labeled title,
`r pct(nrow(filter(with_ar, !labeled_title)), nrow(filter(by_paper, !labeled_title)))`%
(`r report_test(chisq.test(table(by_paper$artifact_released, by_paper$labeled_title)))`).

These textual relationships may not be very insightful, because of the difficulty to ascribe any causality to them, but they can clue the paper's reader to the possibility of an artifact, even if one is not linked in the paper.
Indeed, they accelerated the search for such unlinked artifacts during the curation of the data for this study.


### Conference prestige

Finally, we look at conference-specific covariates that could represent how well-known or competitive a conference is.
In addition to textual conference factors, these conference metrics may also be associated with higher rates of artifact release.

Several proxy metrics for prestige appear to support this hypothesis.
Papers with released artifacts tend to appear in conferences that average a **lower acceptance rate**
(`r report_means("acceptance_rate")`),
**more paper submissions**
(`r report_means("submissions")`),
**higher historical mean citations per paper**
(`r report_means("mean_historical_citations")`),
and a **higher h5-index** from GS metrics^[\url{https://scholar.google.com/citations?view_op=top_venues}]
(`r report_means("h5_index")`).
Also note that papers in conferences that offered some option for author response to peer review (often in the form of a rebuttal) were slightly more likely to include artifacts, perhaps as a response to peer review
(`r report_test(chisq.test(table(by_paper$artifact_released, by_paper$rebuttal)))`).

To explain these relationships, we might hypothesize that a higher rate of artifact submission would be associated with more reputable conferences, either because artifact presence contribute to prestige, or because more rigorous conferences are also more likely to expect such artifacts.
Observe, however, that some of the conferences that encourage or require artifacts are not as competitive as the others.
For example, OOPSLA, with the highest artifact rate, had an acceptance rate of 0.3, and SLE, with the fourth-highest artifact rate, had an acceptance rate of 0.42.
The implication here is that it may not suffice for a conference to actively encourage artifacts for it to be competitive, but a conference that already is competitive may also attract more artifacts.


# Discussion {#sec:discussion}


## Accessibility

One commonly used set of principles to assess research software artifacts is termed FAIR: findability, accessibility, interoperability, and reusability [@katz21:fresh; @wilkinson16:fair].
We have overviewed the findability aspect of artifacts in the statistics of how many of these were linked or found via a Web search.
The reusability and interoperability of artifacts unfortunately cannot be assessed with the data available to us.
However, we can analyze the accessibility of artifacts in much more detail and depth.

As mentioned previously,
`r pct(nrow(filter(by_paper, expired)), nrow(filter(by_paper, !is.na(expired))))`%
of released artifacts are already inaccessible, a mere $\approx{3.5}$ years after publication.
Most of the artifacts in our dataset were published in code repositories, predominantly github, that do not guarantee persistent access or even universal access protocols such as digital object identifiers (DOI).
However, only
`r pct(nrow(filter(by_paper, expired, location == "Repository")), nrow(filter(by_paper, !is.na(expired), location == "Repository")))`%
of the "Repository" artifacts were inaccessible.
In contrast,
`r pct(nrow(filter(by_paper, expired, location == "Academic")), nrow(filter(by_paper, !is.na(expired), location == "Academic")))`%
of the artifacts in university pages have already expired, likely because they had been hosted by students or faculty that have since moved elsewhere.
Also, a full
`r pct(nrow(filter(by_paper, expired, location == "Filesharing")), nrow(filter(by_paper, !is.na(expired), location == "Filesharing")))`%
of the artifacts on file-sharing sites such as Dropbox or Google Drive are no longer there, possibly because these are paid services or free to a limited capacity, and can get expensive to maintain over time.

Accessibility is also closely related to the findability of the artifact, which in the absence of artifact DOIs in our dataset, we estimate by looking at the number of papers that explicitly link to their artifacts.
The missing (expired) artifacts consisted of a full
`r pct(nrow(filter(by_paper, expired, !linked)), nrow(filter(by_paper, !is.na(expired), !linked)))`%
of the papers with no artifact link, compared to only
`r pct(nrow(filter(by_paper, expired, linked)), nrow(filter(by_paper, !is.na(expired), linked)))`%
for papers that linked to them
(`r report_test(chisq.test(table(by_paper$expired, by_paper$linked)))`).

Another related question to artifact accessibility is how accessible is the actual paper that introduced the artifact, which may itself be associated with higher citations [@gargouri10:self; @mccabe15:availability; @mckiernan16:point; @tahamtan16:factors].
A substantial proportion of the papers
(`r pct(sum(by_paper$open_access), nrow(by_paper))`%)
were published in `r sum(all_confs$open_access)` open-access conferences.
Other papers have also been released openly as preprints or via other means.
One way to gauge the availability of the paper's text is to look it up on GS and see if an accessible version (eprint) is linked, which is recorded in our dataset.
Of the `r nrow(papers)` papers,
`r pct(sum(!is.na(by_paper$months_to_eprint)), nrow(by_paper))`%
displayed at some point an accessible link to the full text on GS.
Specifically, of the papers that released artifacts,
`r pct(nrow(filter(with_ar, !is.na(months_to_eprint))), nrow(with_ar))`%
were associated with an eprint as well, compared to
`r pct(nrow(filter(without_ar, !is.na(months_to_eprint))), nrow(without_ar))`%
of the papers with no artifacts
(`r report_test(chisq.test(table(by_paper$artifact_released, is.na(by_paper$months_to_eprint))))`).


Moreover, our dataset includes not only the availability of a fulltext link on GS, but also the approximate duration since publication (in months) that it took GS to display this link, offering a quantitative measure of accessibility speed as well.
It shows that for papers with artifacts, GS averaged approximately
`r round(mean(with_ar$months_to_eprint, na.rm = T), 1)`
months post-publication to display a link to an eprint, compared to
`r round(mean(without_ar$months_to_eprint, na.rm = T), 1)`
months for papers with no artifacts
(`r report_test(t.test(with_ar$months_to_eprint, without_ar$months_to_eprint))`).
Both these qualitative and quantitative differences are statistically significant, but keep in mind that the accessibility of papers and artifacts are not independent: some conferences that encouraged artifacts were also open-access, particularly those with the ACM.
Another dependent covariate with accessibility is citations; several studies suggested that accessible papers are better cited [@bernius09:open; @niyazov16:open; @snijder16:revisiting], although others disagree [@calver10:patterns; @davis11:impact].
This dependence may explain part of the higher citability of papers with artifacts, as elaborated next.

## Citation confounders

We observed some strong associations between artifact release and paper attributes such as: citations, length, etc.
Comparing citations across groups with and without artifacts as computed in the previous section is informative, but not necessarily indicative of a causal relationship.
As cautioned throughout this study, these associations are insufficient to draw strong causal conclusion, primarily because there are many confounding variables, most of which related to the publishing conference.
These confounding factors could provide partial or complete explanation to differences in citations beyond artifact availability.

In other words, papers published in the same conference might exhibit strong correlations that interact or interfere with the attribute we are trying to compare to artifact release.
We can attempt to control for these confounding variables by controlling for conference when evaluating associations using a multilevel model.

One obvious factor affecting citations is time since publication, which we control for by measuring all citations at exactly the same interval, `r how_long` months since the conference's official start.
Another crucial factor is the field of study, which we normalize for by focusing on a single field, while providing a wide cross-section of the field to limit the effect of statistical variability.

There are also numerous paper-related factors that have shown positive association with citations, such as: review papers, fewer equations, number of references, statistically significant positive results, length of papers, number of figures and images, and even more obscure features such as the presence of punctuation marks in the title.
We can control for the paper-related features we collected by building a simple linear regression model of citations as a function of artifact availability, and then add control variables as predictor variables and observe the effect on the main predictor.
Because of the right skew of the citation distribution, the response variable we examine is instead `r epsilon = 0` $ln(citations)$, omitting the `r sum(by_paper$citations == 0)` papers with zero citations.

`r clean <- filter(by_paper, citations > 0); model0 <- lm(data = clean, log(citations + epsilon) ~ artifact_released)`
In the simplest form, fitting a linear model of the log-transformed citations as a function of artifact released yields an intercept (baseline citations) of
`r round(coef(model0)[1], 2)`
and a slope of
`r round(coef(model0)[2], 2)`,
meaning that releasing an artifact adds approximately
`r round(100 * (exp(coef(model0)[2]) - 1), 0)`%
more citations to the paper, after exponentiation.
The p-value for this predictor is exceedingly low (less than $2\times10^{-16}$)
but the simplistic model only explains
`r round(100 * summary(model0)$adj.r.squared, 2)`%
of the variance in citations
(Adjusted $R^2$=`r round(summary(model0)$adj.r.squared, 3)`).
The Bayesian Information Criterion (BIC) for this model is
`r format(BIC(model0), scientific=F)`,
with
`r summary(model0)$df[2]` degrees of freedom (df).

We can now add various paper covariates to the linear model in an attempt to get more precise estimates for the artifact released predictor, by iteratively experimenting with different predictor combinations to minimize BIC using stepwise model selection [@garcia-portugues21:modeling, Ch. 3].
The per-paper factors considered were: **paper length** (words), **number of coauthors**, **accessibility speed** (months to eprint^[Papers with no eprint available at the time of this writing were given a time to eprint of 1,000 months, which turned out to be insignificant for the regression analysis.]), **number of references**, **colon in the title**, and **award** given.

```{r model-BIC1, echo=F, message=F, warning=F, cache=T, results = 'hide'}
cleaner <- clean %>%
  mutate(months_to_eprint = ifelse(is.na(months_to_eprint), 1000, months_to_eprint))

model1 <- lm(data = cleaner, log(citations + epsilon) ~ artifact_released + words + months_to_eprint + references + nauthors + award + subtitle)
modelBIC1 <- stepAIC(model1, k = log(nrow(cleaner)))
```

```{r BIC1-params, echo=F, message=F, warning=F, cache=T, eval=F}
tmp <- coef(summary(modelBIC1))
tbl <- data.frame(Factor = c("Intercept", "Artifact released", "Word count", "Months to eprint", "References number", "Coauthors number", "Colon in title"),
                  Coefficient = round(tmp[,1], 5),
                  "p-value" = format(tmp[,4], format = "e", digits = 2),
                  check.names = F)
rownames(tbl) <- NULL
tbl %>%
  knitr::kable(booktabs = T, linesep = "", align = c("l", "r", "r"),
               caption = "Estimated parameters for multilevel model of ln(citations) by paper factors")
```


It turns out that all these paper-level factors except award availability have a statistically significant effect on citations, which brings the model to an increased adjusted $R^2$ value of
`r round(summary(modelBIC1)$adj.r.squared, 3)`
and a BIC of
`r round(BIC(modelBIC1), 2)` (df = `r summary(modelBIC1)$df[2]`).
However, the coefficient for artifact released went down to
`r round(coef(summary(modelBIC1))[2,1], 2)`
(`r round(100 * (exp(coef(modelBIC1)[2]) - 1), 0)`% relative citation increase)
with an associated p-value of
`r round(coef(summary(modelBIC1))[2,4], 14)`.

```{r modelBIC2, cache = T, echo = F, message = F, results = 'hide'}
cleanest <- cleaner %>%
  drop_na(total_npubs, max_h, all_com, same_country, is_usa, is_woman, top_university, top_company)
model2 <- lm(data = cleanest, log(citations + epsilon) ~ artifact_released + words + months_to_eprint + references + nauthors + subtitle + total_npubs + max_h + all_com + same_country + is_usa + is_woman + top_university + top_company)
modelBIC2 <- stepAIC(model2, k = log(nrow(cleanest)))
```

Similar to paper variables, some author-related factors such as their own reputation, author country, and their gender has been associated with citation count [@tahamtan16:factors].
We enhance our linear model with the following metrics (omitting `r nrow(cleaner) - nrow(cleanest)` papers with NA values):

 * Whether all the coauthors with a known affiliation came from the same country, as international collaborations can be associated with higher citations [@puuska14:international].
 * Is the lead author affiliated with the United States, which may also lead to higher citations [@gargouri10:self; @peng12:where].
 * Whether any of the coauthors was affiliated with one of the top 50 universities per [www.topuniversities.com](www.topuniversities.com)  (27% of papers) or a top company (if any author was affiliated with either (Google, Microsoft, Yahoo!, or Facebook: 18% of papers), based on the definitions of a similar study [@tomkins17:reviewer].
 * Whether all the coauthors with a known affiliation came from industry.
 * The gender of the first author [@frachtenberg21:whpc].
 * The sum of the total past publications of all coauthors of the paper [@bjarnason02:nordic].
 * The maximum h-index of all coauthors [@hurley13:deconstructing].

Only the maximum h-index and top-university affiliation had statistically significant coefficients, but hardly affected the overall model.^[Note that the last two metrics are correlated
(`r  report_test(cor.test(cleaner$max_h, cleaner$total_npubs))`),
so one may cancel the other out.]
These minimal changes likely do not justify the cost in increased complexity and reduced data size (because of missing data), so for the remainder of the analysis we ignore author-related factors and proceed with the previous model.

```{r BIC2-params, echo=F, message=F, warning=F, cache=T, eval=F}
tmp <- coef(summary(modelBIC2))
tbl <- data.frame(Factor = c("Intercept", "Artifact released", "Word count", "Months to eprint", "References number", "Coauthors number", "Colon in title", "Maximum h-index", "Top university"),
                  Coefficient = round(tmp[,1], 5),
                  "p-value" = format(tmp[,4], format = "e", digits = 2),
                  check.names = F)
rownames(tbl) <- NULL
tbl %>%
  knitr::kable(booktabs = T, linesep = "", align = c("l", "r", "r"),
               caption = "Estimated parameters for multilevel model of ln(citations) by paper and author factors")
```


Finally, we add the last level: venue factors.
Conference (or journal) factors, such as the conference's own prestige and competitiveness, can have a large effect on citations, as discussed in the previous section.
Although we can approximate some of these factors with metrics, there may also be other unknown or qualitative conference factors that we cannot model.
To account for conference factors, we next build a mixed-methods model, where all the previously mentioned factors become fixed effects and the conference becomes a random effect [@roback21:beyond, Ch. 8].

```{r modelLMER, cache = T, echo = F, message = F, results = 'hide', warning = F}
modelLMER <- lmer(data = cleaner, log(citations + epsilon) ~ artifact_released + words + months_to_eprint + references + nauthors + subtitle + (1|conference))
```

This last model does indeed reduce the relative effect of artifact release on citations to a coefficient of
`r round(coef(summary(modelLMER))[2, 1], 2)`
(95% confidence interval:
`r round(confint(modelLMER, "artifact_releasedTRUE"), 2)[1]`--`r  round(confint(modelLMER, "artifact_releasedTRUE"), 2)[2]`).
But this coefficient still represents a relative citation increase of about a third for papers with released artifacts
(`r round(100 * (exp(coef(summary(modelLMER))[2, 1]) - 1), 0)`%),
which is significant.
We can approximate a p-value for this coefficient via Satterthwaite’s degrees of freedom method using R's `lmerTest` package [@kuznetsova17:lmertest], which is also statistically significant at
`r round(coef(summary(modelLMER))[2, 5], 14)`.
The parameters for this final model are enumerated in Table \@ref(tab:LMER-params).
The only difference in paper-level factors is that award availability has replaced paper word count as a significant parameter, but realistically, both had a negligible effect on citations.

```{r LMER-params, echo=F, message=F, warning=F, cache=T}
tmp <- coef(summary(modelLMER))
tbl <- data.frame(Factor = c("Intercept", "Artifact released", "Award given", "Months to eprint", "References number", "Coauthors number", "Colon in title"),
                  Coefficient = round(tmp[,1], 5),
                  "p-value" = format(tmp[,5], format = "e", digits = 2),
                  check.names = F)
rownames(tbl) <- NULL
tbl %>%
  knitr::kable(booktabs = T, linesep = "", align = c("l", "r", "r"),
               caption = "Estimated parameters for final multilevel mixed-effects model of ln(citations)")
```

Even with all these controls, we observe a strong association between artifact release and citations.
This association may still not suffice to claim causation [@lewis18:open], but it supports the hypothesis that releasing artifacts can indeed improve the prospects of a systems research paper to achieve wider acceptance, recognition, and scientific impact.

# Conclusion {#sec:conclusion}

Several studies across disparate fields found a positive relationship between the sharing of research artifacts and increased citation of the research work.
In this cross-sectional study of computer systems research, we also observed a strong statistical association between the two, although there are numerous potential confounding and explanatory variables to increased citations.
Still, even when controlling for various paper-related and conference-related factors, we observe that papers with shared artifacts receive approximately a third more citations than papers without.

Citation metrics are a controversial measure of a work's quality, impact, and importance, and perhaps should not represent the sole or primary motivation for authors to share their artifacts.
Instead, authors and readers may want to focus on the clear and important benefits to science in general, and to the increased reproducibility and credibility of their work in particular.
Although there remain many obstacles to universal sharing of artifacts, the field of computer systems does have the advantage that many----if not most---of its artifacts come in the form of software, which is much easier to share than artifacts in other experimental fields.
It is therefore not surprising that we find the majority of shared and extant artifacts in computer systems hosted on github.com, a highly accessible source-code sharing platform.

If increased citation counts are not enough to incent more systems authors to share their artifacts, perhaps conference organizers can leverage their substantial influence to motivate authors.
Although artifact evaluation can represent a nontrivial additional burden on the program committee, our data shows that it does promote higher rates of artifact sharing.
However, increasing the sharing rates is not enough for the goals of reproducible science, since many of the shared artifacts in our dataset have since expired or been difficult to locate.

Our analysis found that both the findability and accessibility of systems artifacts can decay significantly even after only a few years, especially when said artifacts are not hosted in dedicated open and free repositories.
Conference organizers could likely improve both aspects by requiring---and perhaps offering---standardized tools, techniques, and repositories, in addition to the sharing itself.
The ACM has taken significant steps in this direction by not only standardizing various artifact badges, but also offering its own supplementary material repository in its digital library.
A few conferences in our dataset, like SC, are taking another step in this direction by also requesting a standrardized artifact description appendix and review for every technical paper, including a citeable link to the research artifacts.

To evaluate the impact of such efforts, we must look beyond the findability and accessibility of artifacts, as was done in this study.
In future work, this analysis can be expanded to the two remaining aspects of the FAIR principles: interoperability and reusability, possibly by incorporating input from the artifact review process itself.
The hope is that as the importance and awareness of research artifacts grows in computer systems research, many more conferences will require and collect this information, facilitating not only better, reproducible research, but also a better understanding of the nuanced effects of software artifact sharing.

## Acknowledgments {-}

I wish to thank Prof. Kelly McConville of Reed College for her thoughtful and patient assistance with the statistical analysis.
