---
title: Gender differences in collaboration patterns in computer science
bibliography: ../sysconf.bib
simplesummary: |
  Analyzing the differences in collaboration patterns across genders and subfields can
  help us understand and possibly address the factors associated with the
  large gender gap in computer science research.
abstract: |
  The research field of computer science (CS) has a well-publicized gender imbalance.
  Multiple studies estimate the ratio of women among publishing researchers
  to be around 15%--30%. Many explanatory factors have been studied
  in association with this gender gap, including differences in collaboration patterns.
  Here, we extend this body of knowledge by looking at differences in collaboration
  patterns specific to various fields and subfields of CS.
  We curated a dataset of nearly 20,000 unique authors of some 7,000 top conference papers
  from a single year. We manually assigned a field and subfield to each conference,
  and a gender to most researchers. We then measured for each subfield the gender gap
  as well as five other collaboration metrics, which we compared to the gender gap.
  Our main findings are that the gender gap varies greatly by field,
  ranging from 6% female authors in theoretical CS to 42% in CS education;
  Subfields with a higher gender gap also tend to exhibit lower female productivity,
  larger coauthor groups, and higher gender homophily. Although women published fewer
  single-author papers, we did not find an association between single-author papers and the
  ratio of female researchers in a subfield.
keywords: |
  Women in science; gender gap; collaboration patterns; computer science research; bibliometrics
acknowledgements: |
  We gratefully acknowledge the assistance of Alexis Richter, Josh Reiss, and Rhody Kaner in collecting some of the gender data for this study. Some of these students have been supported by grants from the Reed Social Justice Research and Education Fund.
authorcontributions: |
  J.Y collected most of the gender data, performed experiments and analyses, and
  edited the paper; E.F conceived and designed the experiments; performed experiments
  and analysis, and wrote the paper.
conflictsofinterest: |
  The authors declare no conflict of interest.
funding: |
  Funding for this work was generously provided by the Reed College Social Justice Research and Education Fund.
institutionalreview: |
  This study was exempted from the informed consent requirement under Exempt Category 4: the use of secondary data by Reed College's Institutional Review Board (No. 2021-S26).
informedconsent: |
  The data collected for this study was sourced from public-use datasets such as conference and academic web pages. The informed consent requirement was waived for this secondary analysis.
dataavailability: |
  All of the code and data for this article are publicly available at \url{https://github.com/eitanf/sysconf} \cite{frachtenberg:github-repo}
output:
  bookdown::pdf_book:
    keep_tex: true
    template: main.tex
    citation_package: natbib
    fig_caption: true
---

```{r code = readLines("../load_data.R"), echo = F, message = F}
```

```{r setup, echo=F, message=F, warning=F, cache=T}
library('rjson')
library('kableExtra')

# Colors:
cwomen <- "#7704FF"
cmen <- "#00C3AA"
cmen <- "gray70"
cwomen <- "gray30"
cavg <- "#FF10F0"
cavg <- "#CB00E6"


# This function reads in a .json conference file and for each paper in the conference it extracts the author names, paper id,
# field, and conference id into a data frame. It then normalizes the names using 'normalized_author_name' and moves
# on to the next paper, stacking the new data frame with those that have already been created each time.
# Finally it normalizes the author names and joins with a gender mapping df to get the corresponding genders.

json_to_df <- function(conf_name, gender_mapping) {
  json_fpath <- paste0(toplevel, "data/conf/", conf_name, ".json")
  conf <- rjson::fromJSON(file = json_fpath)
  confname <- conf[[1]]
  field <- conf[['field']]
  subfield <- conf[['subfield']]
  papers <- conf[['papers']]
  submissions <- conf[['submissions']]

  conf_info_df <- data.frame()

  for (i in seq_along(papers)) {
    paper_id <- unlist(papers[[i]][1])
    authors <- as.data.frame(papers[[i]][3]) %>%
      mutate(conf = confname,
             field = field,
             subfield = subfield,
             paper_id = paper_id,
             n_submissions = submissions)

    conf_info_df <- bind_rows(conf_info_df, authors)
  }

  conf_info_df <- conf_info_df %>%
    mutate(authors = map_chr(authors, normalized_author_name)) %>%
    mutate(authors = str_replace(authors, "\\s+", " ")) %>%
    mutate(paper_id = as.factor(paper_id)) %>%
    rename(name = authors)

  conf_info_df %>%
    left_join(gender_mapping, by = "name")
}

######################
# Here we create a vector of the conference names and use the 'map_dfr' function to apply 'json_to_df' to each conference.
nonsys_confs <-
  c("AAAI", "ACL", "CHI", "CVPR", "FSE", "ICML", "ICSE", "MM", "NIPS", "POPL", "SIGCSE", "SIGGRAPH", "SODA", "STOC", "WSDM", "WWW", "ITICSE", "FOCS", "TACAS")

nonsys_authors <- nonsys_confs %>%
  map_dfr(~json_to_df(.x, all_genders)) %>%
  mutate(across(.cols = everything(), na_if, "NA")) %>%
  mutate(across(c(field, subfield, gender), factor))
  

###########
name_combination_creator <- function(df)
{
  authors <- df$name
  if (length(authors) <= 1) {
    return (data.frame(name1 = c(), name2 = c()))
  }

  ret <- as.data.frame(t(combn(authors, 2)))
  names(ret) <- c("name1", "name2")
  ret <- rbind(ret, data.frame(name1 = ret$name2, name2 = ret$name1))
  return(ret)
}

#######
# All-to-all coauthor pairs for non-sys and for sys:
nonsys_collaboration_pairs <- nonsys_authors %>%
  group_by(paper_id) %>%
  group_modify(~name_combination_creator(.x)) %>%
  ungroup() %>%
  left_join(all_genders, by = c("name2" = "name")) %>%
  rename(gender2 = gender) %>%
  left_join(all_genders, by = c("name1" = "name")) %>%
  rename(gender1 = gender) %>%
  dplyr::select(paper_id, name1, gender1, name2, gender2)

sys_collaboration_pairs <- coauthors %>%
  left_join(persons, by = c("name1" = "name", "gs_email1" = "gs_email")) %>%
  rename(gender1 = gender, paper_id = paper) %>%
  left_join(persons, by = c("name2" = "name", "gs_email2" = "gs_email")) %>%
  rename(gender2 = gender) %>%
  dplyr::select(paper_id, name1, gender1, name2, gender2) %>%
  distinct(name1, name2, paper_id, .keep_all = T)

all_pairs <- rbind(nonsys_collaboration_pairs, sys_collaboration_pairs)

#####################
gender_summary_table <- function(data)
{
  data %>%
    summarise(.groups = "keep",
      n_papers = n_distinct(paper_id, conf),
      n_na = sum(is.na(gender)),
      n_male = sum(gender == "M", na.rm = T),
      n_female = sum(gender == "F", na.rm = T),
      prop_female = n_female / (n_female + n_male)
    ) %>%
    arrange(desc(prop_female)) %>%
    knitr::kable()
}

#######

sys_auths <- roles %>%
  filter(role == "author") %>%
  left_join(persons, by = c("name" = "name", "gs_email" = "gs_email")) %>%
  dplyr::select(name, gender, key, gs_email) %>%
  mutate(conf = as.factor(gsub("_\\d\\d\\d$", "", key))) %>%
  left_join(all_confs, by = c("conf" = "conference")) %>%
  rename(key = key.x) %>%
  dplyr::select(name, gender, key, field, subfield)

nonsys_auths <- nonsys_authors %>%
  dplyr::select(-conf, -n_submissions) %>%
  dplyr::select(name, gender, paper_id, field, subfield) %>%
  rename(key = paper_id) %>%
  mutate(gs_email = NA)

full_auths <- bind_rows(sys_auths, nonsys_auths) %>%
  group_by(key) %>%
  mutate(coauthors = n() - 1)

full_auths <- full_auths %>%
  group_by(name, gender) %>%
  mutate(prod = n(), fractional_prod = sum(1 / (coauthors + 1)))

full_auths$subfield = recode_factor(full_auths$subfield,
                                    "Heterogeneous Computing" = "Computer Architecture",
                                    "Network" = "Communications",
                                    "Virtualization" = "Operating Systems",
                                    "Energy" = "Computer Architecture",
                                    "Architecture" = "Computer Architecture")

full_auths$field = recode_factor(full_auths$field,
                                 "Systems" = "Computer Systems",
                                 "Computer Science Education" = "CS Education",
                                 "Artificial Intelligence" = "AI",
                                 "Human-Computer Interaction" = "HCI",
                                 "Theory and Algorithms" = "Theory",
                                 "Software Engineering & Languages" = "SE/languages")

gen_auths <- full_auths %>% drop_na(gender)

########
pairs_with_field <- full_auths %>%    # Add field and subfield to all_pairs
  mutate(paper_id = key) %>%
  group_by(paper_id) %>%
  summarize(.groups = "keep", field = first(field), subfield = first(subfield)) %>%
  right_join(all_pairs) %>%
  ungroup() %>%
  drop_na(gender1) %>%
  drop_na(gender2) %>%
  group_by(subfield) %>%
  mutate(internal = sum(gender1 == gender2), external = sum(gender1 != gender2)) %>%
  mutate(ei_index = (external - internal) / (external + internal)) %>%
  mutate(w_prob = sum(gender1 == "F" & gender2 == "F") / sum(gender1 == "F"))
  
#######
productivity_diff <- function(sf)
{
  men <- filter(gen_auths, gender == "M", subfield == sf)
  women <- filter(gen_auths, gender == "F", subfield == sf)
  paste0(round(mean(men$prod) - mean(women$prod), 2), "; ",
         report_test(t.test(men$prod, women$prod)))
}

#### Summary table of properties of subfields.
tmp <- pairs_with_field %>%
  group_by(subfield, field) %>%
  summarize(.groups = "keep", ei_index = first(ei_index), wprob = first(w_prob))

subfield_summary <- gen_auths %>%
  group_by(subfield) %>%
  summarize(.groups = "keep",
            FAR = sum(gender == "F") / n(),
            avg_prod = mean(prod),
            avg_frac = mean(fractional_prod),
            avg_coauthors = mean(coauthors),
            single_ratio = sum(coauthors == 0) / n_distinct(key)
            ) %>%
  ungroup() %>%
  left_join(tmp) %>%
  mutate(whomo = wprob / FAR)
```

# Introduction

The gender gap in science, technology, engineering and mathematics (STEM), and in particular in computer science (CS), is a well-known and well-studied problem.
It carries significant societal effects, such as inequality in economic opportunities for women and an undersupply of researchers and engineers in the rapidly growing discipline [@nielsen17:opinion; @mattis07:upstream].
The gender gap among researchers is particularly severe: the people who participate in research, publish about it, and have their research acknowledged for its value are predominantly men [@charman17:championing].
Numerous studies estimate that only about 15%--30% of the CS research community are women [@cohoon11:cspapers; @holman18:gender; @national20:science; @way16:gender; @zweben18:taulbee].
Although some recent indications show these numbers could be growing, they remain low, and the rate of growth remains slow [@wang21:trends].

The gender gap is a complex, multifaceted challenge [@avolio20:factors].
Numerous approaches to understand and perhaps address the gender gap have focused on aspects such as resource availability, gender stereotypes, child care, structural barriers, gender differences, discrimination, and other factors.
This article focuses on one of these factors: the collaboration patterns of paper coauthors across genders and CS fields.

Scientific collaborations are the backbone of a successful career in science [@whittington18:tie].
For example, researchers with more collaborators have been found to publish more articles, publish in higher impact journals, and accrue citations more quickly [@lee05:impact].
Consequently, many studies have investigated whether women and men collaborate at different rates across disciplines, and have often found significant differences [@bozeman04:scientists; @hunter08:collaborative; @kyvik96:child; @scott90:disadvantage].

In CS, and in particular in its more experimental fields such as computer systems, graphics, and artificial intelligence, collaboration is crucial because the large-scale implementation efforts involved often require teams of researchers with various experience levels.
In this article we focus on gender differences in collaboration patterns across the fields and subfields of CS.

Our study design is descriptive and observational in nature.
We did not start out with any preset hypotheses to validate.
Instead, our goal was to collect and analyze up-to-date, accurate and extensive data on CS authorship and collaboration patterns across genders.
This data and analysis provide baseline statistics for comparison across different time points and scientific disciplines.
But it also provides immediate answers and comparisons to existing work, thereby offering new insights into the current state of collaboration differences across genders and CS fields.
Specifically, in this article we address the following research questions:

 *  **RQ1**: What are the ratios of women and men among CS conference authors?
 
 *  **RQ2**: Do women publish less than men?

 *  **RQ3**: Are productivity differences affected by collaboration size?

 *  **RQ4**: Do women collaborate with fewer people than men?

 *  **RQ5**: Do women publish fewer single-author papers?

 *  **RQ6**: Are team sizes (coauthor groups) larger in more experimental subfields?

 *  **RQ7**: Do authors exhibit gender homophily in their choice of coauthors?

To bring these questions into historical context, we next briefly survey some of the previous work in the area.

## Related work {-}

There exists rich literature on the gender gap in the sciences in general, and in computer science research in particular.
For a recent review of these works, refer to Avolio's et al. review from 2020 [@avolio20:factors].
Instead, we limit our focus to the relevant literature on collaboration patterns and differences.

For example, a recent study of differences in collaboration patterns across disciplines found that female scientists have a lower probability of repeating previous coauthors than males.
It also found that female faculty have significantly fewer distinct coauthors over their careers than males, but that this difference can be fully accounted for by femalesâ€™ lower publication rate and shorter career lengths [@zeng16:differences].

This productivity gap, which we observed in our dataset as well, has been thoroughly explored in several other studies [@abramo09:gender; @huang20:historical; @kyvik96:child; @lariviere13:bibliometrics; @symonds06:gender].
In the social sciences, one study has found that women generally publish fewer papers than men, and that two thirds of the single-author papers were written by men [@schucan11:women].
In mathematics, women also publish less than men, especially early in their careers, and leave academia at a higher rate then men [@mihaljevic16:effect].
Women are also underrepresented in the three top-ranked journals and publish fewer single-author papers. In terms of mean number of coauthors, women's statistics are similar to men's.
That being said, there is even a gap in recognition, as women are also less likely to receive tenure the more that they coauthor [@sarsons17:recognition].


CS researchers in particular tend to collaborate more than researchers in other fields, regardless of gender, and there are no gender-specific differences in how collaborative behavior impacts scientific success [@jadidi18:gender].
This study also found that gender homophily in CS has been increasing over the past few years.

On a related note, another study of collaboration patterns across the sciences found that CS papers average 2.84 coauthors, and electrical engineering papers average 3.04 [@ghiasi15:compliance], which is somewhat lower than what we have found in this study.
It also found that generally, in engineering, female-female collaborations accounted for only 7% of all total pairs.
In CS, the percentage was even lower (5%).
Since 1990, there have been even more same-gender (gender homophily) coauthorships than expected [@wang21:trends].
But this property can vary across CS fields, necessitating more nuanced analysis.
For example, in the field of data visualization, women collaborated with substantially more women than men [@tovanich21:gender].

Corroborating this result for biotech patent networks, women have been found to be more likely to collaborate with women, and benefit from it, but both genders mostly collaborate with men [@whittington18:tie].
There are also fewer women "stars", which we also found to hold specifically for the subfield of high-performance computing [@frachtenberg21:whpc].

A surprising result came from a survey of 1,714 scientists in 2011, finding that when accounting for various confounding factors, women actually have more collaborators then men [@bozeman11:men].
The paper also reported that regression models that take into account different collaboration strategies are better at predicting a researcher's number of collaborators.

Research in computer science, and in particular in its more applied and experimental fields, can sometimes require expensive resources.
Several studies have found that the gender gap in research tends to be higher in disciplines with expensive barriers to entry [@duch12:possible; @elsevier17:gender; @head13:differences; @lariviere13:bibliometrics], which appears to agree with our findings for CS fields.

Some studies analyzed the gender gap by aggregating the coauthors of each paper into one "gender".
One study analyzed different aggregations based the proportion of female authors, gender of most senior authors, and single-author papers [@hengel17:publishing].
Looking at author position for aggregation, another study found that there are fewer women in first and last author positions in science overall, as well as in single-author papers [@west13:role].
Other ways to aggregate genders including counting all papers that have at least one female author, and those where at least half the authors are female.
In this study, we do not aggregate papers by gender, except for the trivial case when they have a single author.

## Organization {-}

The rest of this paper is organized as follows.
In the next section (Sec. \@ref(sec:data)), we describe in detail our data collection methodology, including the manual assignment of genders to authors to avoid the well-known issues of name-based gender inference.
In the results section (Sec. \@ref(sec:results)), we enumerate our findings, organized by research question, and then summarize an answer to each of the questions.
The discussion (Sec. \@ref(sec:discussion)) that follows then elaborates on these answers in an attempt to synthesize insights.
Finally, we conclude in Sec. \@ref(sec:conclusion) and suggest directions for future research.

<!-------------------------------------------------------------------------------->
<!-------------------------------------------------------------------------------->

# Materials and Methods {#sec:data}

To answer our research questions, we needed to collect expansive data on CS publications and their authors.
Such data collection involves many choices, such as which publications to collect and how to assign gender to authors.
The following list enumerates our main data decisions.
Each choice necessarily involves trade-offs, and we attempt to justify our choices by explaining which aspects we prioritized.

```{r all-confs, echo=F, cache=T}
# this is all nonsys conferences
nonsys_full_table <- nonsys_authors %>%
  group_by(conf) %>%
  mutate(Papers = n_distinct(paper_id, conf), Authors = n_distinct(name, conf, na.rm = T)) %>%
  distinct(conf, .keep_all = T) %>%
  ungroup() %>%
  mutate(Acceptance = round(Papers / n_submissions, 2)) %>%
  mutate(Conference = gsub("_\\d*", "", conf)) %>%
  dplyr::select(Conference, subfield, field, Papers, Authors, Acceptance) %>%
  rename(Field = field, Subfield = subfield)

# this is all other conferences
sys_full_table <- all_confs %>%
  mutate(Conference = gsub("_\\d*", "", conference)) %>%
  rename(Papers = npapers, Authors = authors_num, Field = field, Subfield = subfield) %>%
  mutate(Acceptance = round(acceptance_rate, 2)) %>%
  dplyr::select(Conference, Subfield, Field, Papers, Authors, Acceptance)

full_table <- dplyr::bind_rows(nonsys_full_table, sys_full_table) %>%
  arrange(Subfield, Acceptance)

full_table %>%
  dplyr::select(-c(Field)) %>%
  mutate(Acceptance = ifelse(is.na(Acceptance), "Unknown", Acceptance)) %>%
  knitr::kable(format = "latex",
               booktabs = T,
               align = "llrrr",
               linesep = "",
               caption = "All conferences, ordered by subfield and acceptance rate") %>%
  kable_styling(latex_options = c("hold_position"), font_size = 7)
```

**Conference data instead of journal data.**
In CS, original scientific results are typically first published in peer-reviewed conferences [@patterson99:evaluating; @patterson04:health], and then possibly in archival journals, sometimes years later [@vrettas15:conferences]. To increase the coverage and relevance of our dataset, we only looked at conference publications. The complete list of selected conference can be found in Table \@ref(tab:all-confs).

**Choice of conferences.**
Our dataset evolved from our previous study of conferences related to one major field, computer systems [@frachtenberg20:survey].
The conferences we selected include some of the most prestigious systems conferences (based on indirect measurements such as Google Scholar's metrics), as well as several smaller or less-competitive conferences for contrast.
For this specific study, we decided to expand the analysis to include some of the most influential conferences in most subfields of CS, based on the same measures, for a total of `r n_distinct(full_auths$key)` papers across CS.
Obviously, not all subfields have equal numbers of participants or conferences, and we had no set quota for either to be included in our dataset.
Instead, we tried to ensure that each subfield is represented by at least a few hundred authors for statistical power.

**Limit data to a single year.**
Many fields and researchers shift characteristics over time, complicating collaboration analyses.
To control for these effects, all of the conferences in our dataset are from a single year, 2017.

**Focus on manual gender assignment.**
Most studies of author gender at scale use automated approaches to assign gender to authors, typically inferred from given names [@huang20:historical; @karimi16:gender].
These statistical approaches can be reasonably accurate for names of Western origin, and especially for male names  [@cohoon11:cspapers; @mattauch20:bibliometric; @santamaria18:comparison], but can fall short when inferring from Indian and East Asian names.
We opted instead to rely primarily on a manual approach that can overcome the limitations of name-based inference.
Using web lookup, we assigned the gender of
`r fmt(nrow(verified_gender) + nrow(verified_gender_nonsys))`
of the unique researchers for whom we could identify an unambiguous web page with a recognizable gendered pronoun, or absent that, a photo.
(For example, many Linkedin profiles may lack a photo, but include a gendered pronoun in the recommendations section.)
For `r fmt(nrow(inferred_gender) + nrow(inferred_gender_nonsys))`
others, we used genderize.io's automated gender designations if it was at least 90% confident about them [@santamaria18:comparison].
The remaining
`r fmt(filter(full_auths, is.na(gender)) %>% pull(name) %>% unique() %>% length())`
persons were assigned "NA" instead of a gender and were excluded from most analyses.
This method provided more gender data and higher accuracy than automated approaches based on forename and country, especially for women [@karimi16:gender; @lariviere13:bibliometrics; @mattauch20:bibliometric; @squazzoni20:noevidence; @wang21:trends].
Consequently, we have very few NA genders relative to comparable studies.
We believe that high coverage is critical when analyzing coauthorship networks, because omitting a large number of connected sub-networks (such as people from Asia) may distort our results.

**Assignment of field and subfield.**
We could find no standard definition and delineation of fields and subfields of CS, so we had to come up with our own, which was necessarily subjective (Table \@ref(tab:subfield-mapping)).
Moreover, conferences do not always fall neatly into a single subfield, and some papers may stray from the primary focus of the conference.
We note, however, that in most of our analyses, papers in subfields assigned to the same field often exhibited similar characteristics to each other and distinct from other subfields.
This affinity provides some evidence that these assignments are not entirely arbitrary.
That said, other researchers may choose different assignments of papers or conferences to subfields and fields.
Since our dataset and code are both open and available, we encourage such reevaluations of the data.

```{r subfield-mapping, echo=F, cache=T}
df <- full_auths %>%
  group_by(subfield) %>%
  summarize(.groups = "keep", Field = first(field)) %>%
  rename(Subfield = subfield)

df$Field = recode_factor(df$Field,
                         "AI" = "Artificial Intelligence (AI)",
                         "HCI" = "Human-Computer Interaction (HCI)",
                         "SE/languages" = "Software Engineering & Programming Languages")

df %>%
  arrange(Field, Subfield) %>%
  knitr::kable(format = "latex",
               booktabs = T,
               align = "ll",
               linesep = "",
               caption = "All CS subfields analyzed, arranged by fields") %>%
  kable_styling(font_size = 7)
```


## Limitations {-}

The decisions listed above also represent some compromises that limit the generalization or applicability of our analysis.
One such limitation is that the data reflects a snapshot in time to avoid the complexities of gender differences in retention rates.
However, this choice precludes analyses of changes and trends in collaboration patterns over time.

Another limitation is our choice of which conferences to include out of the hundreds or thousands of annual CS conferences.
Our conference choices may not be not representative of all of CS or even a proportional representation of subfields with CS.
The relative metrics we measured comparing different subfields are nevertheless meaningful, but metrics over the entire dataset should be taken with a grain of salt.
We believe that the large number of authors we included in our analysis provides some statistical robustness and therefore does not significantly deviate from a representative sample of the field of CS as a whole.

For this study, the most critical piece of information on these researchers is their \emph{perceived gender}.
Gender is a complex, multifaceted identity, but most bibliometric studies still rely on binary genders---either collected by the journal, or inferred from first name---because that is the only designator available to them [@bhagat18:data; @cohoon11:cspapers; @holman18:gender; @national20:science; @wang21:trends; @way16:gender; @zweben18:taulbee].
In the absence of self-identified gender information for our authors, we also necessarily compromised on using binary gender designations.
We therefore use the gender terms "women" and "men" interchangeably with the sex terms "female" and "male".
The conferences in our dataset did not collect or share specific gender information, so we had to collect this information from other public sources.

This labor-intensive approach does introduce the prospect of human bias and error.
For example, a gender assigned by an outdated biography paragraph with pronouns may no longer agree with the self-identification of the researcher.
To verify the validity of our approach, we compared our manually assigned genders to self-assigned binary genders in a separate survey we conducted among 918 of the authors  [@frachtenberg20:survey].
We found no disagreements for these authors, which suggests that the likelihood of disagreements among the remaining authors is low.
But the main limitation that arises from this manual process of data collection and gender assignment, is that it does not scale well to a larger number of conferences or years.

Finally, the nature of the current analysis is more descriptive than prescriptive.
Rather than presenting preconceived hypotheses and testing them with the data, we ask and answer open-ended research questions that fit in the scope of this paper.
The answers to these questions will surely instigate further hypotheses and questions requiring deeper analysis, such as social network analysis, which is also important to understanding many collaboration patterns [@whittington18:tie]
The open dataset we provide with this article should enable any interested researcher to perform such analyses.

## Statistics {-}

For statistical testing, group means were compared pairwise using Welch's two-sample t-test and group medians using the Wilcoxon Signed Rank Test; differences between distributions of two categorical variables were tested with the $\chi^{2}$ test; and correlations between two numerical variables were evaluated with Pearson's product moment correlation coefficient.
All statistical tests are reported with their p-values.


<!-------------------------------------------------------------------------------->
<!-------------------------------------------------------------------------------->

# Results {#sec:results}

For each research question, we start with descriptive statistics across the entire sample population, and then break the statistics down by field and subfield.

## RQ1 What are the ratios of women and men among CS conference authors?

Before we can look at collaboration patterns, we need to establish a baseline for authorship numbers across genders.
For example, the question of how many women or men an author collaborates with makes little sense without the context of how many women and men are available to collaborate with overall.
Consequently, the first question we ask is what is the female author ratio (FAR) in our dataset.

Summarizing across all `r fmt(nrow(full_auths))` authors (with repeats for multiple papers) and omitting the `r fmt(sum(is.na(full_auths$gender)))` repeated authorships for which we could establish no gender, we find a total of `r fmt(sum(gen_auths$gender == "F"))` women, which represents an overall FAR of
`r pct(sum(gen_auths$gender == "F"), nrow(gen_auths))`% across authors.
This result is on the low end of previously reported statistics in the range of 15--30% [@cohoon11:cspapers; @holman18:gender; @national20:science; @way16:gender; @zweben18:taulbee].
It is quite possible that our results are on the low end because our choice of conferences, with its emphasis on computer systems, overrepresents fields with lower representation of women.
Keep in mind, however, the differences between previous studies and the current one, both in data and in methodology.
Our data includes only conferences and only from one year is by no means exhaustive.
On the other hand, the smaller sample size allowed us to apply a primarily manual approach to gender assignment, which provides higher accuracy and coverage of researchers.
In contrast, most comparable studies use a gender inference approach based on given names, which can fail for names with unclear or no gender association at all, as are many East Asian names, and tend to misidentify women in particular [@cohoon11:cspapers; @mattauch20:bibliometric; @santamaria18:comparison].

```{r FAR-by-subfield, echo=F, warning=F, message=F, cache=T, out.width='0.75\\textwidth', fig.cap="Female author ratio by subfield"}
gen_auths %>%
  group_by(subfield) %>%
  summarize(.groups = "keep", FAR = sum(gender == "F") / n(), N = n(), field = first(field), subfield = first(subfield)) %>%
  ggplot(aes(x = fct_reorder(subfield, FAR), y = FAR, fill = field)) +
    geom_bar(stat = "identity") +
    geom_text(aes(x = fct_reorder(subfield, FAR), y = 0.42, label = paste0(subfield, "  N=", fmt(N))), hjust = 1, angle = 0, size = 3) +
    scale_y_continuous(labels = scales::percent) +
    xlab("Subfield") +
    ylab("Female author ratio") +
    labs(fill = "") +
    coord_flip() +
    scale_fill_brewer(palette = "Paired") +
    theme_minimal() +
    theme(legend.position = "bottom",
#          legend.justification = c(0, 0),
          axis.text.y = element_blank(),
          #element_text(angle = 90, hjust = 1, vjust = -0.00001),
          legend.box = "vertical",
          legend.margin = margin())
```

Breaking down FAR values for each field and subfield (Fig. \@ref(fig:FAR-by-subfield)), we find large differences across CS.
The highest FAR was exhibited in CS education conferences
(`r pct(nrow(filter(gen_auths, gender == "F", subfield == "Computer Science Education")), nrow(filter(gen_auths, subfield == "Computer Science Education")))`%)
and the lowest in theoretical CS
(`r pct(nrow(filter(gen_auths, gender == "F", subfield == "Theoretical Computer Science")), nrow(filter(gen_auths, subfield == "Theoretical Computer Science")))`%).
Most conferences in the field of computer systems hovered around 10% FAR, while the average across the entire field of AI was a little bit larger at
(`r pct(nrow(filter(gen_auths, gender == "F", field == "AI")), nrow(filter(gen_auths, field == "AI")))`%).



---

## RQ2: Do women publish less than men?

Many papers across disciplines discuss the existence and potential reasons for a productivity gap, that is, the observation that men generally publish more scholarly articles than women.
Here, we continue our exploration of the data by looking at the productivity rates across genders and subfields of CS.


```{r productivity-dist, echo=F, warning=F, message=F, cache=T, out.width='0.75\\textwidth', fig.cap="Distribution of number of distinct papers per author"}
gen_auths %>%
  ggplot(aes(x = prod, fill = gender)) +
    geom_histogram(position = "dodge", binwidth = 1) +
    scale_fill_manual(values = c(cwomen, cmen), labels = c("Women", "Men")) +
    theme_classic() +
    theme(legend.position = "bottom") +
    xlab("Papers in dataset") +
    ylab("Total authors")
```

Fig. \@ref(fig:productivity-dist) shows the overall distributions of paper productivity in CS across genders.
Aside from the now-obvious observation that men far outnumber women authors, we can also observe a longer tail for the men's distribution overall.
The interpretation is that the most prolific authors are especially skewed male.
On the opposite tail, we find that
`r pct(nrow(filter(gen_auths, gender == "F", prod == 1)), nrow(filter(gen_auths, gender == "F")))`%
of female authors published only one paper in our dataset, compared to
`r pct(nrow(filter(gen_auths, gender == "M", prod == 1)), nrow(filter(gen_auths, gender == "M")))`%
of men.

Overall, men average
`r round(mean(filter(gen_auths, gender == "M")$prod), 2)`
papers per author, compared to women's
`r round(mean(filter(gen_auths, gender == "F")$prod), 2)`
(`r report_test(t.test(filter(gen_auths, gender == "F")$prod, filter(gen_auths, gender == "M")$prod))`).
Looking at medians---to attempt to attenuate the large effect of the long tail on means---does not help much.
Both medians are naturally 1; but a Wilcoxon signed rank test still shows a significant difference
(`r report_test(wilcox.test(filter(gen_auths, gender == "F")$prod, filter(gen_auths, gender == "M")$prod))`).


```{r productivity-by-subfield, echo=F, warning=F, message=F, cache=T, fig.height=6.5, out.width='0.75\\textwidth', fig.cap="Distribution of number of distinct papers per author by gender, field, and subfield. Triangles denote means and veritcal notches denote medians. Triangles denote outlier points outside the 5--95\\% range."}
gen_auths %>%
  group_by(subfield) %>%
  mutate(avg_prod = mean(prod), N = n()) %>%
  ungroup() %>%
  ggplot(aes(x = fct_reorder(subfield, avg_prod), y = prod, fill = gender, color = field)) +
    geom_boxplot(lwd = 0.5) +
    geom_point(aes(y = avg_prod), color = cavg, shape = 2) +
#    geom_text(aes(x = fct_reorder(subfield, avg_prod), y = 21, label = paste0("N=", N)), hjust = 1.1) +
    xlab("Subfield") +
    ylab("Paper count per author") +
    theme_classic() +
    theme(legend.position = "bottom", legend.box = "vertical") +
    guides(fill = guide_legend(direction = 'horizontal', title = "Gender"),
           color = guide_legend(direction = 'horizontal', title = "Field")) +
    scale_fill_manual(values = c(cwomen, cmen), labels = c("Women", "Men")) +
    scale_color_brewer(palette = "Paired") +
    coord_flip()
```


Fig. \@ref(fig:productivity-by-subfield) shows the breakdown of productivity distribution by gender, field, and subfield.
The field of AI and its subfields show the highest average productivity (and highest outliers, for men), while software engineering, programming languages, and CS education exhibit the lowest average papers per author.
In terms of gender, the largest differences in productivity exhibit in the subfield of theoretical CS (difference in means $\Delta$=`r productivity_diff("Theoretical Computer Science")`), followed by machine learning ($\Delta$=`r productivity_diff("Machine Learning")`), benchmarking ($\Delta$=`r productivity_diff("Benchmarking")`), HPC ($\Delta$=`r productivity_diff("High-Performance Computing")`), AI ($\Delta$=`r productivity_diff("Artificial Intelligence")`), and security ($\Delta$=`r productivity_diff("Security")`).
A few subfields show a small productivity advantage for women, such as programming languages ($\Delta$=`r productivity_diff("Programming Languages")`), but none are statistically significant.

---

## RQ3: Are productivity differences affected by collaboration size?


Comparing raw productivity across subfields in this way can be misleading, because the typical collaboration size is also related to productivity, but varies by subfield.
For example, if in some subfield the typical number of authors per papers is larger than another field, we might also expect that each author's name would show up in more papers under otherwise equal assumptions.
We therefore also look at the "fractional count" of publications instead, dividing each authorship event by the number of authors on the paper [@lee05:impact].

Overall, men average a total of
`r round(mean(filter(gen_auths, gender == "M")$fractional_prod), 2)`
fractional papers per author, compared to women's
`r round(mean(filter(gen_auths, gender == "F")$fractional_prod), 2)`,
which is statistically significant
(`r report_test(t.test(filter(gen_auths, gender == "F")$fractional_prod, filter(gen_auths, gender == "M")$fractional_prod))`).


```{r fractional-productivity, echo=F, warning=F, message=F, cache=T, fig.height=6.5, out.width='0.75\\textwidth', fig.cap="Distribution of fractional paper counts per author by gender, field, and subfield. Triangles denote means and veritcal notches denote medians. Triangles denote outlier points outside the 5--95\\% range."}
gen_auths %>%
  group_by(subfield) %>%
  mutate(avg_prod = mean(fractional_prod), N = n()) %>%
  ungroup() %>%
  ggplot(aes(x = fct_reorder(subfield, avg_prod), y = fractional_prod, fill = gender, color = field)) +
    geom_boxplot(lwd = 0.5) +
    geom_point(aes(y = avg_prod), color = cavg, shape = 2) +
#    geom_text(aes(x = fct_reorder(subfield, avg_prod), y = 21, label = paste0("N=", N)), hjust = 1.1) +
    xlab("Subfield") +
    ylab("Fractional paper count per author") +
    theme_classic() +
    theme(legend.position = "bottom", legend.box = "vertical") +
    guides(fill = guide_legend(direction = 'horizontal', title = "Gender"),
           color = guide_legend(direction = 'horizontal', title = "Field")) +
    scale_fill_manual(values = c(cwomen, cmen), labels = c("Women", "Men")) +
    scale_color_brewer(palette = "Paired") +
    coord_flip()
```

This normalized productivity metric exhibits on the one hand smaller differences across fields, but on the other, larger differences across genders (Fig. \@ref(fig:fractional-productivity)).
The medians for men's fractional productivity appear noticeably higher in most subfields, and men exhibit much larger and more numerous outliers on the prolific tail of the spectrum.
However, fractional productivity does not appear to affect the relative rankings of subfields very much.
Of the fields that are more affected by this metric, the most notable perhaps is theoretical CS, which now exhibits the highest mean fractional productivity.

The modified productivity metric also segues into the next three research questions that examine in depth the differences in collaboration sizes in CS.


---

## RQ4: Do women collaborate with fewer people than men?

This question could be addressed by two distinct measures: the mean number of coauthors per paper and the size of the total coauthor network for each author.


```{r coauthors-by-subfield, echo=F, warning=F, message=F, cache=T, fig.height=6.5, out.width='0.75\\textwidth', fig.cap="Mean number of coauthors per person by gender and subfield (triangles denote overall mean for subfield)."}
gen_auths %>%
  group_by(subfield) %>%
  mutate(avg_co = mean(coauthors)) %>%
  ungroup() %>%
  group_by(subfield, avg_co, gender) %>%
  summarize(.groups = "keep", Coauthors = mean(coauthors), field = first(field), subfield = first(subfield)) %>%
    ggplot(aes(x = fct_reorder(subfield, avg_co), y = Coauthors, color = gender, fill = field)) +
    geom_bar(stat = "identity", position = "dodge", size = 1) +
    geom_point(aes(y = avg_co), color = cavg, shape = 2) +
    xlab("Subfield") +
    ylab("Coauthor count per author") +
    theme_classic() +
    theme(legend.position = "bottom", legend.box = "vertical") +
    guides(color = guide_legend(direction = 'horizontal', title = "Gender"),
           fill = guide_legend(direction = 'horizontal', title = "Field")) +
    scale_color_manual(values = c(cwomen, cmen), labels = c("Women", "Men")) +
    scale_fill_brewer(palette = "Paired") +
    coord_flip()
```

Women in our dataset average
<!--excludes solo authors: `r coauthor_count <- all_pairs %>% group_by(paper_id, name1, gender1) %>% summarize(.groups = "keep", n = n()); round(mean(filter(coauthor_count, gender1 == "F")$n), 2)` -->
`r round(mean(filter(gen_auths, gender == "F")$coauthors), 2)`
coauthors per paper, while men average
`r round(mean(filter(gen_auths, gender == "M")$coauthors), 2)`
(`r report_test(t.test(filter(gen_auths, gender == "F")$coauthors, filter(gen_auths, gender == "M")$coauthors))`).
This metric appears to show no significant differences in the aggregate.
Breaking it down by field (Figure \@ref(fig:coauthors-by-subfield)) shows that the gender differences still remain minimal throughout almost all of CS.
The largest differences appear in the subfields of computer architecture
(`r report_test(t.test(filter(gen_auths, subfield == "Computer Architecture", gender == "F")$coauthors, filter(gen_auths, subfield == "Computer Architecture", gender == "M")$coauthors))`)
and operating systems
(`r report_test(t.test(filter(gen_auths, subfield == "Operating Systems", gender == "F")$coauthors, filter(gen_auths, subfield == "Operating Systems", gender == "M")$coauthors))`).
In general, the field of computer systems stands out with a an average gender gap of
`r round(mean(filter(gen_auths, field == "Computer Systems", gender == "M")$coauthors) -  mean(filter(gen_auths, field == "Computer Systems", gender == "F")$coauthors), 2)`
fewer coauthors for women than for men
(`r report_test(t.test(filter(gen_auths, field == "Computer Systems", gender == "F")$coauthors, filter(gen_auths, field == "Computer Systems", gender == "M")$coauthors))`).


The second measure is how many distinct authors each person collaborates with across all of their papers---in other words, the size of the network of all collaborators of a person.
This time, the difference is more pronounced, with women averaging
`r cohorts <- all_pairs %>% group_by(name1, gender1) %>% summarize(.groups = "keep", cohort = n_distinct(name2, gender2)); round(mean(filter(cohorts, gender1 == "F")$cohort), 2)`
total cohort size, while men average
`r round(mean(filter(cohorts, gender1 == "M")$cohort), 2)`
(`r report_test(t.test(filter(cohorts, gender1 == "F")$cohort, filter(cohorts, gender1 == "M")$cohort))`).
Unfortunately, this metric cannot be neatly broken down by fields, because coauthor networks often include authors that span more than one field.

Overall, men exhibit slightly higher collaboration metrics than women (more coauthors per paper and more coauthors overall), but not dramatically so.

---


## RQ5: Do women publish fewer single-author papers?

Next, we turn our attention to single-author papers (Table \@ref(tab:single-authors)).
In our dataset, there is a total of
`r nrow(filter(full_auths, coauthors == 0))`
such papers, of which
`r nrow(filter(full_auths, coauthors == 0, gender == "F"))`
were written by a woman,
`r nrow(filter(full_auths, coauthors == 0, gender == "M"))`
by a man, and the rest unknown.
The ratio of women among single authors with known gender is
`r pct(nrow(filter(gen_auths, coauthors == 0, gender == "F")), nrow(filter(gen_auths, coauthors == 0)))`%,
which is significantly lower than that of the overall
`r pct(nrow(filter(gen_auths, gender == "F")), nrow(filter(gen_auths)))`% FAR
(`r report_test(chisq.test(table(gen_auths$gender == "F", gen_auths$coauthors == 0)))`).

```{r single-authors, eval=T, echo=F, warning=F, message=F, cache=T}
df <- full_auths %>%
  group_by(subfield) %>%
  summarize(.groups = "keep",
    Papers = n_distinct(key),
    ratio = sum(coauthors == 0) / n_distinct(key),
    single = sum(coauthors == 0),
    Women = sum(coauthors == 0 & gender == "F", na.rm = T),
    Men = sum(coauthors == 0 & gender == "M", na.rm = T),
    Unknown = sum(coauthors == 0 & is.na(gender), na.rm = T)
  ) %>%
  arrange(desc(ratio))

df <- bind_rows(df, data.frame(
    subfield = "Total",
    Papers = sum(df$Papers),
    ratio = nrow(filter(full_auths, coauthors == 0)) / sum(df$Papers),
    single = nrow(filter(full_auths, coauthors == 0)),
    Women = nrow(filter(full_auths, coauthors == 0, gender == "F")),
    Men = nrow(filter(full_auths, coauthors == 0, gender == "M")),
    Unknown = nrow(filter(full_auths, coauthors == 0, is.na(gender)))
  )) %>% 
  mutate(ratio = paste0(round(100 * ratio, 2), "%")) %>%
  rename(Subfield = subfield, "Number of Papers" = "Papers", "All single author" = single, "Percent single" = ratio)

df %>%
  knitr::kable(format = "latex",
               booktabs = T,
               align = "lrrrrrr",
               linesep = c(rep("", nrow(df) - 2), "\\addlinespace"),
               caption = "Number of papers by subfield and gender, sorted by the overall percentage of single-author papers.") %>%
  column_spec(1, width = "3.5cm") %>%
  column_spec(2:6, width = "1.1cm") %>%
  kable_styling(font_size = 7)
```


```{r solo-props-by-subfield-and-gender, eval=F, echo=F, warning=F, message=F, cache=T, fig.height=6.5, out.width='0.75\\textwidth', fig.cap="Percentage of papers in each subfield that were written by a single author, grouped by the gender of the first author. Triangles demote overall ratio of single-author papers in the subfield irrespective of gender."}
full_auths %>%
  group_by(subfield) %>%
  mutate(N = n(), avg_solo = sum(coauthors == 0) / n_distinct(key)) %>%
  ungroup() %>%
  group_by(key) %>%
  mutate(first_gender = first(gender)) %>%
  ungroup() %>%
  filter(!is.na(first_gender)) %>%
  group_by(subfield, field, avg_solo, first_gender, N) %>%
  summarize(.groups = "keep", pct_solo = sum(coauthors == 0) / n_distinct(key)) %>%
  ggplot(aes(x = fct_reorder(subfield, avg_solo), y = pct_solo, fill = field, color = first_gender)) +
    geom_col(stat= "identity", position = "dodge", size = 1) +
    geom_point(aes(y = avg_solo), color = cavg, shape = 2) +
    geom_text(aes(x = fct_reorder(subfield, avg_solo), y = 0.18, label = paste0("N=", fmt(N))), hjust = 1, angle = 0, size = 3, color = "black") +
    scale_color_manual(values = c(cwomen, cmen), labels = c("Women", "Men")) +
    theme_classic() +
    theme(legend.position = "bottom", legend.box = "vertical") +
    guides(
      fill = guide_legend(direction = 'horizontal', title = "Gender"),
      color = guide_legend(direction = 'horizontal', title = "Field")
      ) +
    labs(y = "Percent of papers written by single authors", x = "Subfield") +
    scale_y_continuous(labels = scales::percent) +
    scale_fill_brewer(palette = "Paired") +
    coord_flip()
```

```{r solo-props-by-subfield, eval=F, echo=F, warning=F, message=F, cache=T, fig.height=6.5, out.width='0.75\\textwidth', fig.cap = "Percentage of papers in each subfield that were written by a single author."}
full_auths %>%
  group_by(subfield) %>%
  mutate(N = n(), avg_solo = sum(coauthors == 0) / n_distinct(key)) %>%
  ungroup() %>%
  group_by(key) %>%
  ungroup() %>%
  group_by(subfield, field, avg_solo, N) %>%
  summarize(.groups = "keep", pct_solo = sum(coauthors == 0) / n_distinct(key)) %>%
  ggplot(aes(x = fct_reorder(subfield, avg_solo), y = avg_solo, fill = field)) +
    geom_col(stat= "identity", size = 1) +
    geom_text(aes(x = fct_reorder(subfield, avg_solo), y = 0.13, label = paste0("N=", fmt(N))), hjust = 1, angle = 0, size = 3) +
    theme_classic() +
    theme(legend.position = "bottom", legend.box = "vertical") +
    guides(color = guide_legend(direction = 'horizontal', title = "Field")) +
    labs(y = "Percent of papers written by single authors", x = "Subfield") +
    scale_y_continuous(labels = scales::percent) +
    scale_fill_brewer(palette = "Paired") +
    coord_flip()
```

Another way to look at the same data is from the perspective of papers instead of authors.
As the data in Table \@ref(tab:single-authors) shows, in most CS subfields fewer than 5% of papers were written by a single author, adding up to only `r df[nrow(df),4]` papers.
(Contrast this, for example, with astronomy, physics, and biology, where the rates of single-author papers average over 10% [@abt07:future].)

Breaking down the data by gender offers little information, because the numbers of single-author papers per subfield are too small for statistical significance.
In fact, the numbers of single-author papers are so low, as is the number of women authors overall, that their intersection is actually empty for most subfields.
That said, women published relatively fewer single-author papers than men (by percentage) in all but three of the subfields: WWW, Data Science, and Storage.
In all three, the numbers are simply too small to draw any conclusive inferences.

It is also worth noting that in much of the systems field the percentage of single-author papers is extremely low.
This observation may be another indication that the systems field in particular depends on larger collaboration teams for published research.
We suspect that the field's emphasis on complex implementations and experimental platforms requires larger teams to pull off, which we address in the next research question.

---

## RQ6: Are team sizes larger in more experimental subfields?

The data we collected on coauthorship size and single-authorship shows that authorship norms vary significantly by field, if not by gender. 
The largest coauthorship groups appear in computer systems papers, averaging
`r round(mean(filter(full_auths, field == "Computer Systems")$coauthors + 1), 2)`
coauthors per paper, followed by HCI
(`r round(mean(filter(full_auths, field == "HCI")$coauthors + 1), 2)`),
knowledge systems
(`r round(mean(filter(full_auths, field == "Knowledge Systems")$coauthors + 1), 2)`),
software engineering and programming languages
(`r round(mean(filter(full_auths, field == "SE/languages")$coauthors + 1), 2)`),
AI
(`r round(mean(filter(full_auths, field == "AI")$coauthors + 1), 2)`),
CS education
(`r round(mean(filter(full_auths, field == "CS Education")$coauthors + 1), 2)`),
and finally theory
(`r round(mean(filter(full_auths, field == "Theory")$coauthors + 1), 2)`).

These data do appear to confirm the hypothesis that experimental fields generally require larger teams to design, engineer, implement, and measure research results.
For example, research in computer architecture, the most collaborative of our subfields, often requires large investments in effort (and often, in capital as well).
This characterization extends to most computer systems subfields that occupy the top spots in terms of collaboration sizes.
It appears indeed that the larger effort and resource requirement is associated with larger collaborations, as expressed in mean number of coauthors.

On the opposite end, research in computer theory requires virtually no equipment and is often carried out by individuals, as we have previously observed.
The characteristics of theory research are naturally very similar to those of mathematicians as a whole, so it is perhaps not surprising that the mean number of coauthors we found for theory is nearly identical to the one found for mathematics [@mihaljevic16:effect].


---

## RQ7: Do authors exhibit gender homphily?

For our last research question, we follow the approach of Wang et al. to estimate whether authors collaborate with coauthors of the same gender at rates higher than expected [@wang21:trends].
For this computation, we look at every pairing of coauthors as one coauthoring event (omitting single-author papers), and ask whether same-gender pairings occur at a higher frequency than we would observe from a random pairing.
A random pairing is expected to follow the same overall statistics for gender distribution, i.e., the expected probability of any (co)author to be a woman should be the same as the overall FAR.

As Wang's study has also found, our data suggests that is not the case for CS, and authors---especially women---are actually more likely to collaborate with coauthors of the same gender.
Overall, the probability of a woman's coauthor to be a woman in our dataset is
`r pct(nrow(filter(all_pairs, gender1 == "F", gender2 == "F")), nrow(filter(all_pairs, gender1 == "F")))`%,
nearly ten percentage points above the overall FAR.
For men, the probability to collaborate with a woman is 
`r pct(nrow(filter(all_pairs, gender1 == "M", gender2 == "F")), nrow(filter(all_pairs, gender1 == "M")))`%,
slightly below the overall FAR.

```{r prob-female-by-subfield, echo=F, warning=F, message=F, cache=T, fig.height=6.5, out.width='0.75\\textwidth', fig.cap="Probability of an author to coauthor with a woman (triangles denote overall probability for subfield, which is similar to FAR but excludes single authors). Women show gender homophily when their probability to coauthor with a woman is higher than the overall probability, and men exhibit homophily when their probability is lower than the overall's."}
pairs_with_field %>%
  group_by(subfield) %>%
  mutate(.groups = "keep", avg_prob = sum(gender2 == "F") / n()) %>%
  ungroup() %>%
  group_by(subfield, avg_prob, gender1) %>%
  summarize(.groups = "keep", Probability = sum(gender2 == "F") / n(), field = first(field), subfield = first(subfield)) %>%
    ggplot(aes(x = fct_reorder(subfield, avg_prob), y = Probability, color = gender1, fill = field)) +
    geom_bar(stat = "identity", position = "dodge", size = 1) +
    geom_point(aes(y = avg_prob), color = cavg, shape = 2) +
    xlab("Subfield") +
    ylab("Probability of coauthor being female") +
    theme_classic() +
    theme(legend.position = "bottom", legend.box = "vertical") +
    guides(color = guide_legend(direction = 'horizontal', title = "Gender"),
           fill = guide_legend(direction = 'horizontal', title = "Field")) +
    scale_color_manual(values = c(cwomen, cmen), labels = c("Women", "Men")) +
    scale_fill_brewer(palette = "Paired") +
    coord_flip()
```



We can also break down these probabilities by subfield (\@ref(fig:prob-female-by-subfield)).
As a generalization, most subfields exhibit gender homophily, especially in the two subfields with the highest FAR (CS education and HCI).
A few subfields exhibit gender heterophily, but typically very little.
A curious exception is the subfield of programming languages, where women only have a probability of
`r pct(nrow(filter(pairs_with_field, gender1 == "F", gender2 == "F", subfield == "Programming Languages")), nrow(filter(pairs_with_field, gender1 == "F", subfield == "Programming Languages")))`%
to collaborate with a woman, less than half the overall FAR for the field.
Together with the subfield of software engineering, this field appears to show consistent heterophily among its female authors.

All other fields show fairly consistent gender homophily to varying degrees.
For example, in the large field of computer systems, women have a probability of
`r pct(nrow(filter(pairs_with_field, gender1 == "F", gender2 == "F", field == "Computer Systems")), nrow(filter(pairs_with_field, gender1 == "F", field == "Computer Systems")))`%
to collaborate with a woman, above the $\approx{10}\%$ FAR.
In all cases, the more pronounced deviations from the expected probability is for women, suggesting that perhaps collaborating with same-gender authors is more important to women.
Note, however, that since most authors are men and therefore overall FAR is mostly determined by men, we would expect men's deviations from FAR to be smaller than women's.

---


<!-------------------------------------------------------------------------------->
<!-------------------------------------------------------------------------------->

# Discussion {#sec:discussion}

In this section we dive deeper into the data by exploring the relationships between the different measurements and metrics across subfields, summarized in Table \@ref(tab:subfield-aggregation).
Our goal is to see whether the variations in any of the separate metrics we collected can help explain higher---or lower---values of FAR in the field, possibly providing hints to causal relationships.

The first relationship we investigate is between productivity and FAR.
In other words, can the higher observed productivity of men explain why we observe so many more male authors than female authors?

The answer appears to be "mostly not."
Obviously, the observation that men publish more than women implies that we would find more names of men on papers than we would of women, leading to lower FAR.
When aggregating the data on a subfield basis we do indeed find a moderate negative correlation between a subfield's FAR and the mean productivity of its practitioners
(`r report_test(cor.test(subfield_summary$FAR, subfield_summary$avg_prod))`).
This correlation weakens when using fractional paper counts,
(`r report_test(cor.test(subfield_summary$FAR, subfield_summary$avg_frac))`),
since the slightly larger team sizes for men attenuate their productivity advantage.
Some subfields show no apparent relationship between FAR and productivity, like storage and machine learning, which have very similar FARs but very dissimilar productivity gaps.
Other subfields exhibit a stronger opposite relationship, like HCI and compilers, with similar productivity metrics but very dissimilar FARs.

Overall, the relatively modest advantage in productivity for men
(`r round(mean(100 * filter(gen_auths, gender == "M")$prod) / mean(filter(gen_auths, gender == "F")$prod) - 100, 2)`% more papers per author) does not translate directly to the nearly 9:1 ratio of male-to-female authorship.
In fact, if we ignore repeated publications altogether and look simply at the ratio of unique women among all unique authors, we still observe a ratio of
`r tmp <- gen_auths %>% group_by(gender) %>% summarize(.groups = "keep", names = n_distinct(name)); pct(filter(tmp, gender=="F")$names, sum(tmp$names))`%
women overall.
While this ratio represents a slight improvement of one percentage point over the non-unique FAR, it is still far from parity, suggesting that higher productivity alone cannot fully explain the gender gap.


```{r subfield-aggregation, echo = F, cache = T}
rounding = 3

df <- data.frame(
    Subfield = subfield_summary$subfield,
    FAR = round(subfield_summary$FAR, rounding),
    "Mean productivity" = round(subfield_summary$avg_prod, rounding),
    "Fractional productivity" = round(subfield_summary$avg_frac, rounding),
    "Mean coauthors" = round(subfield_summary$avg_coauthors, rounding),
    "Single-author ratio" = round(subfield_summary$single_ratio, rounding),
    "Women's homophily" = round(subfield_summary$wprob, rounding),
    check.names = F
) %>%
  arrange(desc(FAR))

df$Subfield = recode_factor(
  df$Subfield,
  "Artificial Intelligence" = "AI",
  "Theoretical Computer Science" = "Theoretical CS",
  "Computer Architecture" = "Architecture",
  "Computer Science Education" = "CS Education",
  "Computational Linguistics" = "CL",
  "Data Science & Mining" = "DS & Mining",
  "High-Performance Computing" = "HPC",
  "Human-Computer Interaction" = "HCI",
  "Information Retrieval" = "IR",
  "Programming Languages" = "PL",
  "Operating Systems" = "OS",
  "Software Engineering" = "SE"
)

df <- bind_rows(
  df,
  data.frame(check.names = F,
    "Subfield" = c("Overall mean", "FAR correlation"),
    "FAR" = c(round(sum(gen_auths$gender == "F") / nrow(gen_auths), rounding), 1.0),
    "Mean productivity" = c(round(mean(full_auths$prod), rounding),
                            round(cor(subfield_summary$FAR, subfield_summary$avg_prod), rounding)),
    "Fractional productivity" = c(round(mean(full_auths$fractional_prod), rounding),
                                  round(cor(subfield_summary$FAR, subfield_summary$avg_frac), rounding)),
    "Mean coauthors" = c(round(mean(full_auths$coauthors + 1), rounding),
                         round(cor(subfield_summary$FAR, subfield_summary$avg_coauthors), rounding)),
    "Single-author ratio" =c(round(mean(subfield_summary$single_ratio), rounding),
                            round(cor(subfield_summary$FAR, subfield_summary$single_ratio), rounding)),
    "Women's homophily" = c(round(mean(subfield_summary$wprob), rounding),
                            round(cor(subfield_summary$FAR, subfield_summary$wprob), rounding))
  )
)

df %>%
  knitr::kable(format = "latex",
               booktabs = T,
               align = "lrrrrrr",
               linesep = c(rep("", nrow(df) - 3), "\\addlinespace"),
               caption = "Comparison of subfields by different gender metrics, ordered by FAR. Metrics include mean productivity (papers per author), fractional paper total, mean total coauthors (with repeats), ratio of papers written by a single author, and the ratio between a woman's probability to coauthor with a woman and FAR.") %>%
  column_spec(1, width = "2.0cm") %>%
  column_spec(3:7, width = "1.5cm") %>%
  kable_styling(font_size = 7)
```

We can extend this analysis of correlation with FAR to three other research questions we asked: collaboration size, single-author counts, and gender homophily.

A hypothetical relationship between FAR and the typical collaboration size in a subfield can also be easily refuted with counter examples.
Consider the subfields of algorithms and architecture.
Although their FAR values are nearly identical, they are on extreme ends of the average team sizes.
The overall correlation between the two metrics is indeed negative, but too close to zero for significance.
(`r report_test(cor.test(subfield_summary$FAR, subfield_summary$avg_coauthors))`).

From the related perspective of single-author papers, a similar hypothesis would be that subfields with more single-author papers would have lower FAR, because single-author papers have an even lower FAR than the overall sample population.
But the small number of single-author papers and their minuscule weight in computing FAR present a statistical obstacle to testing this hypothesis.
As a matter of fact, our data shows a positive but nonsignificant correlation between the two
(`r report_test(cor.test(subfield_summary$FAR, subfield_summary$single_ratio))`), so we must reject this hypothesis as well.

Our last metric for comparison is gender homophily.
The method we previously used to measure homophily, deviation from the expectation (FAR), produces two measures per field, one for men and one for women.
We focus on the latter because the deviation from FAR for men is nearly negligible, owing to the high ratio of men in the data.
We therefore look at "women's homophily", defined as the ratio between a subfield's probability for a woman to coauthor with a woman and its FAR.
This variable too appears uncorrelated with FAR
(`r report_test(cor.test(subfield_summary$FAR, subfield_summary$whomo))`).
This finding is explained by the already low statistical probability of a woman (or anyone) to collaborate with a woman.

This high skew towards men also means we cannot use standardized metrics for homophily such as Krackhardt's Index [@krackhardt88:informal], defined simply as
$EI=\frac{External - Internal}{External + Internal}$
(where $Internal$ represents all the same-gender pairings in our dataset and $External$ all other pairings).
In our data, this metric is strongly correlated with a subfield's FAR
(`r tmp <- pairs_with_field %>% group_by(subfield) %>% summarize(ei_index = first(ei_index)) %>% right_join(subfield_summary); report_test(cor.test(tmp$FAR, tmp$ei_index))`),
to the point of adding no valuable information.
This finding also makes sense: if there are very few women in a field, most coauthor pairings will be internal male-male, all other things being equal.
The upshot here is that homophily measures appear to be much more the result of a skewed FAR than its cause.
In other words, gender homophily appears to have little role in explaining variations in FAR, at least when it deviates significantly from 50% as it does in our dataset.


In summary, none of the collaboration metrics we collected in the various research questions can produce a satisfactory explanation for FAR in a given subfield.
For productivity (RQ2, RQ3), the correlation we found can only explain a small fraction of the large gender gap.
For team sizes (RQ4, RQ5, RQ6) we found no correlation with FAR.
And for gender homophily (RQ7), the strong correlation suggests, if anything, an opposite-direction causal relationship, i.e., low FAR causing high homophily, and not the other way around.


Despite these negative results, we can still draw some interesting general observations on specific CS fields.
One such example is theoretical computer science, which shows extreme values in many of the metrics we collected.
Not only does it exhibit the lowest FAR, but also the lowest average team size, highest single-author ratio, and the highest fractional productivity.
In these metrics it matches the observations of Mihaljevic for mathematicians, which is not surprising, given the thematic similarity of the two fields [@mihaljevic16:effect].
Theoretical CS is an extreme point by almost all metrics, and resembles closely what has been found for mathematics.

Another example is the field of systems, which also generally exhibits very low FAR.
Systems is a large and influential field, with many industrial and technological applications [@frachtenberg20:survey].
It is therefore particularly of interest to try to explain and reduce the large gender gap, as this could have far-reaching societal impact [@nielsen17:opinion].
As discussed in RQ6, one possible explanation for the large gender gap is the high cost in participation in experimental fields such as systems, which has been associated with lower diversity [@duch12:possible].
It is also possible that the combination larger team sizes, fewer single-author papers, and stronger gender homophily makes systems a particularly unwelcoming field because of self-reinforcing network effects [@araujo17:specific].
Teams are large but mostly-male, which discourage women seeking female collaborators from participating, which in turns worsens the problem for the next woman.

It has also been suggested that women are more likely to work in human-centered fields [@diekman13:navigating; @fisher02:unlocking; @sax19:disciplinary].
We can certainly observe corroboration of this hypothesis in our data, with CS education and HCI as the two fields with the highest FAR, followed by WWW and multimedia, which also put the human in the center of the research.
Our FAR figure for CS education is in fact is remarkably similar to the one found by West et al. for the field of education as a whole [@west13:role].


<!-------------------------------------------------------------------------------->
<!-------------------------------------------------------------------------------->

# Conclusion and future work {#sec:conclusion}

Computer science is a collaborative discipline.
Many papers exhibit larger team sizes than is typical in most other scientific disciplines, and very few papers are written by single authors.
Understanding collaboration patterns in CS authorship is therefore imperative to understanding related bibliometric and societal phenomena, such as the large gender gap in CS.
But research in CS is also highly heterogeneous across fields and subfields.
Although a number of papers have studied collaboration patterns in CS and the sciences, and even more have studied the gender gap, this study may be the first to examine the two at a finer resolution of CS subfields, and with highly accurate gender data.

Because subfields vary so much by their representation of female researchers---varying from 6% to 42%---we can examine how these variations relate to different collaboration metrics, especially in the extremes.
Our findings do confirm that some collaboration patterns appear indeed to be associated with the gender gap in each subfield.
Fields that exhibit lower FAR than average also tend to exhibit larger team sizes, smaller cohort sizes for women, higher gender homophily, and higher author productivity.
And although we found a significant productivity gap across genders, as has been measured in many other fields, it is too small to explain most of the overall gender gap across fields.

All of these associations have exceptions, and perhaps the most notable is the subfield of theoretical CS, which stands alone in the extreme of most metrics, perhaps being better categorized as a subfield of mathematics than of CS.
On the opposite end, CS subfields that have increased focus on people rather than computers, such as CS education and HCI, show better overall representation of women and less extreme gaps across most metrics.
On the other hand, many of the subfields of computer systems, which focuses on building, measuring, and improving the concrete implementation of computers and their tools, show larger gender gaps across most metrics.

This last field of computer systems is of particular interest to us and the focus of our future work, because of its large size, its impact on technology and the economy, and its very low representation of women, at about 10%.
We will therefore turn our attention next not just on statistical association with low FAR, but on causal association.
We plan to collect and analyze additional data to try to address the question: why is the representation of women in computer systems lower than in most other CS fields?
We have already presented a few hypotheses for this question in this article: the low representation could be partially explained to the higher cost of research in the field and the network effects that perpetuate the lack of female peers and mentors.
These hypotheses require additional data before we can accept or reject them.

This dataset currently exposes facts and factors from a single year, 2017.
It could also be instructive to follow up on this study with data collected from later years to observe any trends and changes in collaboration patterns and representation of women.
Additional future work can leverage the dataset we collected to dive deeper into the analysis of collaboration patterns, using tools such as social network analysis, degree centrality, citation networks, and information diffusion.
