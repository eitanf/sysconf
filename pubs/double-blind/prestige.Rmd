---
output: 
  bookdown::pdf_book:
    keep_tex: true
    template: main.tex
    fig_caption: true
title: Evaluating the relationships between author reputation and single-blind peer review
bibliography: ../sysconf.bib
csl: ieee.csl
abstract: |
 The integrity of peer review is essential for modern science. Numerous studies have therefore focused on identifying, quantifying, and mitigating biases in peer review. One of these better-known biases is prestige bias, where the recognition of a famous author or affiliation leads reviewers to subconsciously treat their submissions preferentially.

 A common mitigation approach for prestige bias is double-blind reviewing, where the identify of authors is hidden from reviewers. Studies on prestige bias often compare article acceptance statistics between single- and double-blind reviews, under the assumption that increased acceptance of famous authors in single-blind reviews implies prestige bias. Some such studies, both experimental and observational, have found famous authors to either be strongly associated with single-blind, while others found no such association or even an opposite link. Few of these studies are directly comparable to each other, leading to difficulty in generalization of their results.

 In this paper, we explore the large design space for such studies. Using an observational approach with a large dataset of peer-reviewed papers in computer systems, we systematically evaluate the effects of different prestige metrics, aggregation methods, and control variables. We show that depending on these choices, the data can lead to contradictory conclusions with high statistical significance. The implication for future study designs in this area is that a narrow evaluation may lead to unreliable results.
---

```{r code = readLines("../load_data.R"), echo=F, message=F}
```


```{r setup, echo=F, message=F, warning=F, cache=F}
library("GGally")
library("kableExtra")
library("lme4")
library("lmerTest")
library("corrplot")
library("RColorBrewer")

###############
# Create author-level datasets of interest
###############

# ppl: row = author on one of the pubs
ppl <-  roles %>%
  left_join(persons) %>%
  filter(role == "author") %>%
  mutate(paper_cites = citedby / npubs)

# ppl.lead: row = lead author on a pub (so 1 row per pub)
ppl.lead <- left_join(roles, persons) %>%
  filter(role == "lead_author") %>%
  mutate(paper_cites = citedby / npubs)

# ppl.last: row = last author on a pub (so 1 row per pub)
ppl.last <- left_join(roles, persons) %>%
  filter(role == "last_author") %>%
  mutate(paper_cites = citedby / npubs)

######################
# Imputation
######################

# There are some 31 authors with 1000+ npubs, which would mean nearly one publication per week for 20 years. Clearly, many of their pubs on GS are not theirs or not real pubs. So we remove or impute those. We also remove those where the ratio of npubs to s2npubs is too large.

# Arbitrary thresholds
# Comparing two sources for npubs
THRESH_RATIO <- 3
# Bound on S2 since have lots of name disambiguation issues
S2_THRESH <- 1000

# Add an indicator for not missing npubs and npubs isn't too much larger than 
# other source
cleaning_imputation <- function(dat, ratio = THRESH_RATIO, s2_limit = S2_THRESH)
{
  dat <- dat %>% 
    mutate(imp = is.na(npubs) | npubs/s2npubs >= ratio & s2npubs <= s2_limit) %>%
    mutate(npubs_all_good = na_if(npubs/s2npubs < ratio | s2npubs/npubs < ratio, npubs))

    
  # Create training dataset for imputation
  dat_train <- dat %>%
  filter(!imp, !is.na(s2npubs), s2npubs/npubs < ratio, s2npubs < s2_limit) %>%
  group_by(name, gs_email) %>%    # Since many authors are in the dataset several times
  summarize(npubs = first(npubs), s2npubs = first(s2npubs),
            hindex = first(hindex)) %>%
    ungroup()

  # Build imputation models
  # Linear imputation
  model.npubs_from_s2npubs <- lm(data = dat_train, npubs ~ s2npubs)
  model.hindex_from_s2npubs <- lm(data = dat_train, hindex ~ s2npubs)
  
  # Impute (but truncate predictor s2npubs at S2_THRESH)
  dat <- dat %>%
  mutate(npubs_imp = ifelse(!imp, npubs, predict(model.npubs_from_s2npubs,
                                                 data.frame(s2npubs = c(pmin(s2npubs, s2_limit)))))) %>%
  mutate(hindex_imp = ifelse(!imp, hindex, predict(model.hindex_from_s2npubs,
                                                   data.frame(s2npubs = c(pmin(s2npubs, s2_limit))))))
  
  
  # Flat imputation
  dat %>%
    mutate(npubs_flat = ifelse(!imp, npubs, 1),
           hindex_flat = ifelse(!imp, hindex, 0))
}

# Impute for our author datasets
ppl <- ppl %>%
  cleaning_imputation(THRESH_RATIO, S2_THRESH)

ppl.last <- ppl.last %>%
  cleaning_imputation(THRESH_RATIO, S2_THRESH)

ppl.lead <- ppl.lead %>%
  cleaning_imputation(THRESH_RATIO, S2_THRESH)

########################
### Find PC members
########################

any_pc <- ppl %>%
  filter(!imp) %>%
  group_by(as.factor(key)) %>%
  summarize(conf = as.character(first(conf)), any_pc = sum(as_pc) > 0) %>%
  left_join(select(all_confs, c(key, double_blind)), by = c("conf" = "key"))

db_confs = filter(all_confs, double_blind)
sb_confs = filter(all_confs, !double_blind)

########################
# compute_experience: Compute the collection of experience (reputation) metrics by paper
compute_experience <- function(who, fame_percentile = 0.9)
{ 
  who %>%
    mutate(key = as.factor(key)) %>%
    group_by(key) %>%
    summarize(conf = as.character(first(conf)), 
              coauthors = n(),
              top_university = any(top_university),
              top_company = any(top_company),
              across(c(as_author, as_pc, hindex, i10index, citedby, paper_cites, npubs, s2npubs,
                       npubs_imp, hindex_imp, imp, npubs_flat, hindex_flat),
                     list(max = ~max(.x, na.rm = T),
                          mean = ~mean(.x, na.rm = T),
                          median = ~median(.x, na.rm = T),
                          total = ~sum(.x, na.rm = F),
                          first = ~first(.x),
                          last = ~last(.x),
                          top = ~max(.x, na.rm = T) >= quantile(who[[cur_column()]], fame_percentile, na.rm = T)
                         ),
                     .names = "{fn}_{col}"
                     )
    ) %>%
    left_join(all_confs, by = c("conf" = "key")) %>%
    mutate_at(c(7:ncol(.)), ~ifelse(is.infinite(.) | is.nan(.), NA, .)) %>%
    mutate(top_npubs = ifelse(is.na(max_npubs), NA, top_npubs),
           top_hindex = ifelse(is.na(max_hindex), NA, top_hindex),
           top_i10index = ifelse(is.na(max_i10index), NA, top_i10index),
           top_citedby = ifelse(is.na(max_citedby), NA, top_citedby),
           top_paper_cites = ifelse(is.na(max_paper_cites), NA, top_paper_cites),
           top_s2npubs = ifelse(is.na(max_s2npubs), NA, top_s2npubs))
}

experience.all <- compute_experience(ppl)
experience.lead <- compute_experience(ppl.lead)
experience.last <- compute_experience(ppl.last)
experience.best <- compute_experience(ppl %>% filter(npubs_all_good))

total_db = nrow(filter(experience.all, double_blind))
total_sb = nrow(filter(experience.all, !double_blind))

# EDA
# ggplot(data = experience.all, aes(x = max_npubs, y = max_npubs_imp)) + 
#   geom_point(alpha = 0.1)
# sum(experience.all$max_npubs == experience.all$max_npubs_imp, na.rm = TRUE)
# mean(experience.all$max_npubs == experience.all$max_npubs_imp, na.rm = TRUE)
# mean(experience.lead$max_npubs == experience.lead$max_npubs_imp, na.rm = TRUE)
# mean(experience.last$max_npubs == experience.last$max_npubs_imp, na.rm = TRUE)



########################
# over_thresh: compute how many papers in SB and DB conferences have at least one author with [[metric]] at thresh or higher
over_thresh <- function(metric, thresh)
{
  experience.all$over <- experience.all[[metric]] >= thresh
  c("db" = nrow(filter(experience.all, over, double_blind)),
    "sb" = nrow(filter(experience.all, over, !double_blind)),
    "p.value" = chisq.test(table(experience.all$over, experience.all$double_blind))$p.value
   )
}


########################
# compute_db: for a given population, aggregation, and metric, return a vector with different
# statistics on the difference of means and medians for the metric between SB and DB
compute_db <- function(exper = experience, aggr = "max", metric = "npubs")
{
  column <- paste0(aggr, "_", metric)
  sb <- filter(exper, !double_blind)
  db <- filter(exper, double_blind)
  ttest <- t.test(sb[[column]], db[[column]])
  wtest <- wilcox.test(sb[[column]], db[[column]])

  c(aggregation = aggr,
    metric = metric,
    mean_sb = round(mean(sb[[column]], na.rm = T), 2),
    mean_db = round(mean(db[[column]], na.rm = T), 2),
    median_sb = round(median(sb[[column]], na.rm = T), 2),
    median_db = round(median(db[[column]], na.rm = T), 2),
    t_stat = round(ttest$statistic, 3),
    t_pval = ttest$p.value,
    w_stat = round(wtest$statistic, 3),
    w_pval = wtest$p.value
  )
}

########################
report_db <- function(exper = experience, aggr = "max", metric = "npubs")
{
  tmp <- compute_db(exper, aggr, metric)
  paste0("mean SB: ", tmp["mean_sb"],
         ", DB: ", tmp["mean_db"],
         "; $t=", tmp["t_stat.t"],
         "$, ", format_p_value(as.numeric(tmp["t_pval"])))
}

########################
# compute_glm: for a given population and metric, return a vector with different statistics on
# the regression of the metric by double-blindess (linear and logistic and linear mixed model).
# The function can take an optional additional predictor variable to include in the model.
compute_glm <- function(exper = experience.all, predictor = "", metric = "npubs")
{
  exper$single_blind <- !exper$double_blind
  idx = ifelse(predictor == "", 2, 3)

  col <- paste0("max_", metric)
  form = as.formula(paste(col, "~", predictor, ifelse(predictor == "", "", "+"), "single_blind"))
  model <- glm(data = exper, form)
  linear_coef <- model$coefficients[idx]
  linear_pval <- coef(summary(model))[idx, 4]

  col <- paste0("top_", metric)
  form = as.formula(paste(col, "~", predictor, ifelse(predictor == "", "", "+"), "single_blind"))
  model <- glm(data = exper, form, family = "binomial")
  logit_coef <- model$coefficients[idx]
  logit_pval <- coef(summary(model))[idx, 4]
  

  #LMM
  col <- paste0("max_", metric)
  form = as.formula(paste(col, "~", predictor, ifelse(predictor == "", "", "+"), "single_blind + (1|conference)"))
  model <- lmerTest::lmer(data = exper, formula = form)
  linear_mixed_coef <- coef(summary(model))[idx, 1]
  linear_mixed_pval <- coef(summary(model))[idx, 5]  
  
  #GLMM
  col <- paste0("top_", metric)
  form = as.formula(paste(col, "~", predictor, ifelse(predictor == "", "", "+"), "single_blind + (1|conference)"))
  model <- glmer(data = exper, formula = form, family = binomial)
  logit_mixed_coef <- coef(summary(model))[idx, 1]
  logit_mixed_pval <- coef(summary(model))[idx, 4] 
  
  c(metric = metric,
    predictor = predictor,
    linear_coef = linear_coef,
    linear_pval = linear_pval,
    logit_coef = logit_coef,
    logit_pval = logit_pval,
    linear_mixed_coef = linear_mixed_coef,
    linear_mixed_pval = linear_mixed_pval,
    logit_mixed_coef = logit_mixed_coef,
    logit_mixed_pval = logit_mixed_pval
  )
}

########################
# For a given metric in a data frame df, compute the maximum absolute value of given column
metric_denominator <- function(df, met, col)
{
  filter(df, metric == met) %>% select(all_of(col)) %>% abs() %>% max()
}
```

<!-------------------------------------------------------------------------------------------->
<!-------------------------------------------------------------------------------------------->

# Introduction {#sec:intro}


<!-- send to?
  PLOS One
  PeerJ/CS
  IEEE Access
  J. of the Assoc. for Inf. Science. and Tech. (https://asistdl.onlinelibrary.wiley.com/journal/23301643)
  Journal of Information Science (https://journals.sagepub.com/home/jis)
  Quant. Science Studies (https://www.mitpressjournals.org/loi/qss)
  Scientometrics (https://www.springer.com/journal/11192)
  Trans. Prof. Comm. (https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=47) 
  Journal of Scientometric Research
  JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY
 -->

Numerous studies attempt to answer the question of whether prestige bias exists in the peer review process for a particular discipline of science [blank91:blind; @budden08:double; @cox19:cases; @fisher94:effects; @kalavar21:single; @okike16:single; @peters82:peer; @seeber17:newcomers; @tomkins17:reviewer].
This paper began as one of these studies, focusing on the field of computer systems.
We soon realized that our particular dataset was insufficient to provide conclusive evidence for or against prestige bias.
We also realized that the limitations we encountered may actually be shared by many of these studies and had not been well documented before.
The main focus of our study thus became the exploration of how subtle changes in the data and methods used to detect prestige bias can radically change the resulting outcome.

But first, what is prestige bias, and why should we care about it?
Scientists normally publish their findings in peer-reviewed venues such as journals and conferences.
Their work is judged by a small set of peers from their field, who typically remain anonymous so that their critique can remain free from author influence.
This so-called single-blind (SB) reviewing has been widely accepted as the baseline standard of credibility in the scientific process [@hames08:peer].
However, single-blind reviewing still lets reviewers know the identities of the work's authors.
Several researchers have argued that this knowledge can affect the review outcomes in subjective ways.
In particular, when reviewers encounter a famous author or affiliation they may subconsciously experience authority bias, where higher accuracy is attributed to the opinion of authority figures [@ramos17:biases]. 
This bias, also known as "the halo effect", "status bias", or "prestige bias", the term we use in this paper.
It has been implicated as an important but unscientific factor in the review process, which in turn can lead to bad science, lower credibility for the scientific process as a whole, and an exacerbation of the underrepresentation of minorities in research [@tomkins17:reviewer].

A commonly suggested antidote to prestige bias is double-blind (DB) reviewing, where authors' identities and affiliations are hidden from reviewers.
Not all venues employ DB reviewing, both for practical and principled considerations.
However, the coexistence of SB and DB reviewing (sometimes in the same venue) opens the door to comparative studies of the effects of prestige bias.
For example, the association between reviewing policy and various biases has been assessed in fields as disparate as economics [blank91:blind], behavioral ecology [@budden08:double; @cox19:cases], ophthalmology [@kalavar21:single], medicine [@fisher94:effects; @okike16:single], psychology [@peters82:peer], and computer science [@seeber17:newcomers; @tomkins17:reviewer].
Some of these studies have found conclusive evidence for bias while others have found no significant evidence of bias, sometimes in the same field.
Often, these studies were not directly comparable because of various differences in the data, methodologies, or metrics used.
One limitation of these studies is that there are various ways to estimate a person's name recognition, and they all likely yield different results.
Another limitation is that the reviewing policy of a venue is not always independent from other factors, so DB reviewing could be confounded with other factors that affect submission or acceptance decisions.
When we tried to apply similar methodologies to another broad field of research, computer systems, we discovered that the outcome can vary significantly based on the methodology and metrics used.

The main contribution of this paper is therefore an analysis of the sensitivity of the association between reviewing policy and prestige bias.
To emphasize, this study does not attempt to directly address the question of the existence of prestige bias in this field.
Instead, we focus on exposing and analyzing the factors that could change the outcomes of studies that do attempt to address this question.
These factors include questions such as: how to estimate the prestige or reputation of authors? where should data be sourced from, and how should it be cleaned? and how should we correct for confounding variables like conference prestige?
A meaningful analysis of review bias requires a careful consideration of these factors, and a cognizant effort to address the sensitivity of the outcomes to these choices.
Understanding these factors can help accomplish more reliable and stable results in future studies on bias in general, and prestige bias in particular.

The rest of this paper is organized as follows.
We start by describing the extensive dataset we collected on our field of focus in Sec. \@ref(sec:data).
The next section then describes empirical evidence for and against prestige bias, varying based on the types of metrics, their aggregations, statistical analyses performed, and data cleaning choices.
This section demonstrates how contradictory but statistically significant results can be obtained by different approaches, despite working on the very same dataset.
We elaborate and discuss these findings and practical recommendations in Sec. \@ref(sec:discussion).
Finally, we review related work in Sec. \@ref(sec:related) and conclude in Sec. \@ref(sec:conclusion).


<!--

## Challenges: {-}

 - How do we measure researcher prestige? h-index? npubs? s2npubs? other?
 - Selection bias: most authors with missing h-index data are also less famous.
 - Survivorship bias: The submission rate of famous people in S.B and D.B conferences may not be the same as the acceptance rate.
 - Confounding conference variables: Other conference factors, like conference prestige, may be associated with S.B/D.B but much more influential in the submission choice of famous authors.
 - Computation of S2npubs is bad for people with shared names (predominantly Chinese), and therefore the imputation is as well.

-->

<!-------------------------------------------------------------------------------------------->
<!-------------------------------------------------------------------------------------------->

# Materials and methods {#sec:data}

The primary dataset we analyze comes from a hand-curated collection of `r nrow(all_confs)` peer-reviewed systems conferences from a single publication year (2017).
In CS, and in particular in its more applied fields such as systems, original scientific results are typically first published in peer-reviewed conferences [@franceschet10:role; @freyne10:relative; @patterson99:evaluating; @vardi09:conferences], and then possibly in archival journals, sometimes years later [@vrettas15:conferences].
The conferences we selected include some of the most prestigious systems conferences (based on indirect measurements such as Google Scholar's metrics), as well as several smaller or less-competitive conferences for contrast (Table \@ref(tab:sys-confs)).
We chose to focus on a large cross-sectional set of conferences from a single publication year (2017), to reduce variations in time.
Our choice of which conferences belong to "systems" is necessarily subjective.^[For the purpose of this study, we define systems as the study and engineering of concrete computing systems, which includes research topics such as: operating systems, computer architectures, data storage and management, compilers, parallel and distributed computing, and computer networks.]
Not all systems papers from 2017 are included in our set, and some papers that are in our set may not necessarily be considered part of systems (for example, if they lean more towards algorithms or theory).
However, we believe that our cross-sectional set is both wide enough to represent the field well and focused enough to distinguish it from the rest of CS.
In total, our sample includes `r sum(all_confs$npapers)` accepted peer-reviewed papers.

```{r sys-confs, echo=F, message=F, warning=F, cache=T}
tmp <- all_confs %>%
  mutate(Conference = gsub("_\\d*", "", conference)) %>%
  rename(Date = postdate, Papers = npapers, Authors = authors_num, Country = country) %>%
  mutate(Acceptance = round(acceptance_rate, 2)) %>%
  select(Conference, Date, Papers, Authors, Acceptance) %>%
  arrange(Conference)
  
cbind(tmp[1:(nrow(tmp)/2),], tmp[(1+nrow(tmp)/2):nrow(tmp),]) %>%
  knitr::kable(booktabs = T, linesep = "",
               align = c("|l", "c", "r", "r", "r|"),
               caption = "System conferences, including start date, number of published papers, total number of named authors, and acceptance rate.") %>%
  kable_styling(font_size = 8)
```

To address decisively the question of prestige bias in computer systems, we would need to compare the prestige of accepted authors to that of rejected authors, for which we have no information.
The information we collected measures publication rates for authors, not acceptance rates, so our observations do not lead to conclusive claims on which papers/authors are accepted more.
Higher publication rate of certain authors does not necessarily imply a higher acceptance rate, because, for example, these authors could be submitting more papers to these conferences.
But the two metrics are nevertheless related: all else being equal, a higher acceptance rate will imply a higher publication rate.
So instead, we concentrate not on the existence of prestige bias, but on a related question:
Is there a statistically significant difference in the rates of famous authors published in SB or DB conferences?

To answer this question, we collected extensive information to help us estimate the prestige of authors and conferences.
For each conference, we downloaded all papers and gathered information about all authors, program committee (PC) members, and other roles.
Conferences do not generally offer information on authors' demographics, but we were able to unambiguously link approximately two thirds (`r pct(nrow(filter(persons, !is.na(hindex))), nrow(persons))`%) of
researchers in our dataset to a Google Scholar (GS) profile [@vine06:google].
For each author and PC member, we collected all metrics in their GS profile, such as total previous publications (ca. 2017), h-index, etc.
Note that we found no GS profile for about a third of the researchers
(`r pct(sum(is.na(authors$hindex)), nrow(authors), 2)`%),
and these researchers appear to be less experienced than researchers with a GS profile.
We therefore also considered another proxy metric for author experience (total number of past publications) from another source, the Semantic Scholar database [@fricke18:semantic].

In addition to researcher information, we gathered various statistics on each conference, either from its web page, proceedings, or directly from its chairs.
These data included review policies, important dates, the composition of its technical PC, and the number of submitted papers.
We also collected historical metrics from the Institute of Electrical and Electronics Engineers (IEEE), Association for Computing Machinery (ACM), and Google Scholar websites, including past citations, age, and total publications, and downloaded all `r nrow(papers)` papers.

<!-------------------------------------------------------------------------------------------->
## Statistics

For statistical testing, group means were compared pairwise using Welch's two-sample t-test; differences between distributions of two categorical variables were tested with the $\chi^{2}$ test; differences between distributions of medians were tested with the Wilcoxon Signed Rank Test; and comparisons between two numerical variables were evaluated with Pearson's product moment correlation coefficient.
All statistical tests are reported with their p-values.

<!-------------------------------------------------------------------------------------------->
## Code and data availability

For reproducibility, all of the data and source code files we used can be found  in the supplementary information and at [https://github.com/eitanf/sysconf/](https://github.com/eitanf/sysconf/).
The specific statistical analyses for this study can be found in the file
`pubs/double-blind/prestige.Rmd`.

<!-------------------------------------------------------------------------------------------->
<!-------------------------------------------------------------------------------------------->


# Empirical results {#sec:results}

In this section, we explore the observed relationships between reputation and reviewing policy across four design dimensions, or factors: metrics of reputation, aggregation of prestige across coauthors, additional explanatory (confounding) variables, and approaches to data cleaning.

<!--
Because of the complexity introduced by multiple independent factors, we start with a common set of choices for comparisons and investigate the effect of each factor and level in relation to this baseline.
The baseline we chose is to approximate author reputation by their total number of past publications. 
Specifically, we look for the highest number of past publications among a paper's group of coauthors, ignoring any confounding variables.
This method has been commonly used in other studies as well [@fisher94:effects; @madden06:impact; @tomkins17:reviewer; @tung06:impact].
-->


<!-------------------------------------------------------------------------------------------->
## Reputation metrics {#subsec:metrics}

Author reputation is an intangible, abstract concept. Nevertheless, the quantitative methodology that we focus on requires a concrete approximation.
As a proxy for the intangible reputation, most bibliometric studies employ either metrics based on productivity (publication counts), on impact (citation metrics), or both [@carpenter14:metrics; @dettori19:measuring].
We chose an initial list of nine such metrics to analyze, whose distributions are depicted in Fig. \@ref(fig:metric-distribs).

This list is certainly not exhaustive; the debate on their relative merits and effectiveness is an ongoing research topic, and new and improved metrics are proposed constantly. But this list represents some of the most commonly used metrics, and as our results show, is sufficient to surface diverse and even contradictory results in the context of reviewing policy. 

We next describe these metrics and their characteristics in our dataset.

```{r metric-distribs, echo=F, cache=T, warning=F, message=F, fig.cap="Histograms of the author-level prestige proxies.  The number of non-missing values (n) is provided for each variable."}

metrics.summary <- ppl %>%
  pivot_longer(cols = c("npubs", "hindex", "citedby", "paper_cites", "hindex5y", "i10index", "i10index5y", "s2npubs", "as_author"), names_to = "Metric") %>%
  mutate(Metric = recode_factor(Metric,
                                npubs = "Total publications (GS)", hindex = "h-index", 
                                citedby = "Total citations",
                                s2npubs = "Total publications (S2)", 
                                as_author = "Papers in dataset", 
                                paper_cites = "Citations per paper",
                                i10index = "i10-index", hindex5y = "h-index (5 years)",
                                i10index5y = "i10-index (5 years)")) %>%
  group_by(name, gs_email, Metric) %>%
  summarize(Metric = first(Metric), value = first(value), .groups = "keep") %>%
  group_by(Metric) %>%
  mutate(count = paste0("n = ", sum(!is.na(value)))) %>%
  ungroup()

ggplot(metrics.summary,
       aes(x = value)) +
  geom_histogram(bins = 50) +
  geom_text(aes(x = Inf, y = Inf, hjust = 1.1, vjust = 1.5, label = count)) +
  theme_minimal() +
  xlab("Metric value") +
  ylab("Count") +
  facet_wrap( ~ Metric, scales = "free")
```


<!---------------------------------------------------->
### Publication counts

Counting the number of past publications is ostensibly the most straightforward way to quantify how well-known an author is. After all, the more papers that carry their name, the more likely it is that their name is recognized by reviewers. But the relationship is hardly linear in practice:

 * Not all papers are equally well-known, read, or cited (a problem that the impact-based metrics attempt to address).
 * Rising researchers may be receiving significant name recognition without having yet accumulated a long publication record.
 * The decision of what counts as a "publication" can be subjective and vary from database to database. For example, do patents count? Arxiv preprints? blog posts?
 * Accurate collection of even the raw data is complicated by factors such as author name disambiguation, paper disambiguation, and noisy or crowd-sourced data.
 * Systems papers in particular vary significantly in number of coauthors because some implementations take a large team effort. Credit attribution therefore becomes particularly tricky in this field, even when using h-index [@koltun21:hindex].

With these limitations in mind, we start our analysis by looking at the count of previous publications for each author.
For each accepted paper, we looked at the publication count of the most prolific author to capture the potential name recognition for the paper as a whole.
In our dataset, authors in SB conferences average a higher GS publication count than in DB conferences
(`r report_db(experience.all, "max", "npubs")`).
<!-- Also holds when we impute GS (`r report_db(experience.all, "max", "npubs_imp")`) -->
<!-- Also holds when we impute GS (`r report_db(experience.all, "max", "npubs_flat")`) -->

This relationship appears to support a hypothesis of prestige bias for these conferences (to reiterate, there is insufficient evidence for a conclusive causal relationship).
When we compare the publication counts from our other data source, S2, which covers all authors, we find similar evidence, albeit weaker:
(`r report_db(experience.all, "max", "s2npubs")`).

We can also use a third source of paper counts to differentiate between active and mostly inactive researchers, using only current statistics.
If we look only at the number of papers published in our cross-section of conferences from 2017, we find that SB conference authors actually average significantly *fewer* publications than those in DB conferences
(`r report_db(experience.all, "max", "as_author")`).
This result could provide evidence against a prestige bias, but it could also point to a completely different hypothesis, such as authors preferring to submit to DB conferences for these specific conferences and point in time.
The evidence based on publication counts alone is inconclusive.


<!---------------------------------------------------->
### Citation-based metrics

We next turn our attention to another set of commonly used metrics, those incorporating citations, which is one measure of impact.
Citation-based metrics try to capture the fact that not all publications have the same influence on other researchers, and indirectly, on the author's reputation.
Note that the correlation between total past citations and total past papers is not particularly high, leading potentially to divergent results when evaluating prestige bias
(Pearson correlation of
$r=`r round(cor(ppl$npubs, ppl$citedby, use = "complete.obs"), 2)`$
when using GS paper counts,
$r=`r round(cor(ppl$s2npubs, ppl$citedby, use = "complete.obs"), 2)`$
with S2 data, see Fig. \@ref(fig:metric-corrs)).

```{r metric-corrs, echo=F, warning=F, cache=T, message=F, fig.width = 3.5, fig.height = 3.5, fig.cap="Correlation matrix for the author-level prestige metrics."}
cors <- ppl %>%
  select(npubs, hindex, citedby, paper_cites, hindex5y, i10index, i10index5y, s2npubs, as_author, name, gs_email) %>%
  distinct() %>%
    select(-name, -gs_email) %>%
  rename(`Publications\n(GS)` = npubs,
         `Publications\n(S2)` = s2npubs,
         `h-index` = hindex,
         `Total\ncitations` = citedby,
         `Papers\nin\nset` = as_author,
         `Citations\nper\npaper` = paper_cites,
         `i10-index` = i10index,
         `h-index\n(5 years)` = hindex5y,
         `i10-index\n(5 years)` = i10index5y)


corrplot.mixed(cor(cors, use = "pairw"),
               tl.col = "black",
               tl.cex = .6,
               order = "hclust",
#               cl.pos = "n",
               lower.col = colorRampPalette(c("red", "white", "black"))(100),
               upper.col = colorRampPalette(c("white", "white", "black"))(100))
```


From GS, we collected for each author with an identifiable profile their total number of past citations, h-index, i10-index (number of papers with 10 or more citations), the h-index over the past 5 years and the i10-index over the past 5 years.
We omit the last two metrics from our discussion because of their high correlation with h-index and i10-index, respectively (Fig. \@ref(fig:metric-corrs)).
We also computed another metric from these statistics, citations per paper, by dividing total citations with total GS publications.

Once more, we look at the most senior coauthor in each paper group, and compare the averages across SB and DB papers.
Of these, the citations-per-paper metric shows the strongest association with DB reviewing
(`r report_db(experience.all, "max", "paper_cites")`).
The h-index metric also strongly indicated higher values in DB conferences
(`r report_db(experience.all, "max", "hindex")`).

The total citations metric shows a much weaker, nonsignificant association in the same direction
(`r report_db(experience.all, "max", "citedby")`),
while the i10 metric shows a nonsignificant association in the opposite direction
(`r report_db(experience.all, "max", "i10index")`).
Again, the evidence for or against prestige bias is inconclusive.

<!---------------------------------------------------->
### Program committee metrics

Two other reputation metrics that are not regularly reported or measured are overall participation of the authors in review roles.
Many studies on prestige bias focus on journal authors, where most reviewers remain unidentified, and editorial boards are relatively small.
But in CS, and systems in particular, where the main publication venue is conferences, most of the peer review is carried out by broad, publicized program committees (PCs).
A researcher's participation in these PCs is related to their reputation in two ways, as follows.

First, to be invited to serve in a PC, an author needs to typically be regarded as an authority in the field.
We can extend this intuition to reputation metrics, either as a quantitative one---how many PC in our set an author belongs to---or a qualitative one, simply noting whether they participate in any PC at all.
For the former, we observed that the most senior coauthors in each paper group (by PC participation) average marginally more PC roles in DB conferences
(`r report_db(experience.all, "max", "as_pc")`).
For the latter, we observed an opposite indication: the ratio of papers authored by at least one member of a PC in our set is actually lower for DB conferences than for SB
(`r pct(nrow(filter(any_pc, double_blind, any_pc)), nrow(filter(any_pc, double_blind)))`% vs.
`r pct(nrow(filter(any_pc, !double_blind, any_pc)), nrow(filter(any_pc, !double_blind)))`%;
`r report_test(chisq.test(table(any_pc$any_pc, any_pc$double_blind)))`).

Second, when a person submits a paper to a specific SB conference wherein they also serve on the PC, they are quite likely recognized by reviewers, who also compose the PC.
We can compute the percentage of papers in each conference where at least one of the authors participates in the PC for that same conference.
Comparing across review polices, we find no significant difference in the mean percentage for DB and SB conferences
(`r 100 * round(mean(filter(all_confs, double_blind)$pc_paper_ratio), 2)`% vs.
`r 100 * round(mean(filter(all_confs, !double_blind)$pc_paper_ratio), 2)`%;
`r report_test(t.test(filter(all_confs, double_blind)$pc_paper_ratio, filter(all_confs, !double_blind)$pc_paper_ratio))`).

In summary, neither PC metric for author reputation shows a strong association with the review policy of the conference.

<!-------------------------------------------------------------------------------------------->

## Paper summaries {#subsec:aggregation}

The analysis so far made two implicit assumptions on the likelihood that a paper would be recognized as "reputable": that the author with the highest reputation metric determines the reputation of the paper, and that the likelihood increases linearly with the value of the reputation metric.
In this section, we expand our analysis beyond these assumptions.
We first look at the effect of summarizing a paper's author reputation based on other statistics than maximum value.
Then, we look at qualitative (categorical) classifications of a paper's potential for recognition.

### Aggregations by paper

The typical systems paper has a large number of coauthors compared to other disciplines, perhaps because of its emphasis on concrete systems that can require significant team effort to implement.
In our dataset, papers average
`r co <- group_by(ppl, key) %>% summarize(nauthors = n()); round(mean(co$nauthors), 2)` authors per paper
(SD: `r round(sd(co$nauthors), 2)`).
We have assumed so far that a paper is most likely to be recognized based solely on the author with the highest reputation metric.
But other authors may play an important role in name recognition, as well as the whole group.

For example, the total reputation of the coauthor group could also play a potential role in the reviewer's bias.
It could therefore be interesting to compare total paper reputation to review policy (omitting all papers that had one or more missing reputation values, where a partial sum is meaningless).
We can alternatively look at the mean or median reputation (omitting only missing authors, not entire papers).
Finally, we can aggregate papers by looking only at the reputation of the first or last author.
Systems papers usually list the primary contributor in the first author position, while the head of the lab is often listed last; in our dataset, only
`r papers <- left_join(papers, co); p3 <- filter(papers, nauthors > 2); pct(nrow(filter(p3, alphabetized)), nrow(p3), 1)`%
of papers with three or more coauthors ordered the author list alphabetically.


```{r mean-diffs, echo = F, warning = F, fig.height = 3, fig.cap="Test statistics for difference in means with various reputation metrics and aggregations across papers using two-sided Student's t-test. Positive t-statistic values (purple) indicate higher reputation in SB conferences. Darker colors reflect larger-magnitude values, normalized by columns. Negative values (orange) indicate higher reputation in DB conferences. Stars indicate p values below 0.05, 0.01, and 0.001, respectively."}
tbl <- rbind(compute_db(experience.all, "max", "s2npubs"),
             compute_db(experience.all, "total", "s2npubs"),
             compute_db(experience.all, "mean", "s2npubs"),
             compute_db(experience.all, "median", "s2npubs")
            )

for (metric in c("npubs", "as_author", "citedby", "paper_cites", "hindex", "i10index")) {
  for (aggr in c("max", "total", "mean", "median")) {
    tbl <- rbind(tbl, compute_db(experience.all, aggr, metric))
  }
}

# Append lead-author stats
for (metric in c("s2npubs", "npubs", "as_author", "citedby", "paper_cites", "hindex", "i10index")) {
  tbl <- rbind(tbl, compute_db(experience.lead, "first", metric))
}
tbl <- ifelse(tbl == "first", "lead", tbl)

# Append last-author stats
for (metric in c("s2npubs", "npubs", "as_author", "citedby", "paper_cites", "hindex", "i10index")) {
  tbl <- rbind(tbl, compute_db(experience.last, "first", metric))
}
tbl <- ifelse(tbl == "first", "last", tbl)

rep.all <- as_tibble(tbl)
rep.all$metric <- factor(rep.all$metric, ordered = T,
                         levels = c("npubs", "s2npubs", "as_author", "citedby", "paper_cites", "hindex", "i10index"),
                         labels = c("Publications (GS)", "Publications (S2)", "Papers in set", "Total citations", "Citations per paper", "h-index", "i10-index"))
rep.all$aggregation <- factor(rep.all$aggregation, ordered = T,
                              levels = c("lead", "last", "median", "mean", "total", "max"),
                              labels = c("Lead author", "Last author", "Median", "Mean", "Total", "Maximum"))
rep.all$t_stat.t <- as.numeric(rep.all$t_stat.t)
rep.all$t_pval <- as.numeric(rep.all$t_pval)
rep.all$w_stat.W <- as.numeric(rep.all$w_stat.W)
rep.all$w_pval <- as.numeric(rep.all$w_pval)


ggplot(rep.all, aes(x = metric, y = aggregation)) +
  geom_tile(aes(fill = t_stat.t)) +
  scale_fill_gradient2(low = "orange", high = "blue") +
  geom_text(aes(label = paste0(round(t_stat.t, 2), format_p_value(t_pval, p_option = "stars")))) +
  ylab("Paper aggregation method") +
  xlab("Reputation metric") +
  labs(fill = "t statistic") +
  theme_minimal() +
  guides(fill = "none") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


Looking at all six aggregation methods across seven reputation metrics reveals a mixed picture (Fig. \@ref(fig:mean-diffs)).
The first row---using maximum reputation per paper aggregation---repeats the results we discussed in Sec. \@ref(subsec:metrics).
All other rows represent new aggregations.
Depending on one's choice of metrics and aggregations, one can measure statistically significant reputation bias, anti-bias, or nonsignificant bias.
Any statistically sound evidence to suggest bias can find a counter-argument just as sound in the same dataset.

Nevertheless, we can discern a few consistent properties from these comparisons:

 * The citations-per-paper and papers-in-set metrics indicate lower mean reputation in SB conferences across all paper aggregations. It is noteworthy because these two metrics are completely disjoint in time: one looks at the past, and one at the present.
 
 * Conversely, measuring reputation by number of publications results in higher mean reputation for SB conferences, regardless of aggregation or database used.
 
 * However, using the more complete S2 database lowers the t-statistic values and significance. This result may be unsurprising, as it incorporates more data on the lower side of the reputation distribution; but it serves as a cautionary tale for ascribing statistical significance from partial data.

 * Similarly, aggregating papers by the reputation of the lead author results in the least conclusive findings. We believe that many lead authors are relatively inexperienced students.
 
 * Looking at each paper's median reputation metric instead of mean leads to higher t-statistic values, i.e., higher indicated reputation in SB conferences and higher likelihood of prestige bias. The p-values are also lower, likely because the standard error of the median is smaller than that of the mean since it is less susceptible to outliers. 
 
 
Related to the last point, we can look not only at median aggregation per paper, but also across papers.
That is, we can compare how the median reputation per paper (however it is aggregated) differs between SB and DB conferences, again with the purpose of attenuating the effect of large right-skewed outliers.


```{r median-diffs, echo = F, warning = F, fig.height = 3, fig.cap="Difference in medians for various reputation metrics and aggregations across papers using two-sided Wilcoxon test. Positive values (purple) indicate higher reputation in SB conferences. Negative values (orange) indicate higher reputation in DB conferences. Darker colors reflect larger-magnitude values, normalized by columns. Stars indicate p values (from a Wilcoxon two-sided unpaired test) below 0.05, 0.01, and 0.001, respectively."}
rep.all$diff <- as.numeric(rep.all$median_sb) - as.numeric(rep.all$median_db)

rep.all %>%
  group_by(metric) %>%
  mutate(normalized = diff / metric_denominator(rep.all, metric, "diff")) %>%
  ggplot(aes(x = metric, y = aggregation)) +
  geom_tile(aes(fill = normalized)) +
  scale_fill_gradient2(low = "orange", high = "blue") +
  geom_text(aes(label = paste0(round(diff, 2), format_p_value(w_pval, p_option = "stars")))) +
  ylab("Paper aggregation method") +
  xlab("Reputation metric") +
  labs(fill = "Difference") +
  theme_minimal() +
  guides(fill = "none") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```





As Fig. \@ref(fig:median-diffs) shows, many of the metrics and paper aggregations agree in direction (sign) whether looking at the mean values across SB/DB or medians, although confidence levels vary significantly between the two.
Observe for example how the total-citations metric went from nonsignificant means to highly significant medians, perhaps because this metric's values are the most widely distributed and right-skewed, thereby creating large differences between means and medians.

The implication from this comparison is, again, that there exist combinations of metrics and aggregations, all arguably representing reasonable choices, that can change the outcome from statistical significance to nonsignificance.
As additional confirmation, a 2006 study found no significant difference in the mean publications of authors in two conferences [@madden06:impact], but a followup study on the same dataset found evidence for prestige bias when comparing across medians [@tung06:impact].
The choices for metrics and aggregations must therefore be carefully justified, compared, and balanced to increase the credibility of such analyses.

<!---------------------------------------------------->
### Binary classifications of fame {#subsec:binary}

Up to this point, we have treated the reputation or fame of researchers as a continuous variable.
This treatment allowed us to answer the question of whether the likelihood of an author to publish in SB proceedings increases commensurate with their reputation metrics.
But the question of reputation bias may instead be more binary, based on the accept/reject outcome of a paper: either a paper is accepted based on an author's reputation being recognized or it is not. 
This qualitative classification has the advantage of avoiding the problem of the long tails of the reputation metrics that could skew the mean reputation of a sample.

The quantitative question then becomes, what is the proportion of accepted papers in SB and DB conferences for which at least one of the authors carries enough reputation to be recognized? Do these proportions vary significantly by the conference reviewing policy?
Since we do not know a-priori what "enough reputation" means, we can observe how these proportions change based on different reputation metrics and thresholds.


```{r rep-threshold-npubs, echo = F, warning = F, fig.width = 4.5, fig.height = 3.5, fig.cap = "Proportion of papers that have at least one author with at least as many publications as the threshold (Google Scholar data)."}
tbl <- data.frame(cbind("thresh" = 0, "db" = total_db, "sb" = total_sb, "p.value" = NA), stringsAsFactors = F, check.names = F)
for (t in seq(1, 1000, by = 1)) {
  tbl <- rbind(tbl, cbind("thresh" = t, t(over_thresh("max_npubs", t))))
}

tbl %>%
  mutate(sb = sb / total_sb, db = db / total_db) %>%
  pivot_longer(cols = c("sb", "db"), names_to = c("db_status"), values_to = "ratio") %>%
  ggplot(aes(x = thresh, y = ratio, color = db_status)) +
    geom_line() +
    theme_minimal() +
    theme(legend.position = "bottom") +
    xlab("Publication count threshold") +
    ylab("Proportion of papers") +
    scale_color_manual(name = "Review Policy", labels = c("Double", "Single"), values = c("orange", "blue"))
```

```{r, eval = FALSE, include = FALSE}
tbl <- data.frame(cbind("thresh" = 0, "db" = total_db, "sb" = total_sb, "p.value" = NA), stringsAsFactors = F, check.names = F)
for (t in seq(1, 1000, by = 1)) {
  tbl <- rbind(tbl, cbind("thresh" = t, t(over_thresh("max_npubs_imp", t))))
}

tbl %>%
  mutate(sb = sb / total_sb, db = db / total_db) %>%
  pivot_longer(cols = c("sb", "db"), names_to = c("db_status"), values_to = "ratio") %>%
  ggplot(aes(x = thresh, y = ratio, color = db_status)) +
    geom_line() +
    theme_minimal() +
    theme(legend.position = "bottom") +
    xlab("Publication count threshold") +
    ylab("Proportion of papers") +
    scale_color_manual(name = "Review Policy", labels = c("Double", "Single"), values = c("#BD1550", "#8A9B0F"))
```


For example, Fig. \@ref(fig:rep-threshold-npubs) shows the proportion of papers written by famous authors in SB or DB conferences, based on number of publications.
It shows that beyond a certain level of reputation (around 100 papers), famous authors are always overrepresented in SB conferences, consistent with a possible prestige bias.
Comparing these proportions across SB and DB conferences, the $\chi{}^2$-test-derived p-values are below 0.05 for `r pct(sum(tbl$p.value < 0.05, na.rm = T), nrow(tbl) - 1, 1)`% of the threshold values.


```{r rep-threshold-h, echo = F, warning = F, fig.width = 4.5, fig.height = 3.5, fig.cap = "Proportion of papers that have at least one author with an h-index value above the threshold."}
tbl <- data.frame(cbind("thresh" = 0, "db" = total_db, "sb" = total_sb, "p.value" = NA), stringsAsFactors = F, check.names = F)
for (t in seq(1, 50)) {
  tbl <- rbind(tbl, cbind("thresh" = t, t(over_thresh("max_hindex", t))))
}

tbl %>%
  mutate(sb = sb / total_sb, db = db / total_db) %>%
  pivot_longer(cols = c("sb", "db"), names_to = c("db_status"), values_to = "ratio") %>%
  ggplot(aes(x = thresh, y = ratio, color = db_status)) +
    geom_line() +
    theme_minimal() +
    theme(legend.position = "bottom") +
    xlab("h-index threshold") +
    ylab("Proportion of papers")  +
    scale_color_manual(name = "Review Policy", labels = c("Double", "Single"), values = c("orange", "blue"))
```

```{r, eval = FALSE, include = FALSE}
tbl <- data.frame(cbind("thresh" = 0, "db" = total_db, "sb" = total_sb, "p.value" = NA), stringsAsFactors = F, check.names = F)
for (t in seq(1, 50)) {
  tbl <- rbind(tbl, cbind("thresh" = t, t(over_thresh("max_hindex_imp", t))))
}

tbl %>%
  mutate(sb = sb / total_sb, db = db / total_db) %>%
  pivot_longer(cols = c("sb", "db"), names_to = c("db_status"), values_to = "ratio") %>%
  ggplot(aes(x = thresh, y = ratio, color = db_status)) +
    geom_line() +
    theme_minimal() +
    theme(legend.position = "bottom") +
    xlab("h-index threshold") +
    ylab("Proportion of papers")  +
    scale_color_manual(name = "Review Policy", labels = c("Double", "Single"), values = c("#BD1550", "#8A9B0F"))
```

The two lines are even more distinctly divergent when using h-index for reputation, but in reverse.
Fig. \@ref(fig:rep-threshold-h) shows that papers by famous authors in DB conferences consistently outnumber those in SB conferences by a near-constant amount of some 5%--10%. 
Again, p-values below 0.05 show up for `r pct(sum(tbl$p.value < 0.05, na.rm = T), nrow(tbl) - 1, 1)`% of the threshold values.

These two examples show strong statistical evidence with remarkable insensitivity to the threshold chosen, but in opposite directions.
They demonstrate once more how critically the conclusion for or against prestige bias can depend on the chosen reputation metric.


<!---------------------------------------------------->
### Banded classification of seniority

Another related approach to categorize seniority and experience was described in the original h-index study [@hirsch05:index]. It proposed that researchers may fall into experience bands, roughly corresponding to novice/assistant professor, mid-career/associate professor, and experienced/full professor. Based on his observations in the field of physics, Hirsch suggested h-index limits of under 11, between 12 and 17, and 18+ to distinguish among these bands.

```{r seniority, echo = F, message = F, out.width='0.75\\textwidth', fig.height=3, fig.cap = "Experience bands of papers' authors by review policy. Each paper is counted exactly once, based on the highest h-index among its coauthors."} 
e_with_seniority <-
experience.all %>%
    mutate(Experience = as.factor(ifelse(max_hindex < 12, "Novice", ifelse(max_hindex <= 18, "Mid-career", "Experienced"))))

e_with_seniority %>%
  drop_na(Experience) %>%
  group_by(double_blind, Experience) %>%
  ggplot(aes(fill = Experience, x = double_blind)) +
    geom_bar(position = "fill") +
    ylab("Proportion of papers") +
    theme_minimal() +
    theme(legend.position = "bottom") +
    #scale_fill_brewer(breaks = c("Novice", "Mid-career", "Experienced"), palette = "Greens", direction = -1) +
    scale_fill_viridis_d(breaks = c("Novice", "Mid-career", "Experienced"), option = "D") +
    guides(fill = guide_legend(title = "Most senior author's experience")) +
    scale_x_discrete(name = "Review policy", labels = c("Single", "Double")) +
    coord_flip()
```

When we apply these (admittedly arbitrary) limits to our dataset (Fig. \@ref(fig:seniority)), we find a slight advantage to senior researchers in DB reviewed papers
(`r report_test(chisq.test(table(e_with_seniority$Experience, e_with_seniority$double_blind)))`).
This anti-prestige bias is consistent with the evidence we uncovered in previous sections using maximum h-index, both for quantitative and qualitative aggregations.


<!---------------------------------------------------->
### Prestige by affiliation

To complete our exploration of paper summaries, we look at one more useful aggregation method to approximate name recognition of authors, based on their affiliation.
For this analysis, we follow the methodology of Tomkins, Zhang, and Heavlin from their study examining prestige bias in the 2017 Web Search and Data Mining (WSDM) conference [@tomkins17:reviewer].
Their study tagged a paper as coming from a "top university" if any of its coauthors is affiliated with one of the top 50 universities per [www.topuniversities.com](www.topuniversities.com)  (27% of papers).
They similarly tagged a paper as coming from a top company if any author was affiliated with either Google, Microsoft, Yahoo!, or Facebook (18% of papers).

Since our data comes from the same publication year as theirs, we used exactly the same criteria for tagging for an apples-to-apples comparison.
Unfortunately, the original study and supporting information does not contain the list of universities or how it was processed.
We therefore downloaded the list of the top 50 "Engineering and Technology" universities and saved it as a list of internet domain names (see supplementary materials, under `data/top50uni.csv`).

In our dataset,
`r pct(sum(experience.all$top_university), nrow(experience.all))`%
of papers had at least one coauthor from a top university, and
`r pct(sum(experience.all$top_company), nrow(experience.all))`%
of papers had at least one coauthor from a top company.
The percentage of papers for authors from a top university in DB conferences is significantly higher than in SB conferences
(`r pct(nrow(filter(experience.all, top_university > 0, double_blind)), nrow(filter(experience.all, double_blind)))`% vs.
`r pct(nrow(filter(experience.all, top_university > 0, !double_blind)), nrow(filter(experience.all, !double_blind)))`%,
`r report_test(chisq.test(table(experience.all$top_university, experience.all$double_blind)))`)
The same can be said for top-company affiliation
(`r pct(nrow(filter(experience.all, top_company > 0, double_blind)), nrow(filter(experience.all, double_blind)))`% vs.
`r pct(nrow(filter(experience.all, top_company > 0, !double_blind)), nrow(filter(experience.all, !double_blind)))`%,
`r report_test(chisq.test(table(experience.all$top_company, experience.all$double_blind)))`).

It appears that this aggregation, like total citations or maximum h-index, but contrary to Tomkins' results, is consistent with anti-prestige bias (higher prestige in SB conferences).
Unlike Tomkins' results, however, our data does not come from a controlled experiment on a single conference, so it cannot provide counterfactual evidence.

<!-------------------------------------------------------------------------------------------->
## Confounding variables {#subsec:confounding}

The next assumptions we dismantle are that the reviewing policy of a conference is an independent, random variable and that each paper is a independent observation.
In this section, we ask the question, how would our findings on prestige bias change if we examined them in a larger context of conference factors and after accounting for conference level dependencies in the data? 

A conference choice of review policy is often connected to other factors that may also be associated with the experience of researchers submitting to it.
Specifically, DB conferences tend to have a **longer history** in years
(`r report_test(t.test(db_confs$age, sb_confs$age), show_stat = F)`), 
a **lower acceptance rate**
(`r report_test(t.test(db_confs$acceptance_rate, sb_confs$acceptance_rate), show_stat = F)`),
**higher H5-index** in GS
(`r report_test(t.test(db_confs$h5_index, sb_confs$h5_index), show_stat = F)`),
**more pages per paper**
(`r report_test(t.test(db_confs$mean_pages, sb_confs$mean_pages), show_stat = F)`),
**more paper submissions**
(`r report_test(t.test(db_confs$submissions, sb_confs$submissions), show_stat = F)`),
with **more coauthors per paper**
(`r report_test(t.test(db_confs$mean_authors_per_paper, sb_confs$mean_authors_per_paper), show_stat = F)`),
a **lower ratio of authors coming from the PC itself**
(`r report_test(t.test(db_confs$pc_author_ratio, sb_confs$pc_author_ratio), show_stat = F)`),
and more **citations to past papers**
(`r report_test(t.test(db_confs$past_citations, sb_confs$past_citations), show_stat = F)`),
especially when **normalized to citations per paper**
(`r report_test(t.test(db_confs$mean_historical_citations, sb_confs$mean_historical_citations), show_stat = F)`).

In other words, DB conferences themselves show several distinct characteristics, some of which suggest higher reputation.
The higher implied prestige of the conference could be associated with the prestige of authors as well, confounding the relationship.
Additionally, papers published in a particular conference are likely more correlated with each other than papers published across conferences.
Therefore, we should account for this correlation structure in our models directly.  



### Linear regression

To evaluate the effect of these confounding variables, we built mixed-effects linear-regression models with author prestige as the outcome variable (maximum in paper), an indicator variable for reviewing policy (where 1 = SB, 0 = DB) as the predictor variable, and conference as a random effect (Fig. \@ref(fig:linear-regression)).
The first row is very similar to the first row in Fig. \@ref(fig:mean-diffs) and shows the coefficient of the reviewing policy alone on author prestige, which often exhibits a statistically strong association.
Each row afterwards includes one of the variables listed above as an additional fixed effect that could stand in for conference prestige.

```{r linear-regression, echo = F, warning = F, fig.asp = 0.4, fig.cap="Linear regression of various author prestige metrics as predicted by review policy and other conference variables. Values shown are the coefficient of the reviewing policy variable in the linear model and their associated p-values. Positive coefficients (purple) indicate higher reputation in SB conferences. Negative coefficients (orange) indicate higher reputation in DB conferences. Darker colors reflect larger-magnitude values, normalized by columns. Stars indicate p values below 0.05, 0.01, and 0.001, respectively."}
tbl <- compute_glm()  # Placeholder / filler just to have the columns

predictors <- c("", "age", "acceptance_rate", "h5_index", "submissions", "past_citations", "mean_historical_citations")

for (metric in c("s2npubs", "npubs", "as_author", "citedby", "paper_cites", "hindex", "i10index")) {
  for (predictor in predictors) {
    tbl <- rbind(tbl, compute_glm(experience.all, predictor, metric))
  }
}

reg.all <- as_tibble(tbl[-1,])
reg.all$metric <- factor(reg.all$metric, ordered = T,
                         levels = c("npubs", "s2npubs", "as_author", "citedby", "paper_cites", "hindex", "i10index"),
                         labels = c("Publications (GS)", "Publications (S2)", "Papers in set", "Total citations", "Citations per paper", "h-index", "i10-index"))
reg.all$predictor <- factor(reg.all$predictor, ordered = T,
                            levels = predictors,
                            labels = c("None", "Age in years", "Acceptance rate", "h5 index", "Paper submissions", "Total past citations", "Mean citations per paper"))
colnames(reg.all) <- c("metric", "predictor", "linear_c", "linear_p", "logistic_c", "logistic_p", "linear_m_c", "linear_m_p", "logistic_m_c", "logistic_m_p")
reg.all$linear_c <- as.numeric(reg.all$linear_c)
reg.all$linear_p <- as.numeric(reg.all$linear_p)
reg.all$logistic_c <- as.numeric(reg.all$logistic_c)
reg.all$logistic_p <- as.numeric(reg.all$logistic_p)
reg.all$linear_m_c <- as.numeric(reg.all$linear_m_c)
reg.all$linear_m_p <- as.numeric(reg.all$linear_m_p)
reg.all$logistic_m_c <- as.numeric(reg.all$logistic_m_c)
reg.all$logistic_m_p <- as.numeric(reg.all$logistic_m_p)

reg.all %>%
  group_by(metric) %>%
  mutate(normalized = linear_c / metric_denominator(reg.all, metric, "linear_c")) %>%
  ggplot(aes(x = metric, y = predictor)) +
    geom_tile(aes(fill = normalized)) +
    scale_y_discrete(limits = rev) +
    scale_fill_gradient2(low = "orange", high = "blue") +
    geom_text(aes(label = paste0(round(linear_c, 2), format_p_value(linear_p, p_option = "stars")))) +
    ylab("Additional conference predictor variable") +
    xlab("Reputation metric (max per paper)") +
    labs(fill = "DB coefficient") +
    theme_minimal() +
    guides(fill = "none") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1), axis.text.y = element_text(angle = 45, hjust = 1))
```


```{r linear-mixed-regression, echo = F, warning = F, fig.asp = 0.4, fig.cap="Mixed-effects linear regression models of various author prestige metrics as predicted by review policy and other conference variables with conference as a random effect. Values shown are the coefficient of the reviewing policy variable in the linear model and their associated p-values. Positive coefficients (purple) indicate higher reputation in SB conferences. Negative coefficients (orange) indicate higher reputation in DB conferences. Darker colors reflect larger-magnitude values, normalized by columns. Stars indicate p values below 0.05, 0.01, and 0.001, respectively."}
reg.all %>%
  group_by(metric) %>%
  mutate(normalized = linear_m_c / metric_denominator(reg.all, metric, "linear_m_c")) %>%
  ggplot(aes(x = metric, y = predictor)) +
    geom_tile(aes(fill = normalized)) +
    scale_y_discrete(limits = rev) +
    scale_fill_gradient2(low = "orange", high = "blue") +
    geom_text(aes(label = paste0(round(linear_m_c, 2), format_p_value(linear_m_p, p_option = "stars")))) +
    ylab("Additional conference predictor variable") +
    xlab("Reputation metric (max per paper)") +
    labs(fill = "DB coefficient") +
    theme_minimal() +
    guides(fill = "none") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1), axis.text.y = element_text(angle = 45, hjust = 1))
```

(((TODO: remove fig 8, but first explain why the "None" rows are different between Figs. 8--9. Should they be? I like 8's better)))

Looking first at citation-based author metrics (total citations, citations per paper, and h-index), we can indeed confirm that adding a conference prestige confounder and accounting for the intraclass correlation within conferences generally lowers the values of the review-policy coefficient and its significance, sometimes even changing signs.
Some of these citation author-conference relationships are almost tautological in nature.
For example, we can expect to find people with many total past citations publishing more in conferences with a higher total past citations, because it is likely that the same highly-cited papers that contributed to a person's count also contributed to the conference's count, assuming some consistency in publishing venues over time.
Such strong associations obviate other explanatory variables such as review status, which is why the coefficient and significance of DB reviewing drops to near-zero when we correct for conference prestige.

The picture becomes less clear when we look at the first three columns, representing publication-based author prestige metrics.
They suggest that adding an explanatory variable does little to change the direction of prestige bias evidence:
total past papers are still indicating higher prestige in SB conferences and total current papers are still indicating higher prestige in DB conferences.

We believe that the explanation for this apparent contradiction lies in the competitive nature of citation-based metrics.
For publication-based metrics, it is harder to draw the same conclusions.
There is little reason to assume that a person with many past or present publications also has a history of publishing in more competitive conferences.
If anything, we would expect some prolific authors to publish less in conferences with a low acceptance rate, simply by making a probabilistic argument.
In that case, we would also expect to see even higher coefficients for DB reviewing when correcting for low-prestige conferences, as some of the rows indeed appear to show.

As for the last column (i10-index), it appears to behave more like a publication-based metric than a citation-based metric.
Although this metric incorporates both dimensions, we believe it is influenced more by publication count than by their "impact", because the "impact" threshold to inclusion in this index, namely 10 citations, is not particularly restrictive.


In summary, even the effect of correction for confounding variables appears to be sensitive to the metrics chosen.
the inclusion alone of confounding variables could weaken any observed association between DB reviewing and author prestige for "impact-based" metrics and strengthen it for "productivity-based" metrics.
Every one of the conference variables we tested could potentially serve as a better predictor of author reputation than DB reviewing, because of their strong correlations. 
<!--This extra dimension could complicate drawing conclusions on review bias from other observational studies as well, especially if they treat author prestige as a continuous variable.-->

### Logistic regression

Next, we revisit the treatment of an author's prestige as a categorical variable, rather than a continuous variable.
If we treat fame as a Boolean factor instead, i.e., an author's name is either recognized or not, then we can repeat our analysis with mixed effects logistic regression.
We can do so by picking a "fame threshold", above which we consider an author's name to be recognized.
As discussed in Sec. \@ref(subsec:binary), senior authors show the same preference for DB or SB conferences regardless of the threshold selected to define seniority, at least for h-index and number of publications.
We will therefore use an arbitrary binary threshold of "top 10%" for each reputation metric to classify those authors who are most likely to be recognized.


```{r logistic-regression, echo = F, warning = F, fig.height = 3, fig.cap="Logistic regression of the top decile of various author prestige metrics as predicted by review policy and other conference variables. Values shown are the coefficient of DB reviewing in the logistic model and their associated p-values. Positive coefficients (purple) indicate higher reputation in SB conferences. Negative coefficients (orange) indicate higher reputation in DB conferences. Darker colors reflect larger-magnitude values. Stars indicate p values below 0.05, 0.01, and 0.001, respectively."}
reg.all %>%
  group_by(metric) %>%
  mutate(normalized = logistic_c / metric_denominator(reg.all, metric, "logistic_c")) %>%
  ggplot(aes(x = metric, y = predictor)) +
    geom_tile(aes(fill = logistic_c)) +
    scale_y_discrete(limits = rev) +
    scale_fill_gradient2(low = "orange", high = "blue") +
    geom_text(aes(label = paste0(round(logistic_c, 2), format_p_value(logistic_p, p_option = "stars")))) +
    ylab("Additional conference predictor variable") +
    xlab("Reputation metric in top 10% of authors") +
    labs(fill = "DB coefficient") +
    theme_minimal() +
    guides(fill = "none") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1), axis.text.y = element_text(angle = 45, hjust = 1))
```

```{r logistic-mixed-regression, echo = F, warning = F, fig.asp = 0.4, fig.cap="Mixed effects logistic regression of the top decile of various author prestige metrics as predicted by review policy and other conference variables with conference as a random effect. Values shown are the coefficient of DB reviewing in the logistic model and their associated p-values. Positive coefficients (purple) indicate higher reputation in SB conferences. Negative coefficients (orange) indicate higher reputation in DB conferences. Darker colors reflect larger-magnitude values. Stars indicate p values below 0.05, 0.01, and 0.001, respectively."}
reg.all %>%
  group_by(metric) %>%
  mutate(normalized = logistic_m_c / metric_denominator(reg.all, metric, "logistic_m_c")) %>%
  ggplot(aes(x = metric, y = predictor)) +
    geom_tile(aes(fill = logistic_c)) +
    scale_y_discrete(limits = rev) +
    scale_fill_gradient2(low = "orange", high = "blue") +
    geom_text(aes(label = paste0(round(logistic_m_c, 2), format_p_value(logistic_m_p, p_option = "stars")))) +
    ylab("Additional conference predictor variable") +
    xlab("Reputation metric in top 10% of authors") +
    labs(fill = "DB coefficient") +
    theme_minimal() +
    guides(fill = "none") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1), axis.text.y = element_text(angle = 45, hjust = 1))
```



Figure \@ref(fig:logistic-mixed-regression) depicts the effects of this treatment of author prestige on the association with reviewing policy, corrected for conference prestige.
Directionally, the results are similar to those of the linear regression.
But restricting name recognition to a specific quantile offers two desirable properties for the coefficient comparison: it normalizes all the coefficients to the same range from 0 to 1, and eliminates the outsize effect of high-prestige author outliers on the linear regression.
Consequently, observing general trends becomes easier and less noisy.

The most noticeable general trend with this classification of prestige is that most coefficients are now statistically nonsignificant, meaning that it is now harder to claim prestige bias one way or the other, regardless of confounding variables.
The two columns that show significant results are papers-in-set and mean-past-citations per paper.
Interestingly, they both show the same direction (higher reputation in DB conferences) and the significance of this preference does not abate with confounding conference factors.

(((WHY these two? why do they agree? why does confounding not change things?)))

<!-------------------------------------------------------------------------------------------->
## Handling Missing and Suspect Observations {#subsec:cleaning}

To complete our experimental evaluation, we consider the effects of *data cleaning*.
Preparing this dataset for analysis required a thorough exploration of each variable and observations for missing or suspect values.
This process does not follow a formal set of rules but instead usually requires the researchers to make several subjective choices, each of which potentially impacts the final results.
In this subsection, we explore how the handling of missing and suspect observations potentially impacts conclusions of prestige bias.  

Figure \@ref(fig:metric-distribs) displays the distributions of the various author-level prestige metrics.
All metrics are right skewed and there are reasons to suspect the quality of the values in both the upper and lower tails of these distributions.

On the upper-end tails, reputation metrics are likely inflated because of name disambiguation issues, i.e., attributing too many publications to certain names who may be shared across different researchers.
Previously, we explored a few analysis techniques that dampen the impact of outliers, such as the Wilcoxon Signed Rank Test and a binary classification of fame, to see if they changed the story.
For a given metric and aggregation technique, the conclusions regarding prestige bias were similar between these outlier-resistant measures and the other techniques, suggesting a limited effect for these inflated data.
We therefore chose not to winsorize these inflated metrics, a common data cleaning technique for highly right-skewed data.


```{r, echo = FALSE}
# Find prop missing by double_blind (author counted twice if published in both double and single)
ppl_blind <- ppl %>%
  left_join(all_confs, by = c("conf" = "key")) %>%
  distinct(name, gs_email, double_blind, .keep_all = TRUE)

ppl_distinct <- ppl %>%
  distinct(name, gs_email, .keep_all = TRUE)

missing_props <- count(ppl_blind, double_blind, is.na(npubs)) %>%
  group_by(double_blind) %>%
  mutate(prop = n/sum(n)) %>%
  filter(`is.na(npubs)` == TRUE) 
```


On the lower tail, note that `r round(mean(is.na(ppl_distinct$npubs)) * 100, digits = 2)`% of the values for the GS measures are missing for authors for who we could not uniquely identify a GS profile (`r round(mean(missing_props %>% filter(double_blind) %>% pull(var = prop)) * 100, digits = 2)`% of DB authors and `r round(mean(missing_props %>% filter(!double_blind) %>% pull(var = prop))*100, digits = 2)`% of SB authors).
Since the median number of publications from S2 is only `r filter(ppl, is.na(npubs)) %>% summarize(median(s2npubs)) %>% pull()` for those without GS measures, compared to `r filter(ppl, !is.na(npubs)) %>% summarize(median(s2npubs)) %>% pull()` for those with GS measures, these authors with missing values are likely less experienced and therefore would contribute to the lower tail of the author-level prestige metric distributions.
The variation in S2 publications between those with and without GS profiles provides evidence that the missing values are not missing at random and that we should determine how sensitive our results are to how the missingness is treated.  

((( How can the separate values for SB and DB be lower than overall values?!?)))

For handling these missing values, we consider three strategies.
The simplest approach, which we have used in all previous experiments, involves removing the missing values when computing aggregate measures.
This strategy likely inflates some paper aggregation metrics such as the mean, but has less influence on the maximum metric.
On the other end of the spectrum, we also consider imputing the missing metrics with the minimum value for that metric.
This assumes that the missing values all represent new researchers and will pull down the mean paper aggregation metrics,
but again will hardly influence the maximum metric.
Lastly, we consider imputing the missing values based on a linear regression model built on the total publications from S2, which is moderately correlated with the GS metrics (Fig. \@ref(fig:metric-corrs)).
For the linear imputation, we also imputed the GS metrics if the ratio of the total publications from GS compared to S2 was greater than `r THRESH_RATIO`, which indicated a potential inflation of the values of GS metrics for those authors (Fig. \@ref(fig:imputation)).
Similarly, to minimize imputing inflated GS metrics, we winsorized at `r S2_THRESH` the very heavy tail of the total publications from S2 before computing the imputed values.


```{r imp-results, echo = F, warning = F, fig.height = 4, fig.cap="Test statistics of the Student's t-test and the difference in medians for the Wilcoxon test for publications (GS) and h-index, aggregated across papers for three strategies of handling missing values. Positive values (purple) indicate higher reputation in SB conferences. Darker colors reflect larger-magnitude values, normalized by columns and test method. Negative values (orange) indicate higher reputation in DB conferences. Stars indicate p values below 0.05, 0.01, and 0.001, respectively."}
tbl <- rbind(compute_db(experience.all, "max", "npubs"),
             compute_db(experience.all, "total", "npubs"),
             compute_db(experience.all, "mean", "npubs"),
             compute_db(experience.all, "median", "npubs") ) %>%
  as_tibble() %>%
  mutate(metric_type = "npubs")

for (metric in c("npubs_flat", "npubs_imp", "hindex", "hindex_flat",
                 "hindex_imp")) {
  for (aggr in c("max", "total", "mean", "median")) {
    tbl <- compute_db(experience.all, aggr, metric) %>%
      as_tibble_row() %>%
      mutate(metric_type = str_sub(metric, end = 5)) %>%
      bind_rows(tbl)
  }
}

# Append lead-author stats
for (metric in c("npubs", "npubs_flat", "npubs_imp", "hindex", "hindex_flat",
                 "hindex_imp")) {
      tbl <- compute_db(experience.lead, "first", metric) %>%
      as_tibble_row() %>%
      mutate(metric_type = str_sub(metric, end = 5)) %>%
      bind_rows(tbl)
}

tbl <- tbl %>%
  mutate(aggregation = if_else(aggregation == "first", "lead", aggregation))

# Append last-author stats
for (metric in c("npubs", "npubs_flat", "npubs_imp", "hindex", "hindex_flat",
                 "hindex_imp")) {
        tbl <- compute_db(experience.last, "first", metric) %>%
      as_tibble_row() %>%
      mutate(metric_type = str_sub(metric, end = 5)) %>%
      bind_rows(tbl)
}

tbl <- tbl %>%
  mutate(aggregation = if_else(aggregation == "first", "last", aggregation))


rep.all <- as_tibble(tbl)
rep.all$metric <- factor(rep.all$metric, ordered = T,
                         levels = c("npubs", "npubs_flat", "npubs_imp",
                                    "hindex", "hindex_flat", "hindex_imp"),
                         labels = c("npubs", "npubs_flat", "npubs_imp",
                                    "hindex", "hindex_flat", "hindex_imp"))
rep.all$imp_type <- factor(rep.all$metric, ordered = T,
                         levels = c("npubs",  "npubs_imp", "npubs_flat",
                                    "hindex", "hindex_imp", "hindex_flat"),
                         labels = c("No Imputation", "Linear Imputation", 
                                      "Flat Imputation", "No Imputation", 
                                      "Linear Imputation","Flat Imputation"))
rep.all$aggregation <- factor(rep.all$aggregation, ordered = T,
                              levels = c("lead", "last", "median", "mean", "total", "max"),
                              labels = c("Lead author", "Last author", "Median", "Mean", "Total", "Maximum"))
rep.all$t_stat.t <- as.numeric(rep.all$t_stat.t)
rep.all$t_pval <- as.numeric(rep.all$t_pval)
rep.all$w_stat.W <- as.numeric(rep.all$w_stat.W)
rep.all$w_pval <- as.numeric(rep.all$w_pval)


# Diff in Medians for npubs imputations
rep.all$diff <- as.numeric(rep.all$median_sb) - as.numeric(rep.all$median_db)
rep.all <- rep.all %>%
  group_by(metric_type) %>%
  mutate(normalized = diff/max(abs(diff)), max_diff = max(abs(diff)))

# Create a long format dataset 
dat <- rep.all %>%
  select(imp_type, aggregation, normalized, diff, w_pval, metric_type) %>%
  rename(stat_norm = normalized, stat = diff, pval = w_pval) %>%
  mutate(test = "Wilcoxon Signed Rank Test") %>%
  bind_rows(select(rep.all, 
                   imp_type, aggregation, t_stat.t, t_pval, metric_type) %>%
              rename(stat = t_stat.t, pval = t_pval) %>%
              mutate(test = "Two-Sample t-Test")) %>%
  mutate(stat_norm = if_else(is.na(stat_norm), stat, stat_norm)) %>%
  group_by(test, metric_type, imp_type) %>%
  mutate(stat_norm2 = stat_norm/sd(stat_norm)) %>% # Standardizing
  mutate(metric_type = case_when(metric_type == "hinde" ~ "h-index",
                                 metric_type == "npubs" ~ "Publications (GS)"))


dat %>%
  ggplot(aes(x = imp_type, y = aggregation)) +
  geom_tile(aes(fill = stat_norm2)) +
  scale_fill_gradient2(low = "orange", high = "blue") +
  geom_text(aes(label = paste0(round(stat, 2), 
                               format_p_value(pval, p_option = "stars")))) +
  ylab("Paper aggregation method") +
  xlab("Imputation Method") +
  theme_void() +
  guides(fill = "none") +
  theme(axis.text.x = element_text(angle = 45, hjust = 0.5)) +
  facet_grid(test~metric_type, scales = "free") 
```


Across these three strategies, the sign of the relationship between review policy and the prestige metric stays the same for most aggregations, though the significance of the results does vary (Fig. \@ref(fig:imp-results)).
The linear imputation methods acts like a compromise by typically providing a test statistic and p-value that is between those provided by the other two methods.
Generally, it appears that imputing the missing data has little impact on prestige-bias analysis because most of the missing data are for the less-experienced authors, who do not impact many of our aggregation metrics.
However, researchers must remain cognizant of these choices and transparent about how they handle missing values.


```{r imputation, echo = FALSE, message = FALSE, warning= FALSE, fig.asp = 0.5, fig.cap = "Scatterplot of the relationship between the two author-level, total publication metrics.  The estimated linear regression line used for imputation is included in black and pointed where the GS metric is 3 times the S2 metric are highlighted in yellow."}
ppl %>%
  select(npubs, s2npubs, imp) %>%
  filter(s2npubs < S2_THRESH) %>%
  ggplot(aes(y = npubs, x = s2npubs)) +
    geom_point(alpha = 0.2, mapping = aes(color = imp)) +
    geom_smooth(method = lm, se = FALSE, color = "brown") +
    guides(colour = FALSE) +
    labs(x = "Publications (S2)", y = "Publications (GS)") +
    theme_minimal() +
    scale_color_manual(values = c("black", "goldenrod"))
```



<!-------------------------------------------------------------------------------------------->
<!-------------------------------------------------------------------------------------------->

# Discussion {#sec:discussion}

The previous section explored the observed relationships between author prestige and review policy across numerous dimensions.
We started with a simple model for prestige bias, as many studies do, and then one by one analyzed the effect of the assumptions underlying the model.
Our analysis compared dozens of parameter combinations, which poses the risks of data mining and "p-hacking".
We therefore suggest not to ascribe too much meaning to the individual p-values reported here, but rather to look at the bigger question: how is it that different perspectives on the same data can support diametrically opposed conclusions?

For example, we have seen that different reputation metrics can yield contradictory results.
Even a single popular metric, number of publications, can yield very different distributions based on the database it comes from, because each database comes up with its own choices to inherently hard questions like "what constitutes a publication" and "how to disambiguate names".

The most striking metric contrast displays when comparing number of publications to h-index, even from the same data source (GS).
Under most treatments, famous authors by number of publications appear to strongly associated with SB conferences in our dataset, where the opposite is true for famous authors by hindex.
Since these two metrics are extremely common, it is worth investigating the source of the discrepancy, as a case study for the process of untangling these discrepancies.

```{r npubs-vs-hindex, echo = FALSE, message = FALSE, warning = FALSE, fig.asp = 0.5, fig.cap = "Scatterplot of the relationship between number of publications (GS) and and h-index for all authors with data (log scale). Color denotes the ratio of an author's papers from single-blind conferences in this dataset. Brown line denotes linear regression of all data points."}
db_ratios <- roles %>%
  filter(role == "author") %>%
  left_join(all_confs, by = c("conf" = "conference")) %>%
  group_by(name, gs_email) %>%
  summarize(db2sb_ratio = sum(double_blind) / n(), n = n())

ppl %>%
  left_join(db_ratios) %>%
  ggplot(aes(y = npubs, x = hindex)) +
    geom_point(alpha = 0.2, mapping = aes(color = db2sb_ratio)) +
    geom_smooth(se = FALSE, method = "lm", color = "brown") +
    scale_color_gradient2(low = "orange", high = "blue", midpoint = 0.5, name = "Publication preference", breaks = c(0, 0.5, 1), labels = c("100% SB", "neutral", "100% DB")) +
    labs(x = "h-index", y = "Publications (GS)") +
    scale_x_log10() +
    scale_y_log10() +
    theme_minimal() +
    theme(legend.position = "bottom")
```


Figure \@ref(fig:npubs-vs-hindex) compares these metrics across all authors with GS data, with the color of each point representing the percentage of papers in our set that each person published in SB or DB conferences.
Despite the strong correlation between the two metrics, it is clear that they do not always agree. Some researchers exhibit relatively high h-index and some exhibit relatively high publications, compared to the average population, as represented by the regression line.

Observe that the points below the regression line (relatively high h-index) tend to publish more often in DB conference, whereas the opposite is true for authors with relatively many publications.
This dichotomy suggests two broad types of researchers, which we dub as "impact" and "prolificity", respectively.
Double-blind conference probably attract more of the "impact"-type researchers because these conferences are also correlated with higher reputation metrics, including citations.
On the other hand, "prolificity"-type researchers probably publish more in the SB, less-competitive conferences, which would statistically lead to more accepted papers but probably also to lower relative h-index.

```{r paper-cites, echo = FALSE, message = FALSE, warning = FALSE}
how_long <- 36

cites <-
experience.all %>%
  left_join(filter(citations, near(months, how_long, 3))) %>%  # Pick anything within 3 months of how_long
  group_by(key) %>%     # And narrow down to the closest date
  filter(abs(months - how_long) == min(abs(months - how_long)))

db_cites = filter(cites, double_blind)$citations
sb_cites = filter(cites, !double_blind)$citations
```

We cannot confirm this hypothesis conclusively without information on all submitted papers.
However, we can find some confirmation in a different statistic: how well-cited are the papers in our dataset, now that a few years have passed since publication.
If we look at GS statistics for each paper `r how_long` exactly months after it were published, we find that DB papers average
`r round(mean(db_cites), 1)`
citations per paper, significantly higher than SB's
`r round(mean(sb_cites), 1)`
(`r report_test(t.test(db_cites, sb_cites))`).
The difference in medians,
`r median(db_cites) - median(sb_cites)`,
is also quite significant
(`r report_test(wilcox.test(db_cites, sb_cites))`).
These statistics provide additional evidence that DB conferences in our dataset are the more prestigious ones, and support the notion that type-"impact" researchers would choose to publish in them.
It also lends additional support to the idea that prestige-bias evaluation should account for venue prestige when comparing across venues or across years, for example, using a mixed-effects model.
When we did corrected for conference prestige using using the proxy of mean citations per paper, most of the previously found differences in bias lost statistical significance (\@ref(fig:linear-mixed-regression)).

(((TODO: review the following remaining question for possible inclusion or mention)))
<!--
DB reviewing is also associated with fewer authors that are PC members
(`r report_test(t.test(db_confs$pc_author_ratio, sb_confs$pc_author_ratio))`),
although it has no effect on the number of papers with at least one PC author
(`r report_test(t.test(db_confs$pc_paper_ratio, sb_confs$pc_paper_ratio))`),
--Could be less PC name recognition on review, but some conferences limit how many papers a PC may submit???
-->

<!--
 [@nicholas17:early] Early career researchers appear to have a positive experience overall with peer review. They actually aim for high-impact journals and are influential in choosing where to publish.
-->

<!--
Refer to survey paper [@frachtenberg20:survey]:

Many author factors do not appear to significantly interact with double-blindness, such as: the gender, position, and research experience (based on h-index).
In terms of review scores, there do not appear to be large differences in the mean and median grades, but the distribution of grades in double-blind reviews appears wider for some categories, especially technical merit.

We doubt that double-blinding adequately explains all these phenomena [@godlee98:peer; @mcnutt90:blinding]. Instead, we suggest looking at a third factor: the reputation or prestige of a conference. Prestige may be too abstract to measure accurately, but we did collect two proxy conference metrics: the acceptance rate and H5-index (as measured by GS).
-->

 
<!-------------------------------------------------------------------------------------------->
<!-------------------------------------------------------------------------------------------->

# Related work {#sec:related}

<!-------------------------------------------------------------------------------------------->
## Prestige bias in peer review

The integrity of the peer-review process is of utmost importance to science and scientists, and is thus an active research area all of its own.
As an inherently human process, influenced by the opinions, perspective, and understanding of human reviewers, peer review is potentially subject to various cognitive biases.
Numerous studies attempt to identify, measure, and offer interventions for such biases.
However, the gold standard for such experiments, randomized controlled trials, is exceedingly difficult to carry out and generalize [@beverly13:findings; @ernst94:reviewer; @mahoney77:publication; @mcnutt90:blinding].
Aside from the usual ethical challenges of experimenting on humans, the properties of the scientific publication make repeatable and controllable experiments particularly difficult.
Papers are typically disallowed from concurrent submission, so A/B experimentation with the same papers is both complex and only partially controlled.
Moreover, actual scientific publications have potentially enormous impact on everyone involved, so the review process cannot be arbitrarily tweaked for the purposes of experimentation.

The upshot of such constraints is that there are relatively few studies on peer-review bias using representative controlled experiments, they are often limited in scope to one journal or conference, and depending on their data and methods, can reach incompatible conclusions [@parno17:SPsurvey; @shah18:design; @tomkins17:reviewer].
A literature survey from 15 years ago on the question of prestige bias found mixed results [@snodgrass06:single], as have we in the literature since then.

In one famous study, for example, Fisher et al performed a randomized trial on 57 consecutive submissions to the Journal of Development and Behavioral Pediatrics.
Each paper received 2 SB and 2 DB reviews, and acceptance decisions were compared to the number of publications of each paper's lead author and most-senior author.
The paper concludes that SB reviewing favors authors with higher publication counts.

A similar but more recent example experimented on the submissions to the WSDM'17 conference on data mining [@tomkins17:reviewer].
The experiment split the PC into two halves, one using SB reviewing and the other DB, with two papers from each PC reviewing each paper.
The study looked at the following covariates: author gender; sector of majority coauthors per paper; most common country of majority coauthors; country homophily with reviewers (we don't have that info); famous author (based on a criterion of at least 100 past publications, 3 of which in WSDM); and affiliation prestige (based on top-50 university list or top-4 company list).
Their findings suggest a strong bias in favor of authors with individual or affiliation prestige in SB reviews.
The paper also acknowledges the difficulty and rarity of such controlled experiments and points out to a single other study [@blank91:effects].
As Tomkins et al summarize: 

> Perhaps the best-known experimental study of SB vs. DB reviewing behavior, and to our knowledge the only controlled  experiment  in  this  area  other  than  our  own,  is  the study by Rebecca Blank (15). Over several years, 1,498 papers were randomly assigned to SB vs. DB reviewing condition. While Blank performs detailed analyses of many facets of the data, we may summarize part of the high-level findings  as  follows.  First,  authors  at  top  or  bottom  institutions  do not see significant differences in acceptance decisions based on reviewing model, but authors at midtier institutions perform better in a SB setting, as do foreign authors and those out-side academia. Second, there is a mild indication, not statistically significant, that women do slightly better in DB review. [@tomkins17:reviewer]
 
A different experimental approach, using a fabricated article listing two famous researchers as authors, found that "Reviewers were more likely to recommend acceptance when the prestigious authors names and institutions were visible... than when they were redacted...  and also gave higher ratings for the methods and other categories" [@okike16:single].

Yet another approach, with controversial ethical considerations, was reported in an study that retitled and resubmitted 12 famous papers in psychology with fictitious authors and institutes [@peters82:peer].
Three of those were detected as resubmissions, and eight of the remaining nine were rejected in SB peer review.

Closer to our field of study, Madden and DeWitt [@madden06:impact] looked retrospectively at VLDB and SIGMOD conference papers (SIGMOD is also in our dataset).
Similar to our approach, this observational study did not look at rejected papers or experiment with the review process itself.
While our study compares the publication ratios of famous authors across SB/DB conferences in the same field and point in time, this study compares them across time for the same two conferences, before and after they switched from SB to DB reviewing.
For the purpose of their study, "famous authors" were defined as those 25 "prolific" individuals with at least 20 past publications in these two conferences, including recent publications.
The comparison found that the mean publication rate of famous authors was substantially the same for SB years and DB years.

Interestingly, a followup analysis on the same dataset came to a contrary conclusion by simply comparing median publication rates instead of means [@tung06:impact].
The justification for using medians was that it is more robust to outliers, which for this dataset was particularly relevant because only one of the measured values fell above the mean.
This study not only demonstrates the sensitivity of similar conclusions to the metrics used, as we show as well, but also cautions that "here  are  probably a lot of other factors that must be taken into consideration  before  the  database  community  makes  a  final  choice  on  whether  to  continue  with  double  blind  review."


<!-------------------------------------------------------------------------------------------->
## Prestige metrics

Since we found no consensus in the literature on the existence of prestige bias in SB reviewing, or even on the methods to measure it, could we at least found a common answer to the smaller question, "how should we quantify research prestige?"

Unfortunately, we found no such answer either.
The field of bibliometrics is rich with studies comparing different and contradictory metrics to evaluate the productivity, prestige, and impact of scientific work and researchers, and we can only review a fraction of this discussion in the scope of this paper.

The highly influential h-index, for example, was proposed to combine productivity with echo, and to address specific shortcomings of the publications and citations count metrics, such as sensitivity to a single highly-cited paper ("one-hit wonders") and prolific authors in low-quality venues [@bornmann07:hindex; @hirsch05:index; @masic16:scientometric].

But some researchers argue that a few influential papers should count more than a bevy of poorly cited papers [@egghe06:theory; @masic16:scientometric] and that h-index is overly sensitive to the length of a researcher's career [@vinkler07:eminence].

Furthermore, the h-index has additional shortcomings of its own.
It can be inconsistent [@waltman12:inconsistency], or be manipulated with self-citations [@smith06:peer].
It also does not account properly for the magnitude of individual contributions in team papers [@koltun21:hindex; @masic16:scientometric].
This criticism applies to the total publications metric as well, and most other widely used reputation metrics.
Finally, even when deciding to use h-index, several studies found different counts in different databases (as have we).
They caution that more than one source should be used to compute h-index accurately, and that comparisons between researchers should be limited to the same data source [@barilan08:hindex; @degroote12:coverage].

Other citation-based reputation metrics have been proposed to overcome some of these limitations, such as g-index [@egghe06:theory], p-index [@senanayake14:pindex], I3 [@bornmann11:further], AR-index [@jin07:rindex], and even simply total citations.
All of these metrics have been found to correlate with each other (as we have also observed) and are generally sensitive to changes in number of publications as well [@cronin06:hindex; @ding20:exploring].
In fact, Bertoli-Barsotti and Lando built a model to predict one reputation metric based on the others, which shows that the relationships between metrics can be complex and nonlinear [@bertoli17:theoretical].
These relationships and model mean that most metrics are codependent, so adding reputation metrics to evaluate bias may not add as much signal as desired.

Nevertheless, we believe that it would be wrong to focus on a single metric without performing a sensitivity analysis with additional metrics that may provide counterevidence.
Carpenter et al also concluded, based on a comparison of traditional and emerging publication metrics, that no single metric is descriptive enough [@carpenter14:metrics].
Instead, they suggest that a complete picture of author reputation includes multiple metrics, including traditional productivity- and impact-level metrics, as well as document-level metrics such as views, downloads, tweets, and translations.
Standardizing and incorporating such \emph{altmetrics} as part of an author's reputation or name recognition could potentially improve the analyses of prestige bias in peer review.

<!-------------------------------------------------------------------------------------------->
<!-------------------------------------------------------------------------------------------->

# Conclusions {#sec:conclusion}

This study focuses on the topic of prestige bias in peer review.
It analyzes an expansive dataset from one large subfield of computer science, and examines multiple pieces of statistical evidence on the association between "famous authors" and higher publication rates in single- or double-blind conferences.
Rather than focusing on identifying conclusive evidence one way or the other, which our dataset is too limited to offer, we focus instead on the methodology of this search, and the many choices involved in such research.

Throughout our empirical results, we explore the decisions and assumptions that are involved in prestige-bias analysis, both for experimental and observational studies.
These choices can all make a substantive difference in the direction and significance of the study's conclusions, and include:

* choice of reputation metric;
* reputation aggregation methods across coauthors;
* continuous or categorical treatment of reputation;
* confounding variables and mixed-effect models;
* handling of missing and suspect observations.

Perhaps the most critical of these dimensions is the choice of reputation metric.
This choice is a central deciding factor in whether our dataset yields bias consistent with review bias.
The two most common metrics, number of publications and h-index, produced statistically significant contradictory results on the same data.
Our analysis reveals that the reasons for this apparent contradiction relate to the "type" of publication activities characteristic of different researchers.
Some researchers clearly appear to publish a smaller number of highly competitive papers, some prefer to publish prolifically in less competitive venues, and others lie in the spectrum between these two groups.
These types of researchers exhibit different metric values, and two established researchers could represent extreme opposites across these two common metric.

This phenomenon complicates any analysis of prestige bias that uses a single metric for prestige, as most do, because it may misclassify some authors as "not famous" if they exhibit higher metric value on the metric not chosen for the study.
Cross-sectional observational studies are further complicated by the interaction of researcher type and confounding conference factors.
The review policy of a conference is not an entirely independent random variable, and other conference factors likely play a larger role in the submission (and acceptance) decisions for a researcher.
In our dataset, double-blind conferences tend to also be more competitive and prestigious.
This property, which leads to higher citation-based metrics and lower publications-based metrics, likely attracts more researchers of the "impact" type.

Our main recommendation for future prestige bias research is therefore to not focus on a single reputation metric.
More generally, we recommend that researchers design a thorough sensitivity analysis as part of their prestige-bias study, especially for observational data.
The evidence for or against prestige bias appears to be strongly influenced by multiple factors (listed in this paper), and researchers should consider these carefully before finalizing their conclusions.
Experimental studies allow for more control and elimination of some confounding variables, like the publication venue.
But even experimental studies still require careful evaluation of the question of "who is famous?" with important choices for metrics and aggregations.

We must also acknowledge that "reputation" and "name recognition" are not purely measurable values that lend themselves easily to quantitative analysis.
Perhaps a better approach for experimental studies might be to ask reviewers to rank the "reputation" of the authors they reviewed, but only after all papers have been reviewed, to reduce priming effects.
We hope further that this paper provides a guideline and a roadmap to studies on bias on how to carefully address methodological issues that could limit the generalization or credibility of their results.


# References {-}

