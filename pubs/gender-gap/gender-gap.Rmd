---
title: Underrepresentation of women in computer systems research
author:
  - name: Eitan Frachtenberg
    email: eitan@reed.edu
    affiliation: Reed-cs
    corresponding: eitan@reed.edu (EF)
  - name: Rhody D. Kaner
    affiliation: Reed-cs
address:
  - code: Reed-cs
    address: Department of Computer Science, Reed College, Portland, OR, 97202

bibliography: ../sysconf.bib
output:
  bookdown::pdf_book:
    base_format: rticles::plos_article
    keep_tex: yes
csl: plos.csl
abstract: |
  The gender gap in computer science (CS) research is a well-studied problem, with an estimated ratio of 15%--30% women researchers.  However, far less is known about gender representation in specific fields within CS. Here, we investigate the gender gap in one large field, computer systems. To this end, we combined data from 53 leading systems conferences with external demographic and bibliometric data to evaluate the ratio of women authors and the factors that might affect this ratio.

   Our main findings are that women represent only about 10% of systems researchers, and that this ratio is not associated with various conference factors such as size, prestige, double-blind reviewing, and inclusivity policies.  Author research experience also does not significantly affect this ratio, although author country and work sector do.

  The 10% ratio of women authors is significantly lower than that of CS as a whole. Our findings suggest that focusing on inclusivity policies alone cannot address this large gap. Increasing women's participation in systems research will require addressing the systemic causes of their exclusion, which are even more pronounced in systems than in the rest of CS.
---

```{r code = readLines("../load_data.R"), echo=F, message=F}
```


```{r setup, echo=F, message=F, warning=F, cache=F}
if (!"ggthemr" %in% rownames(installed.packages())) {
  require("devtools")
  devtools::install_github("cttobin/ggthemr")
}

library(cowplot)
library(ggthemr)
library(ggrepel)
library(ggdist)
library(kableExtra)
library(lmerTest)
library(MuMIn)
library(readr)
library(rjson)

ggthemr('solarized')

#sys_authors <- filter(persons, as_author > 0, !is.na(gender))
#sys_pcs <- sys_persons %>% filter(as_pc + as_pc_chair > 0)
sys_pcs$tot_pc <- sys_pcs$as_pc + sys_pcs$as_pc_chair

# sys_people_tidy$gender[is.na(sys_people_tidy$gender)] <- "F"  # Sensitivity testing

repeated <- sys_people_tidy %>%
  filter(!is.na(gender)) %>%
  group_by(role) %>%
  summarize(N = n(), pct_w = round(100 * sum(gender == "F") / n(), 2))

unique <- sys_people_tidy %>%
  filter(!is.na(gender)) %>%
  dplyr::select(name, gs_email, role, gender) %>%
  unique() %>%
  group_by(role) %>%
  summarize(N = n(), pct_w = round(100 * sum(gender == "F") / n(), 2))

role_genders <- data.frame(Role = c("Author", "Lead author", "Last Author", "Keynote speaker", "Panelist", "PC chair", "PC member", "Session chair"),
                  "Total" = repeated$N,
                  "Women" = paste0(format(repeated$pct_w, nsmall = 2), "%"),
                  "Unique" = unique$N,
                  "Unique women" = paste0(unique$pct_w, "%"),
                  check.names = F)

gender_table <- data.frame(female = c(nrow(filter(sys_people_tidy, gender == "F", role == "author")), nrow(filter(people_tidy, gender == "F", role == "pc"))),
                             male = c(nrow(filter(sys_people_tidy, gender == "M", role == "author")), nrow(filter(sys_people_tidy, gender == "M", role == "pc"))),
                        row.names = c("authors", "PC members"))

conf_gender <- sys_people_tidy %>%
  filter(!is.na(gender)) %>%
  group_by(role, conf) %>%
  summarize(ratio_w = sum(gender=="F") / n()) %>%
  left_join(sys_confs, by = c("conf" = "conference")) %>%
  mutate(conf = as.factor(gsub("_\\d+", "", as.character(conf)))) %>%
  mutate(Blind = ifelse(double_blind, "Double-blind", "Single-blind")) %>%
  pivot_wider(names_from = role, values_from = ratio_w) %>%
  rename("PC" = "pc")

group_names <- c("Author (female)", "PC member (female)", "Author (male)", "PC member (male)")

demographics <- sys_roles %>%
  filter(role %in% c("author", "pc")) %>%
  dplyr::select(name, gs_email, role) %>%
  unique() %>%
  left_join(persons) %>%
  filter(!is.na(gender)) %>%
  mutate(gid = group_indices(., gender, role)) %>%
  group_by(gender, role) %>%
  mutate(gname = group_names[gid]) %>%
  add_count() %>%
  ungroup()

hindex <- demographics %>%
  drop_na(gender) %>%
  drop_na(hindex) %>%
  mutate(Experience = as.factor(ifelse(hindex < 12, "Novice", ifelse(hindex <= 18, "Mid-career", "Experienced"))))

repeat_authors <- filter(sys_roles, role == "author") %>%
  left_join(persons)

repeat_pc <- filter(sys_roles, role == "pc") %>%
  left_join(persons)

gendiff <- function(role_, metric_) {
  t.test(filter(demographics, role == role_, gender=="F")$hindex,
         filter(demographics, role == role_, gender=="M")$hindex)
}

pc_diff_p <- gendiff("pc", "h-index")
author_diff_p <- gendiff("author", "h-index")

nauthors <- sys_roles %>%
  filter(role == "author") %>%
  group_by(key) %>%
  summarize(nauthors = n())

sys_papers <- left_join(sys_papers, nauthors)

# Shortcut data frames to compare sectors:
ra <- filter(repeat_authors, !is.na(gender), !is.na(sector))
da <- filter(demographics, !is.na(gender), !is.na(sector), role == "author")
dp <- filter(demographics, !is.na(gender), !is.na(sector), role == "pc");

# The palette with grey:
cbp1 <- c("#999999", "#E69F00", "#56B4E9", "#009E73",
          "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

# The palette with black:
cbp2 <- c("#000000", "#E69F00", "#56B4E9", "#009E73",
          "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

country_count = sys_persons %>%
  drop_na(country) %>%
  group_by(country) %>%
  summarize(n = n()) %>%
  nrow()

country_stats <- function(code_, who_, by_affil = TRUE)
{
  ppl <- who_ %>%
    drop_na(gender) %>%
    drop_na(country) %>%
    left_join(sys_confs, by = c("conf" = "key"))

  tbl <- NULL
  if (by_affil) {
    tbl <- table(as.character(ppl$country.x) == as.character(code_), ppl$gender == "F")
  } else {
    if (nrow(filter(ppl, gender == "F", as.character(country.y) == as.character(code_))) > 0) {
      tbl <- table(as.character(ppl$country.y) == as.character(code_), ppl$gender == "F")
    }
  }

  if (is.null(tbl)) {
    return(NA)
  }
  return(paste0(pct(tbl[2, 2], sum(tbl[2,], 2)), "%", report_test(chisq.test(tbl), p_option = "stars", show_stat = F)))
}

cntry <- demographics %>%
  drop_na(country) %>%
  group_by(country) %>%
  summarize(authors = sum(as_author), pcs = sum(as_pc)) %>%
  arrange(desc(authors)) %>%
  rename(code = country) %>%
  filter(row_number() <= 20)
#  filter(author > 100)

cntry$a_by_affil = unlist(lapply(cntry$code, function (c) country_stats(c, repeat_authors, T)))
cntry$a_by_conf = unlist(lapply(cntry$code, function (c) country_stats(c, repeat_authors, F)))
cntry$pc_by_affil = unlist(lapply(cntry$code, function (c) country_stats(c, repeat_pc, T)))
cntry$pc_by_conf = unlist(lapply(cntry$code, function (c) country_stats(c, repeat_pc, F)))
cntry$total_hosted = unlist(lapply(cntry$code, function (c) sum(as.character(sys_confs$country) == c)))

##############################
# Non-systems conference data:
json_to_df <- function(conf_name, gender_mapping) {
  json_fpath <- paste0(toplevel, "data/conf/", conf_name, ".json")
  conf <- rjson::fromJSON(file = json_fpath)
  confname <- conf[[1]]
  field <- conf[['field']]
  subfield <- conf[['subfield']]
  papers <- conf[['papers']]

  conf_info_df <- data.frame()

  for (i in seq_along(papers)) {
    paper_id <- unlist(papers[[i]][1])
    authors <- as.data.frame(papers[[i]][3]) %>%
      mutate(conf = confname,
             field = field,
             subfield = subfield,
             paper_id = paper_id)

    conf_info_df <- rbind(conf_info_df, authors)
  }

  conf_info_df <- conf_info_df %>%
    mutate(authors = map_chr(authors, normalized_author_name)) %>%
    mutate(authors = str_replace(authors, "\\s+", " ")) %>%
    mutate(paper_id = as.factor(paper_id)) %>%
    rename(name = authors)

  conf_info_df %>%
    left_join(gender_mapping, by = "name")
}

# Here we create a vector of the conference names and use the 'map_dfr' function to apply 'json_to_df' to each conference.
nonsys_confs <-
  c("AAAI", "ACL", "CHI", "CVPR", "FSE", "ICML", "ICSE", "MM", "NIPS", "POPL", "SIGCSE", "SIGGRAPH", "SODA", "STOC", "WSDM", "WWW", "SIGIR", "KDD", "ICDM", "ITICSE", "FOCS", "TACAS")

nonsys_authors <- nonsys_confs %>%
  map_dfr(~json_to_df(.x, all_genders))

```

\newpage{}

# Introduction

The highly publicized gender gap in computer science (CS) carries significant societal effects, such as inequality in economic opportunities for women and an undersupply of researchers and engineers in the rapidly growing discipline [@editorials18:science; @mattis07:upstream].
The gender gap among researchers is particularly severe: the people who participate in research, publish about it, and have their research acknowledged for its value are predominantly men [@charman17:championing].
Numerous studies estimate that only about 15%--30% of the CS research community are women [@cohoon11:cspapers; @holman18:gender; @national20:science; @way16:gender; @zweben18:taulbee].
Although some recent indications show these numbers could be growing, they remain low, and the rate of growth remains slow [@wang21:trends].

CS is an expansive and diverse discipline with different characteristics in each of its constituent fields [@cheong21:communities].
Treating CS as one homogeneous area risks missing some of the gender disparity phenomena that show up more acutely in specific fields.
In this paper, we focus on one such field, computer systems (or "systems" for short).
Systems is a large research field with numerous applications, used by some of the largest technology companies in the world.
For the purpose of this study, we define systems as the study and engineering of concrete computing systems, which includes research topics such as: operating systems, computer architectures, data storage and management, compilers, parallel and distributed computing, and computer networks.

This field stands out from other areas of CS in that it emphasizes scientific exploration through system implementation and combines engineering, experimentation, simulation, and mathematical rigor.
The United States (US) currently dominates the field, both in terms of affiliated researchers and of hosted conferences, so we take particular interest in the gender gap in the US.

<!-- There exists sporadic evidence of an acute gender gap in specific subareas of systems [@destefano18:micro; @jerger17:gender; @mattauch20:bibliometric], but we were unable to find a systematic examination of the entire field. -->
To measure this gap accurately, we manually curated gender data from a large and representative cross section of the field.
We estimate the rate of women's participation in systems research by using the proxy metric of female author ratio (FAR) in a set of peer-reviewed systems conferences.
This approach has been previously tested in numerous researcher populations, typically using automated gender inference from given names [@elsevier17:gender; @lariviere13:bibliometrics; @mattauch20:bibliometric; @west19:discriminating].
Because our methodology relies primarily on manually curated data, it has better coverage and accuracy than that of studies based on automated gender-inference approaches.

In addition to computing gender ratios, we also collected and analyzed conference statistics, demographic data, and bibliometrics from Google Scholar and Semantic Scholar to examine how these factors interact with women researcher ratios.
Our dataset includes `r nrow(sys_confs)` systems conferences, totaling `r sum(sys_confs$npapers)` papers and `r nrow(persons)` unique researchers across different conference roles, as detailed in the next section.


This expansive dataset allows us to explore a number of research questions.
The most important of these is the accurate quantification of the ratio of women in systems, which to the best of our knowledge, had never been computed for the entire field.
To understand the extent of the gender gap in the field, and to benchmark our future progress in addressing it, it is vital that we start with a baseline measurement.

A related important question is, how does the representation of women in systems compare to other fields on CS?
In order to understand whether the representation of women in systems is different than in other CS fields, and if so, why, we must compare gender statistics across fields.
We review the limited literature on the topic, as well as data we collected ourselves from other conferences, to provide additional evidence and hypotheses of the differences across fields.

The third and broadest subject we consider is the relationship between this ratio and various potential explanatory variables, including geography, researcher experience, and policies explicitly designed to improve diversity in CS conferences.
Understanding the factors associated with the gender gap may offer clues to its causes and non-causes, eventually establishing a path towards addressing it.
To this end, we compare gender statistics across multiple explanatory variables we collected, and use these variables to build a multivariate mixed-effects model of women's underrepresentation in systems.

<!----------------------------------------------------------------------------------------------------->
<!----------------------------------------------------------------------------------------------------->

# Materials and methods {#sec:methodology}

```{r sys-confs, echo=F, message=F, warning=F, cache=T}
tmp <- sys_confs %>%
  left_join(countries, by = c("country" = "code")) %>%
  mutate(Conference = gsub("_\\d*", "", conference)) %>%
  mutate(size = factor(ifelse(npapers <= 30, "S", ifelse(npapers <= 60, "M", "L")))) %>%
  group_by(size) %>%
  arrange(desc(acceptance_rate), .by_group = T) %>%
  ungroup() %>%
  rename(Date = postdate, Papers = npapers, Authors = authors_num, Region = subregion) %>%
  mutate(Acceptance = round(acceptance_rate, 2)) %>%
  dplyr::select(Conference, Acceptance, Papers, Authors, Region)

tmp %>%
  knitr::kable(booktabs = T,
#               linesep = c(sum(tmp$size == "L"), sum(tmp$size == "M"))),
              linesep = c(rep("", 9), "\\addlinespace", rep("", 19), "\\addlinespace", rep("", 23)),
               align = c("r", "c", "r", "r", "r", "l"),
               caption = "System conferences, including acceptance rate, number of published papers, total number of named authors, and geographical region. Conferences are grouped by size (over 60 papers, 31--60, and 30 or under) and sorted by acceptance rate in each group.") %>%
  kable_styling(font_size = 8, latex_options = "hold_position")
```

To answer these research questions, we sought data on participants in a large cross-section of the entire research field of computer systems, as well as some non-systems CS conferences for comparison.
The primary dataset we analyze comes from a hand-curated collection of `r nrow(sys_confs)` peer-reviewed systems conferences from a single publication year (2017).

In CS, and in particular in its more applied fields such as systems, original scientific results are typically first published in peer-reviewed conferences [@patterson99:evaluating; @patterson04:health], and then possibly in archival journals, sometimes years later [@vrettas15:conferences].
The conferences we selected include some of the most prestigious systems conferences (based on indirect measurements such as Google Scholar's metrics), as well as several smaller or less-competitive conferences for contrast, shown in Table \@ref(tab:sys-confs).
To reduce time-related variance, we chose to focus on a large cross-sectional set of conferences from a single publication year.

Our choice of which conferences belong to "systems" is necessarily subjective.
Not all systems papers from 2017 are included in our set, and some papers that are in our set may not be universally considered part of systems (for example, if they lean more towards algorithms or theory).
Nevertheless, we believe that our cross-sectional set is both wide enough to represent the field well and focused enough to distinguish it from the rest of CS.
In total, our sample includes `r sum(sys_confs$npapers)` peer-reviewed systems papers.

Because our metric for the gender gap counts the percentage of women among authors, we collected the names and author positions of all
`r filter(sys_roles, role == "author") %>% dplyr::select(name, gs_email) %>% unique() %>% nrow()`
unique coauthors.
Papers in our dataset average
`r round(mean(sys_papers$nauthors), 2)`
coauthors per paper, and of the
`r p3 <- filter(sys_papers, nauthors > 2); nrow(p3)`
papers with three or more coauthors, only
`r pct(nrow(filter(p3, alphabetized)), nrow(p3), 2)`%
ordered the author list alphabetically.
Papers in systems tend to list the primary contributor in the leading (first) position and senior authors last, so we examined the gender of first and last authors as well.

In addition to paper authors, we collected information on researchers in the following conference roles:

 * program committee (PC) chairs, who coordinate the review activities
(`r filter(sys_roles, role == "chair") %>% dplyr::select(name, gs_email) %>% unique() %>% nrow()` total).

 * PC members, who conduct most of the paper reviews and therefore have a direct influence on which papers get accepted
(`r filter(sys_roles, role == "pc") %>% dplyr::select(name, gs_email) %>% unique() %>% nrow()` total).

 * Keynote speakers
(`r filter(sys_roles, role == "keynote") %>% dplyr::select(name, gs_email) %>% unique() %>% nrow()` total),
panelists
(`r filter(sys_roles, role == "panel") %>% dplyr::select(name, gs_email) %>% unique() %>% nrow()` total),
and session chairs
(`r filter(sys_roles, role == "session") %>% dplyr::select(name, gs_email) %>% unique() %>% nrow()` total),
who have no direct influence on the population of authors, but represent the "face" of the conference to attendees. The visibility of women for such role models may have an indirect impact or appeal for women practitioners [@destefano18:micro; @davenport14:studying].

For this study, the most critical piece of information on these researchers is their \emph{perceived gender}.
Gender is a complex, multifaceted identity, but most bibliometric studies still rely on binary genders---either collected by the journal, or inferred from first name---because that is the only designator available to them [@bhagat18:data; @cohoon11:cspapers; @holman18:gender; @national20:science; @wang21:trends; @way16:gender; @zweben18:taulbee].
In the absence of self-identified gender information for our authors, we also necessarily compromised on using binary gender designations.
We therefore use the gender terms "women" and "men" interchangeably with the sex terms "female" and "male".
The conferences in our dataset did not collect or share specific gender information, so we had to collect this information from other public sources.
Similar studies have typically used automated gender-inference services based on forename and sometimes country of origin [@huang19:historical; @karimi16:gender].
These statistical approaches can be reasonably accurate for names of Western origin, and especially for male names  [@cohoon11:cspapers; @mattauch20:bibliometric; @santamaria18:comparison].

We opted instead to rely primarily on a manual approach that can overcome the limitations of name-based inference.
Using web lookup, we assigned the gender of
`r inferred <- nrow(semi_join(sys_persons, inferred_gender)); nongend <- nrow(filter(sys_persons, is.na(gender))); pct(nrow(sys_persons) - inferred - nongend, nrow(sys_persons), 2)`%
of the researchers for whom we could identify an unambiguous web page with a recognizable gendered pronoun or absent that, a photo.
(For example, many Linkedin profiles may lack a photo, but include a gendered pronoun in the recommendations section.)
For `r pct(inferred, nrow(sys_persons), 2)`%
others, we used genderize.io's automated gender designations if it was at least 70% confident about them [@santamaria18:comparison].
The remaining `r nongend` persons were not assigned a gender and were excluded from most analyses.
This method provided more gender data and higher accuracy than automated approaches based on forename and country, especially for women [@karimi16:gender; @lariviere13:bibliometrics; @mattauch20:bibliometric; @squazzoni20:noevidence; @wang21:trends].

This labor-intensive approach does introduce the prospect of human bias and error.
For example, a gender assigned by an outdated biography paragraph with pronouns may no longer agree with the self-identification of the researcher.
To verify the validity of our approach, we compared our manually assigned genders to self-assigned binary genders in a separate survey we conducted among 918 of the authors  [@frachtenberg20:survey].
We found no disagreements for these authors, which suggests that the likelihood of disagreements among the remaining authors is low.

Conferences also do not generally offer information on authors' demographics, but we were able to unambiguously link approximately two thirds (`r pct(nrow(filter(sys_persons, !is.na(hindex))), nrow(sys_persons))`%) of
researchers in our dataset to a Google Scholar (GS) profile.
For each author and PC member, we collected all metrics in their GS profile, such as total previous publications (ca. 2017), h-index, etc.
Note that we found no GS profile for about a third of the researchers
(`r pct(sum(is.na(sys_authors$hindex)), nrow(sys_authors), 2)`%),
and these researchers appear to be less experienced than researchers with a GS profile.
We therefore collected another proxy metric for author experience (total number of past publications) from another source, the Semantic Scholar database.

We also looked up each author's affiliation institute on GS to find their contemporaneous country of residence and work sector whenever they could be clearly inferred using hand-coded regular expressions.
Many authors also included their email address in the full text of the paper, from which we inferred more timely affiliation and country information when available.

From authors' affiliations, we broadly categorized their work sector as either "COM" for industry
(`r pct(nrow(filter(demographics, sector == "COM")), nrow(filter(demographics, !is.na(sector))), 1)`% of all unique authors and PC members),
"EDU" for academia,
(`r pct(nrow(filter(demographics, sector == "EDU")), nrow(filter(demographics, !is.na(sector))), 1)`%),
or "GOV" for government and national labs
(`r pct(nrow(filter(demographics, sector == "GOV")), nrow(filter(demographics, !is.na(sector))), 1)`%).


In addition to researcher information, we gathered various statistics on each conference, either from its web page, proceedings, or directly from its chairs.
We collected data about review policies, important dates, the composition of its technical PC, and the number of submitted papers, among others.
We also collected historical metrics from the Institute of Electrical and Electronics Engineers (IEEE), Association for Computing Machinery (ACM), and Google Scholar (GS) websites, including past citations, age, and total publications, and downloaded all `r nrow(sys_papers)` papers.
Finally, from each conference's web site and proceedings we collected information on any explicit policies the conference made to increase attendance diversity (Table \@ref(tab:diversity-policy)), so that we could measure their effects, if any, on the gender gap.

The focus group of this study is computer systems researchers, but to provide a more accurate picture of where this field stands in comparison to other in CS, we needed to collect additional information on non-systems conferences.
We selected `r nrow(nonsys_confs)` conferences in other CS fields from the same year, primarily based on their ranking on Google Scholar metrics as leaders in their respective fields (Table \@ref(tab:nonsys-confs)).

```{r nonsys-confs, echo=F, message=F, warning=F, cache=T}
nonsys_authors %>%
  group_by(field) %>%
  summarise(
#    Conferences = paste(unique(gsub("_\\d+", "", as.character(conf))), collapse = ", "),
    Papers = n_distinct(paper_id),
    Authors = n(),
    Men = sum(gender == "M", na.rm = T),
    Women = sum(gender == "F", na.rm = T),
    "NA" = sum(is.na(gender)),
    FAR = round(Women / (Women + Men), 2)
    ) %>%
  rename(Field = field) %>%
  arrange(desc(FAR)) %>%
  knitr::kable(booktabs = T,
               linesep = "",
               align = c("l", "l", "r", "r", "r", "r", "r", "r"),
               caption = "Sampled set of non-systems CS conferences, categorized broadly into six fields and ordered by ratio of women authors. Gender data comes from generizer.io when at least 90\\% accuracy of prediction or manual Web search otherwise. The ratio of women among authors (FAR) excludes unknown genders.") %>%
  kable_styling(font_size = 8, latex_options = "hold_position")
```

These conferences accepted papers from
`r nrow(verified_gender_nonsys) + nrow(inferred_gender_nonsys)`
unique authors.
Because of the large manual effort involved in our approach for systems papers, we limited this data collection to genders and author positions for all non-systems authors.
The gender collection methodology followed Chatterjee and Werner [@chatterjee21:gender], first assigning genders to
`r nrow(inferred_gender_nonsys)` authors
using genderize.io's inference service when its probability of accuracy was at least 90%.
For the remaining `r sum(!is.na(verified_gender_nonsys$gender))` authors, we looked up genders manually on the web as we have with systems conferences, leaving only
`r sum(is.na(verified_gender_nonsys$gender))` unknown genders.
The overall gender statistics for these conferences are shown in Table \@ref(tab:nonsys-confs).


## Statistics {-}

For statistical testing, group means were compared pairwise using Welch's two-sample t-test and group medians using the Wilcoxon Signed Rank Test; differences between distributions of two categorical variables were tested with the $\chi^{2}$ test; and comparisons between two numerical variables were evaluated with Pearson's product moment correlation coefficient.
All statistical tests are reported with their p-values.
Mixed-effects logistic regression models were assessed with Satterthwaite's degrees of freedom method for hypothesis testing on model coefficients.

## Code and data availability {-}

The complete dataset and source code necessary to reproduce this analysis can be found in the supplementary materials, as well as at [https://github.com/eitanf/sysconf](https://github.com/eitanf/sysconf).
The specific analyses of this article are in the file `pubs/gender-gap/gender-gap.Rmd`.
<!--A Docker image with the complete software and data environment to reproduce these results is available at [https://hub.docker.com/repository/docker/eitanf/sysconf](https://hub.docker.com/repository/docker/eitanf/sysconf), using the `gender-gap` tag.-->


## Ethics statement {-}

The data collected for this study was sourced from public-use datasets such as conference and academic web pages.
This study was exempted from the informed consent requirement by Reed College's Institutional Review Board (No. 2021-S26) under Exempt Category 4: the use of secondary data.


## Limitations {-}

Our study uses the FAR proxy metric to estimate women's participation in systems research, as do comparable studies estimating the gender gap in other fields [@elsevier17:gender; @lariviere13:bibliometrics; @mattauch20:bibliometric].
FAR has been found to correlate tightly with gender ratios across disciplines [@holman18:gender].
Nevertheless, it is important to keep in mind that FAR may undercount women if men are more likely to submit papers or have them accepted.

We believe and demonstrate that the magnitude of this undercounting is small and insufficient on its own to explain the large gap with the overall CS statistics from past publications (which also use the same metric, with the same limitations).

In the literature, we found few controlled experiments that evaluate the peer-review process on both accepted and rejected papers, and they are typically limited in scope to a single conference or journal [@parno17:SPsurvey; @shah18:design; @tomkins17:reviewer].
We chose an observational approach that allowed us to examine an entire field of study and produce metrics that are comparable with those in other fields.
The main limitation of this approach is that it may miscount women if there is significant gender bias in the publication or review processes.
Nevertheless, the resulting statistics are directly comparable to other studies employing the same approach.
Moreover, our survey results indicate that such peer-review bias may be limited [@frachtenberg20:survey].

Our methodology is also constrained by the manual collection of data.
The effort involved in compiling all the necessary data limits the scalability of our approach to additional conferences or years.
Furthermore, the manual assignment of genders is a laborious process, prone to human error.
Nevertheless, such errors appear to be smaller in quantity and bias than those of automated approaches, as discussed previously.

Even with manual gender assignment, `r pct(nongend, nrow(persons), 2)`% of researchers still have unassigned gender.
Although this ratio is small, and smaller than that of most other studies we reviewed, we nevertheless performed a sensitivity analysis to examine its effect.
We first artificially set the gender of all `r nongend` researchers to women, and then to men, and recomputed all statistical analyses.
None of our observations were subsequently changed in either direction or statistical significance, which justified our decision to omit these missing data points from the analysis.

<!----------------------------------------------------------------------------------------------------->
<!----------------------------------------------------------------------------------------------------->

# Results {#sec:results}

## Representation of women across conference roles

We start with our first research question: estimating the actual ratio of women among computer systems researchers.
With the data we collected on conference participants, we can compute the ratio of women in different conference roles: peer-reviewed authors, reviewers, and invited presenters (Table \@ref(tab:pct-by-role)).
We found that approximately `r filter(role_genders, Role=="Author")$Women` of published authors were women.
Across the various other (invited) roles, women represent a weighted average of
`r round(weighted.mean(unlist(repeated[4:8,3]), unlist(repeated[4:8,2])), 2)`%
of researchers.

Since `r pct(nrow(filter(sys_authors, as_author > 1)), nrow(sys_authors), 2)`%
of authors are named in more than one paper, we compared counting each person exactly once to counting repeated occurrences of each person.
With both counts, the gender ratios remain within a percentage point or so of each other.
We also examined authorship outliers, because these can be linked with gender [@huang19:historical].
In our dataset, all
authors with more than seven papers are men, and only
`r nrow(filter(sys_authors, as_author > 4, gender=="F"))`
of the
`r nrow(filter(sys_authors, as_author > 4))`
authors with more than four papers are women.
But removing all authors with more than four papers from our dataset would change women's underrepresentation by less than a percentage point.
The effect of outliers on PC female representation is similarly small.
We therefore decided to use the complete dataset of persons for the rest of this study, counting with repeats, as do comparable studies.

```{r pct-by-role, echo=F, cache=F, message=F}
knitr::kable(role_genders[c(6,7,4,5,8,1,2,3),], align=c("lrrrr"), booktabs = T, row.names = F,
             caption = "Researcher count and ratio of women by conference role. Researchers are either aggregated by total appearances or identified uniquely, once per role. Lead authors in systems are typically the primary contributor and last authors are typically the senior member of the team.")
```


The second-largest group of researchers, and the largest invited group, is that of program committee (PC) members.
This group can also indirectly affect the representation of women among published authors, because PC members, through their reviews, decide which papers get published.
The ratio of female PC members (FPR) is significantly higher than the ratio of female authors,
[`r filter(role_genders, Role=="PC member")$Women` vs. `r filter(role_genders, Role=="Author")$Women`, `r tst <- chisq.test(gender_table); paste0("$\\chi{}^2=", round(tst$statistic, 3), "$, degrees of freedom (df) $=1$, $p<0.000001$")`].
The large difference in ratios raises the question: which of the two is more representative of women's true participation rate in systems research?

We chose the typical bibliometric approach to estimating participation by gender, namely to look at published authors, or FAR [@cohoon11:cspapers; @mattauch20:bibliometric].
This metric is not always accurate: it ignores researchers with limited access to publishing, and potentially undercounts female scientists because they tend to publish less than men in many fields [@elsevier20:journey; @ghiasi15:compliance; @lariviere13:bibliometrics; @morgan21:unequal; @symonds06:gender], possibly owing to a higher service load [@guarino17:faculty; @misra12:gender; @omeara17:asked].
Confirming this past finding, women published only
`r faa <- filter(sys_authors, gender == "F")$as_author; round(mean(faa, na.rm = T), 2)`
papers in our dataset on average, compared to men's
`r maa <- filter(sys_authors, gender == "M")$as_author; round(mean(maa, na.rm = T), 2)`
(`r report_test(t.test(faa, maa), 2, show_df = T)`).
However, this $\approx{`r 100 * round(mean(maa)/mean(faa) - 1, 3)`\%}$ difference is insufficient to explain the large discrepancy with gender representation in invited roles.

Unlike PC members, authors underwent blind and competitive peer review, averaging an acceptance rate of
`r round(100 * mean(sys_confs$acceptance_rate, na.rm = T), 1)`%
in our dataset.
This selection process is presumably more objective and less biased than one based on invitation [@lee13:bias].
If a biased review process allowed for a disproportionate number of women-authored papers to be published, it would mean that the gender gap in the author sample is not reflective of the researcher population as a whole, but that is not what we found.
Mirroring studies from other fields that found no evidence of gender bias in the peer-review process [@cohoon11:cspapers; @fox16:gender; @squazzoni20:noevidence], we found that women's papers were actually accepted at slightly higher rates when their identity was visible to reviewers
(in `r sum(sys_confs$double_blind)` single-blind conferences)
or when it was prominent in the first author position
(`r leads <- filter(sys_roles, role == "lead_author") %>% left_join(sys_persons) %>% filter(!is.na(gender)); pct(nrow(filter(leads, gender == "F")), nrow(leads))`% of papers).
An author survey also found that the reviews women received in the single-blind conferences in our dataset showed similar or higher grades than men's [@frachtenberg20:survey].

Contrariwise, our data suggests that it is the selection-by-invitation process that exhibits gender bias.
Unlike women's underrepresentation in the editorial boards of many journals [@amrein11:editorial; @lerback17:journals; @mauleon13:assessing; @topaz16:gender], in our dataset women PC roles outnumber women author roles by some 75%.
We hypothesize that this difference stems from an affirmative effort by conference chairs to bring gender closer to parity.
This hypothesis, and our consequent reliance of FAR instead of FPR, is supported by three observations.


First, if chairs are indeed oversampling women for PC roles, we would expect to see differences in experience statistics across genders.
For example, chairs may have to search deeper in the researcher pool to recruit women to the PC, leading to lower research experience among women PC members, compared to their counterparts among men.
Our data corroborates this prediction (Fig. \@ref(fig:bibliometrics)).
For example, the mean (median) h-index of women PC members,
`r round(mean(filter(demographics, role == "pc", gender == "F")$hindex, na.rm = T), 2)`,
(`r round(median(filter(demographics, role == "pc", gender == "F")$hindex, na.rm = T), 2)`),
is significantly lower than mens'
`r round(mean(filter(demographics, role == "pc", gender == "M")$hindex, na.rm = T), 2)`
(`r round(median(filter(demographics, role == "pc", gender == "M")$hindex, na.rm = T), 2)`);
`r report_test(gendiff("pc", "h-index"), 3, show_df = T)`;
`r report_test(wilcox.test(filter(demographics, role == "pc", gender == "F")$hindex, filter(demographics, role == "pc", gender == "M")$hindex))`.
In contrast, the author h-index means (medians) are closer together:
`r round(mean(filter(demographics, role == "author", gender == "F")$hindex, na.rm = T), 2)`
(`r round(median(filter(demographics, role == "author", gender == "F")$hindex, na.rm = T), 2)`)
vs.
`r round(mean(filter(demographics, role == "author", gender == "M")$hindex, na.rm = T), 2)`,
(`r round(median(filter(demographics, role == "author", gender == "M")$hindex, na.rm = T), 2)`);
`r report_test(gendiff("author", "h-index"), 3, show_df = T)`;
`r report_test(wilcox.test(filter(demographics, role == "author", gender == "F")$hindex, filter(demographics, role == "author", gender == "M")$hindex))`.

```{r bibliometrics, message=F, echo=F, cache=F, warning=F, fig.cap="Distribution of h-index [@hirsch05:index] by role and gender (diamond represents mean). h-index values extracted from Google Scholar, ca. 2017. Each researcher was counted exactly once, unless no gender or h-index could be identified."}
demographics %>%
  drop_na(hindex) %>%
  group_by(gid) %>%
  add_count(name = "nona") %>%
  ungroup() %>%
  ggplot(aes(x = gname, y = hindex, fill = gname)) +
    ggdist::stat_halfeye(adjust = 0.5, justification = -.1, .width = 0, point_color = NA) +
    geom_boxplot(aes(fill = gname), width = .12, alpha = 0.5, notch = F, outlier.color = NA) +
    stat_summary(fun.y = mean, geom = "point", shape = 23, size = 3, color = "white", alpha = 0.7) +
    scale_fill_manual(values = c("#7704FF", "#00C3AA", "#7704FF", "#00C3AA")) +
    scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) +
    coord_flip(ylim = c(0, 60)) +
    labs(y = "h-index", x = "density") +
    geom_text(aes(y = 55, label = paste0("n=", nona)), vjust = -3.5) +
    theme(legend.position = "none")

# ggsave("Fig1.tiff", dpi = 300, compression = "lzw")
```


Second, if women are asked to serve on more PCs than men in relative terms, we would expect to find fewer unique women as PC members because of their repeated service [@jerger17:gender], as Table \@ref(tab:pct-by-role) indeed confirms.
This prediction is also corroborated by computing reviewer load, with
`r wpc <- filter(sys_pcs, gender == "F") %>% summarize(mean(as_pc + as_pc_chair)); round(wpc, 2)`
mean PC assignments (member and chair) per woman, compared to
`r mpc <- filter(sys_pcs, gender == "M") %>% summarize(mean(as_pc + as_pc_chair)); round(mpc, 2)`
per man
(`r report_test(t.test(filter(sys_pcs, gender == "F")$tot_pc, filter(sys_pcs, gender == "M")$tot_pc), 2, show_df = T)`).
Conceivably, the additional time committed to PC service explains some of the reduced publication rate we observed among women.
However, authors who serve as PC members also tend to publish more papers
(Pearson's `r report_test(cor.test(sys_authors$as_pc, sys_authors$as_author))`), suggesting that a relative overrepresentation of women in PCs is not commensurate with underrepresentation among authors.


Finally, the smaller population size of PC members (n=`r nrow(sys_pcs)`) compared to that of authors (n=`r nrow(sys_authors)`), magnifies statistical outliers.
Therefore, conferences with uncharacteristic gender gaps introduce more variance to PC gender ratios than to those of authors.
As shown in Fig. \@ref(fig:women-rep-author-vs-PC), the gender gap for PCs exhibits a much higher variance and longer tail across conferences than for authors.
Only two conferences show FPR values near parity, OOPSLA and ISPASS.
Excluding this pair changes the mean FPR across the remaining conferences by
`r without <- filter(sys_people_tidy, !conf %in% c("ISPASS_17", "OOPSLA_17"), !is.na(gender), role == "pc"); round(repeated[6,3] - pct(nrow(filter(without, gender == "F")), nrow(without)), 2)` percentage points.
Conversely, removing the two conferences with the lowest FAR values (HotI and VEE) only bumps up the mean FAR by
`r without <- filter(sys_people_tidy, !conf %in% c("HotI_17", "VEE_17"), !is.na(gender), role == "author"); round(pct(nrow(filter(without, gender == "F")), nrow(without)) - repeated[1,3], 2)` percentage points.
Skewness in distribution therefore pulls the mean women ratios higher among PCs than it pulls it lower among authors, reaffirming our assertion that FAR is more reliable than FPR as an indicator of the overall gender gap.

```{r women-rep-author-vs-PC, message=F, echo=F, cache=F, warning=F,  fig.cap="Underrepresentation of women among authors by conference, compared to conference size in papers, double-blind reviewing, and FPR. None of these factors is significantly associated with FAR. Density plots on the axes show the relative distribution of women authors and PC members for single- and double-blind reviews."}
colors = c(cbp2[2], cbp2[1])
colors = c("black", "red")

pmain <- conf_gender %>%
  ggplot(aes(x = PC, y = author, size = npapers, color = Blind)) +
    geom_point(alpha = 0.5) +
    xlab("Ratio of women among PC members (FPR)") +
    ylab("Ratio of women among authors (FAR)") +
    scale_x_continuous(limits = c(0, 0.53), breaks = seq(0, 0.55, 0.05), labels = paste0(100 * seq(0, 0.55, 0.05), "%")) +
    scale_y_continuous(limits = c(0, 0.24), breaks = seq(0, 0.25, 0.05), labels = paste0(100 * seq(0, 0.25, 0.05), "%")) +
    geom_text_repel(aes(label = conf), size = 1.75, max.overlaps = 20, family = "Times") +
    guides(size = guide_legend(title = "No. of papers"),
           color = guide_legend(title = "Reviews", label = T)) +
    scale_color_manual(values = colors) +
    theme(legend.position = "bottom")

xdens <- axis_canvas(pmain, axis = "x") +
  geom_density(data = conf_gender, aes(x = PC, fill = Blind), alpha = 0.5, size = 0.2) +
  scale_fill_manual(values = colors)

ydens <- axis_canvas(pmain, axis = "y", coord_flip = T) +
  geom_density(data = conf_gender, aes(x = author, fill = Blind), alpha = 0.5, size = 0.2) +
  scale_fill_manual(values = colors) +
  coord_flip()

p1 <- insert_xaxis_grob(pmain, xdens, grid::unit(.2, "null"), position = "top")
p2 <- insert_yaxis_grob(p1, ydens, grid::unit(.2, "null"), position = "right")
ggdraw(p2)
# ggsave("Fig2.tiff", p2, width = 19.05, units = "cm", dpi = 300, compression = "lzw")
```



<!----------------------------------------------------------------------------------------------------->

## Comparison to other fields {#subsec:fields}

The ratio of women among authors represents only a fraction of the ratio in the rest of CS, based on previous authorship studies that spanned the entire field.
This gap surfaces the question of whether it stems from differences across CS fields or from differences in measurement.

To answer this question, we collected more gender data on non-systems conferences from the same year.
Although our comparison data is necessarily constrained by the scalability of our manual collection approach, it still includes `r nrow(nonsys_authors)` non-unique authors from `r length(nonsys_confs)` of the top-cited non-systems CS conferences, based on GS metrics.
Despite the breadth limitations of this additional dataset (not all conferences in all fields are represented), it should be directly comparable to the systems dataset, and large enough to produce statistically significant results.
The data is also limited in depth, including only one year, but there is evidence that the underrepresentation of women in systems  did not vary much across a five-year period including 2017, at least for the subfield of high-performance computing [@frachtenberg21:whpc].

The results across fields are mixed, as expected (Table \@ref(tab:nonsys-confs)).
The fields of CS education and human-computer interaction appear to attract the most women, with the SIGCSE'17 conference approaching gender parity
(`r pct(nrow(filter(nonsys_authors, conf == "SIGCSE_17", gender == "F")), nrow(filter(nonsys_authors, conf == "SIGCSE_17", !is.na(gender))), 2)`% FAR).
The theoretical areas of CS exhibit the highest inequality, with the STOC'17 conference including only
`r nrow(filter(nonsys_authors, conf == "STOC_17", gender == "F"))` women
(`r pct(nrow(filter(nonsys_authors, conf == "STOC_17", gender == "F")), nrow(filter(nonsys_authors, conf == "STOC_17", !is.na(gender))), 2)`%)
among its authors.
The remaining three broad fields we evaluated show moderately higher FAR values than systems.

The overall FAR in the non-systems conferences we sampled was
`r pct(nrow(filter(nonsys_authors, gender == "F")), nrow(filter(nonsys_authors, !is.na(gender))), 2)`%,
which is significantly higher than the systems-only FAR
(`r f_sys <- sum(authors$gender == "F", na.rm = T); all_sys <- sum(!is.na(authors$gender)); f_nonsys <- sum(nonsys_authors$gender == "F", na.rm = T); all_nonsys <- sum(!is.na(nonsys_authors$gender)); report_test(chisq.test(data.frame(sys = c(f_sys, all_sys), nonsys = c(f_nonsys, all_nonsys))))`)
The ratio of women in CS across *all* systems- and non-systems authors in our dataset is
`r pct(f_sys + f_nonsys, all_sys + all_nonsys, 2)`%.
This ratio is lower than most estimates for women in CS in previous studies, and we look at some possible explanations for this difference in the related work section.
But it is still significantly higher than the FAR we found with comparable methodology in systems-conferences alone
(`r report_test(chisq.test(data.frame(sys = c(f_sys, all_sys), nonsys = c(f_sys + f_nonsys, all_sys + all_nonsys))))`).


<!----------------------------------------------------------------------------------------------------->

## Effect of conference factors {#subsec:conference}

The next step in understanding the gender gap is to look at the explanatory variables that may be associated with it, starting with conference-specific factors, and continuing to author-specific factors.
FAR varies considerably from one conference to the next
(minimum: `r round(100 * min(conf_gender$author), 2)`%,
maximum: `r round(100 * max(conf_gender$author), 2)`%,
mean: `r round(100 * mean(conf_gender$author), 2)`%,
SD: `r round(100 * sd(conf_gender$author), 2)`%).
Examining the differences between conferences could offer clues as to which factors might affect the gender gap.
We first examine four major factors: the size of the conference, its double-blind review policy, its gender diversity among reviewers, and its specific diversity and inclusivity policies.
We then explore the association (or lack thereof) between a conference's FAR and myriad other conference factors.
<!--Finally, we build a  mixed-effects linear model of the combination of all of these factors to evaluate their relative power in predicting a conference's FAR.-->


### Conference size {-}

Averaging the ratio of women by conferences, as opposed to by authors or papers (both computed in Table \@ref(tab:pct-by-role)), could produce different results, because smaller conferences receive the same weight as conferences with many more authors and papers.
This choice does not appear to affect the gender gap in our dataset, as all three means are within 0.53% of each other, with the conference mean at the center of the other two.
As shown in Fig. \@ref(fig:women-rep-author-vs-PC), the ratio of women among authors appears to be independent of the size of the conference (papers published), as well as its double-blind review policy, and its ratio of female PC members.
Statistically, there appears to be no correlation between a conference's size and its FAR
(`r report_test(cor.test(conf_gender$author, conf_gender$npapers))`).


### Double-blind reviewing {-}

Several past studies have found evidence of gender bias in the peer-review process, especially in single-blind reviews, although more recent surveys are inconclusive [@lee13:bias; @mcgillivray18:uptake; @squazzoni20:noevidence; @squazzoni21:peer].
In our dataset (Fig. \@ref(fig:women-rep-author-vs-PC)), conferences with double-blind reviewing actually exhibit a lower FAR
(`r round(100 * mean(filter(conf_gender, double_blind)$author), 3)`% mean vs.
`r round(mean(100 * filter(conf_gender, !double_blind)$author), 3)`% for single-blind conferences,
`r report_test(t.test(filter(conf_gender, double_blind)$author, filter(conf_gender, !double_blind)$author), show_df = T)`).

### Diversity across conference roles {-}

One review policy often employed to increase participant diversity is to invite a more diverse reviewer body.
For example, some studies have demonstrated gender homophily between reviewers and authors, leading to higher FAR values when more of the reviewers are women [@helmer17:gender; @murray19:gender].
Women are again far from parity in the composition of most PCs in our dataset, but with higher variance than in the author body.
Nevertheless, we found no correlation between higher FPR and higher FAR values
(`r report_test(cor.test(conf_gender$PC, conf_gender$author))`).
We also looked at other visible conference roles: keynote speakers, session chairs, and panelists.
These roles offer additional indication of the field's diversity, and may play a part in the diversity of conference attendees [@destefano18:micro; @davenport14:studying; @ford18:gender; @le20:ISCB].
However, the correlations between FAR and these roles reveal no such relationships here
(`r report_test(cor.test(conf_gender$keynote, conf_gender$author))`; `r report_test(cor.test(conf_gender$session, conf_gender$author))`; and `r report_test(cor.test(conf_gender$panel, conf_gender$author))`, respectively).

In summary, inviting more women to visible conference roles and implementing diversity-focused policies likely contributes to more inclusive conferences [@campbell18:defence; @ISC19:report], but is insufficient on its own to spontaneously add women authors to the field.

### Diversity initiatives {-}

```{r diversity-policy, echo=F, message=F, warning=F, cache=T}
div_confs = c("ATC", "CCS", "FAST", "HotCloud", "HotStorage", "IMC", "ISC", "ISCA", "ISPASS", "MobiCom", "NSDI", "SC", "SIGCOMM", "SP", "OOPSLA", "SLE", "PLDI")
div <- filter(conf_gender, conf %in% div_confs)
div <- div[match(div_confs, div$conf),] # Rearrange rows to match order of div_confs
nondiv <- filter(conf_gender, !conf %in% div_confs)
nondiv_authors <- filter(sys_people_tidy, !as.factor(gsub("_\\d+", "", as.character(conf))) %in% div_confs, role == "author")
nondiv_female_ratio <- nrow(filter(nondiv_authors, gender == "F")) / nrow(filter(nondiv_authors, !is.na(gender)))

divdf <- data.frame(Conference = div_confs,
#                           1  2  3  4  5  6  7  8  9  10 11 12 13 14 15 16 17
                  Chair = c(F, F, F, F, F, F, T, F, F, F, F, T, F, F, F, F, F),
                  Code =  c(T, F, F, T, T, T, T, F, F, T, F, T, T, F, T, T, F),
                  Event = c(T, T, T, T, T, F, F, F, F, F, T, T, T, F, T, F, T),
              Childcare = c(F, F, F, F, F, F, F, T, T, F, F, T, F, F, T, T, F),
                 Grants = c(T, F, F, F, T, F, F, F, F, T, F, F, F, T, F, F, F),
                   Data = c(F, F, F, F, F, F, T, F, F, F, F, T, F, F, F, F, F),
                 Papers = div$npapers,
                    FAR = round(div$author * 100, 2)
  ) %>%
  arrange(FAR) %>%
  rbind(data.frame(Conference = "All others", Chair = F, Code = F, Event = F, Childcare = F, Grants = F, Data = F, Papers = sum(nondiv$npapers), FAR = round(nondiv_female_ratio * 100, 2)))

Yes <- "Yes"
No <- "---"
divdf$FAR = sprintf("%.2f%%", divdf$FAR)
divdf$Chair = ifelse(divdf$Chair, Yes, No)
divdf$Code = ifelse(divdf$Code, Yes, No)
divdf$Event = ifelse(divdf$Event, Yes, No)
divdf$Childcare = ifelse(divdf$Childcare, Yes, No)
divdf$Grants = ifelse(divdf$Grants, Yes, No)
divdf$Data = ifelse(divdf$Data, Yes, No)

divdf %>%
  knitr::kable(booktabs = T,
               align = "lccccccrr",
               linesep = c(rep("", 16), "\\addlinespace"),
               caption = "Conferences with inclusivity initiatives, including diversity chair, code of conduct, special diversity events or workshops, assistance with childcare, travel grants for underrepresented minorities, and diversity data collection and publication. Conferences are ordered by increasing female author ratio (FAR). The last row summarizes the remaining conferences.")
```

Some specific policies that have been proposed to increase diversity in conferences include: a designated inclusivity chair; a code of conduct or anti-harassment policy; special events and meetings to promote diversity; assistance with childcare during the conference; travel grants for underrepresented populations; and the collection and dissemination of diversity data  [@collins16:diversity; @gould18:conferences; @martin14:ten].
Of our `r nrow(sys_confs)` conferences, `r sum(sys_confs$diversity_effort)` implemented at least one of these proposals (Table \@ref(tab:diversity-policy)), but that did not ostensibly lead to higher FAR values
(`r round(100 * mean(filter(conf_gender, diversity_effort)$author), 2)`% mean FAR vs.
`r round(mean(100 * filter(conf_gender, !diversity_effort)$author), 2)`% for the other conferences,
`r report_test(t.test(filter(conf_gender, diversity_effort)$author, filter(conf_gender, !diversity_effort)$author), 2, show_df = T)`).

As a prominent example, the only two conferences with an inclusivity chair, SC and ISC, ranked among the lowest conferences for FAR.
It is possible that these policies were in fact more reactive than proactive, in an attempt to improve previous statistics.
It is also possible that their effects can only be measured over several years.
Regrettably, none of the conferences have been consistently sharing author demographics to evaluate changes over time, although a few release some data.
The SC conference, for example, has been sharing demographic data since 2016.
Throughout this period, women's attendance rate remained near constant at around 13%-14% (FAR was only shared for 2018 at 12%).
ISC is another large conference that also employs various inclusivity initiatives, including naming a dedicated diversity chair and reporting attendee demographics.
It does not report FAR, but we have manually computed FAR for the four years since 2017 in the range 5%--9%, lower than the average conference in our dataset.

<!--We have also looked at the MICRO conference because it introduced childcare assistance and code of conduct in 2018

For the period examined, neither conference appears to show an improving FAR trend, although these cultural changes could take several years to take effect. Interestingly, starting from 2018, all ACM conferences are covered by a code of conduct. -->

It is plausible that inclusivity initiatives are only one of the selection criteria when choosing a conference to publish in, and that other criteria such as conference date, location, and subfield take precedence.
For example, among the four computer architecture conferences in our set (ASPLOS, HPCA, ISCA, MICRO), all with similar acceptance rates, only ISCA offered any diversity initiative, but all four show similar FAR.

A venue's prestige has also been previously linked to the gender gap in publication.
Examples include prestigious Mathematics journals that underrepresent women [@mihaljevic20:authorship], novel research published by women that is less likely to be impactful [@hofstra20:diversity], and men's tendency to self-cite more than women [@king17:men].
However, we found no direct correlations between a conference's prestige metrics and its ratio of women authors in computer systems.


<!--
It is also possible that these initiatives help indeed with boosting inclusivity in the short term, improving the subjective experience of women, if not their numbers [@campbell18:defence].
For example, in 2019 ISC reported [@ISC19:report] that 89.6% of all surveyed attendees "agreed that ISC is a conference that makes all attendees feel welcome."^[Gender breakdown for this question was unavailable so men could be overrepresented in this survey.]
Another conference, MICRO, has started its own inclusivity initiatives following complaints from its attendees [@martonosi17:statement].
Increasing gender diversity, on the other hand, could mean that more women are choosing and retaining a career in systems research, which is necessarily a longer-term process.

-->

### Additional conference factors {-}

In an attempt to uncover any nonobvious factors, we also collected various descriptive metrics on the different conferences and evaluated whether any of these metrics is associated with variations in FAR.
These metrics could potentially uncover hidden relationships with gender representation, such as: the competitiveness of a conference, the number of authors it attracts, the composition of its PC, its history, and organizational factors.

As Table \@ref(tab:add-factors) shows, none of these associations appears to be significant.
This finding was confirmed by building a combined linear model of a conference's FAR based on all of the factors we presented, where no coefficients turned out to be significant.
It should be noted that many of these factors are correlated, collinear, or connected by a confounding variable, but eliminating some factors with stepwise model selection still yielded no significant coefficients.
The per-conference FAR metric appears to be mostly independent of the factors we collected.

```{r eval = F, echo = F}
all_features <- sys_roles %>%
  filter(role == "author") %>%
  left_join(sys_authors) %>%
  drop_na(gender) %>%
  mutate(conf = gsub("_\\d+", "", conf)) %>%
  left_join(conf_gender, by = c("conf" = "conf"))

model <- glm(data = all_features, family = "binomial", gender ~ is_org_ACM + is_org_IEEE + is_org_USENIX + review_days + mean_pages + submissions + total_reviews + double_blind + rebuttal + open_access + diversity_effort + age + past_papers + past_citations + mean_historical_citations + h5_index + h5_median + chairs_num + pc_size + pc_author_ratio + npapers + authors_num + mean_authors_per_paper + acceptance_rate + pc_paper_ratio + mean_review_load)

summary(model)
```

The largest correlation we did observe, between FAR and ratio of authors from the PC, is still nonsignificant and small.
This correlation is unlikely to reveal a causal relationship, i.e., that inviting more women to the PC necessarily leads to increased FAR.
As we have seen, there is no real correlation between the two, but since conferences generally exhibit higher FPR than FAR, it makes sense that conferences with higher PC participation in the authorship would also exhibit higher relative FAR.

```{r add-factors, echo=F, message=F, warning=F, cache=T}
  facs <- c("acceptance rate", "h5 index (from GS)", "h5 median (from GS)", "Number of submissions",
            "Age in years", "Total past papers", "Mean number of pages", "Total citations", "Mean citations per paper",
            "Total number of authors", "Mean number of coauthors per paper",
            "Number of PC members", "Mean reviewer load (papers/day)", "Ratio of accepted papers from PC", "Ratio of accepted authors from PC",
            "Open access to papers", "IEEE conference", "ACM conference", "USENIX conference"
           )

  stats <- c(report_test(cor.test(conf_gender$author, conf_gender$acceptance_rate, use = "pairwise.complete.obs")),
             report_test(cor.test(conf_gender$author, conf_gender$h5_index)),
             report_test(cor.test(conf_gender$author, conf_gender$h5_median)),
             report_test(cor.test(conf_gender$author, conf_gender$submissions, use = "pairwise.complete.obs")),
             report_test(cor.test(conf_gender$author, conf_gender$age)),
             report_test(cor.test(conf_gender$author, conf_gender$past_papers)),
             report_test(cor.test(conf_gender$author, conf_gender$mean_historical_length)),
             report_test(cor.test(conf_gender$author, conf_gender$past_citations)),
             report_test(cor.test(conf_gender$author, conf_gender$mean_historical_citations)),
             report_test(cor.test(conf_gender$author, conf_gender$authors_num)),
             report_test(cor.test(conf_gender$author, conf_gender$mean_authors_per_paper)),
             report_test(cor.test(conf_gender$author, conf_gender$pc_size)),
             report_test(cor.test(conf_gender$author, conf_gender$mean_review_load, use = "pairwise.complete.obs")),
             report_test(cor.test(conf_gender$author, conf_gender$pc_paper_ratio)),
             report_test(cor.test(conf_gender$author, conf_gender$pc_author_ratio)),
             report_test(t.test(filter(conf_gender, open_access)$author, filter(conf_gender, !open_access)$author), show_df = F),
             report_test(t.test(filter(conf_gender, is_org_IEEE)$author, filter(conf_gender, !is_org_IEEE)$author), show_df = F),
             report_test(t.test(filter(conf_gender, is_org_ACM)$author, filter(conf_gender, !is_org_ACM)$author), show_df = F),
             report_test(t.test(filter(conf_gender, is_org_USENIX)$author, filter(conf_gender, !is_org_USENIX)$author), show_df = F)
            )

data.frame("Factor" = facs, "Test statistic" = stats, check.names = F) %>%
  knitr::kable("latex", booktabs = T, escape = F,
               caption = "Comparisons between conference FAR and additional conference factors") %>%
  pack_rows("Prestige and competitiveness metrics", 1, 4) %>%
  pack_rows("Metrics for past conferences in series", 5, 9) %>%
  pack_rows("Author statistics", 10, 11) %>%
  pack_rows("Program committee statistics", 12, 15) %>%
  pack_rows("Organizational factors", 16, 19)
```


<!----------------------------------------------------------------------------------------------------->

## Author factors {#sec:author}

In addition to conference-related factors, we also analyzed the effects on FAR of three author-related factors: research experience, work sector, and country of affiliation.

### Research experience {-}


As a proxy metric for research experience, we collected the h-index of each researcher with an identifiable GS profile and gender
(`r nrow(filter(hindex, role == "author"))` unique authors and
`r nrow(filter(hindex, role == "pc"))` unique PC members).
As Fig. \@ref(fig:bibliometrics) shows, female PC members exhibit a significantly lower mean and median h-index than males, but for authors, the differences across gender are not so large.
Comparing authors' total past publication count as another proxy metric for experience also reveals nonsignificant difference in means, medians, 1^st^, and 3^rd^ quartiles.
The only significant gender difference shown in Fig. \@ref(fig:bibliometrics) for authors is in the tail of the distribution, with men composing the majority of the top percentile
(`r top1 <- filter(demographics, role == "author", !is.na(hindex)) %>% slice_max(prop = 0.01, order_by = hindex); pct(sum(top1$gender == "M"), nrow(top1), 2)`%).

No woman in our dataset had an h-index above
`r max_w <- max(filter(hindex, gender == "F")$hindex); max_w`, but
`r sum_m <- sum(filter(hindex, gender == "M")$hindex > max_w); sum_m` men have, with a maximum of
`r max_m <- max(filter(hindex, gender == "M")$hindex); max_m`.
This is only a minuscule percentage of the sample population
(`r pct(sum_m, nrow(hindex))`%),
so it is hard to draw any conclusions from this gender difference.
It is nevertheless consistent with data in Table \@ref(tab:pct-by-role), where women in last author position (typically representing the senior member of the team), appear at a lower rate overall than women authors, and especially lower than lead authors (typically representing a junior member of the team).
These findings agree with past observations that women continue to senior academic ranks at a lower rate than men [@elsevier20:journey; @fox06:engineering; @frantzana19:women; @gerhard07:undergraduate; @mattis07:upstream].

### Work sector {-}

Compared to experience, the gender gap across work sectors is more pronounced.
Most unique authors in this dataset are affiliated with academic institutes
(`r pct(sum(da$sector == "EDU"), nrow(da), 1)`%),
followed by industry
(`r pct(sum(da$sector == "COM"), nrow(da), 1)`%)
and government
(`r pct(sum(da$sector == "GOV"), nrow(da), 1)`%).
The respective FAR for each sector---`r pct(nrow(filter(da, sector == "EDU", gender == "F")), sum(da$sector == "EDU"))`%,
`r pct(nrow(filter(da, sector == "COM", gender == "F")), sum(da$sector == "COM"))`%, and
`r pct(nrow(filter(da, sector == "GOV", gender == "F")), sum(da$sector == "GOV"))`%---show women to be significantly underrepresented in industry compared to academia
(`r report_test(chisq.test(table(da$sector, da$gender)[-3,-3]), show_df = T)`).
Other studies have also found relatively fewer women engineers in industry research positions [@fox06:engineering; @ghiasi15:compliance].

The distribution of work sectors among unique PC members appears similar, with
`r pct(sum(dp$sector == "EDU"), nrow(dp), 1)`% affiliated with academia,
`r pct(sum(dp$sector == "COM"), nrow(dp), 1)`% with industry, and
`r pct(sum(dp$sector == "GOV"), nrow(dp), 1)`% with government.
This similarity suggests that no sector is disproportionately favored in program committees.
FPR values continue to be higher than FAR values, but notably, not by the same magnitude across sectors.
For example, the FPR for academics
(`r pct(nrow(filter(dp, sector == "EDU", gender == "F")), sum(dp$sector == "EDU"))`%)
is higher than their FAR by some
`r round(100 * pct(nrow(filter(dp, sector == "EDU", gender == "F")), sum(dp$sector == "EDU")) / pct(nrow(filter(da, sector == "EDU", gender == "F")), sum(da$sector == "EDU")) - 100, 0)`%,
but for industry and government, FPR values are higher than FAR values by
`r round(100 * pct(nrow(filter(dp, sector == "COM", gender == "F")), sum(dp$sector == "COM")) / pct(nrow(filter(da, sector == "COM", gender == "F")), sum(da$sector == "COM")) - 100, 0)`% and
`r round(100 * pct(nrow(filter(dp, sector == "GOV", gender == "F")), sum(dp$sector == "GOV")) / pct(nrow(filter(da, sector == "GOV", gender == "F")), sum(da$sector == "GOV")) - 100, 0)`%, respectively.
It is possible that conference chairs may be more intentional about balancing gender diversity in the two sectors that already show low representation.
But it is unclear whether this actually hurts women's retention in the field, since the evaluation of job performance in industry may be less favorable for academic service tasks, so overburdening industry women without proper recognition could be hurting their future representation further.

### Geographical factors {-}

```{r country-rep, echo=F, message=F, warning=F, cache=T}
# Helper function: for a given country code and population, compute the female ration of all the persons affiliated with the country
# or of all the persons presenting in the country (conference taking place in country)
other_ppl <- demographics %>%
  drop_na(country) %>%
  group_by(country) %>%
  filter(!country %in% cntry$code)

all_others <- data.frame(code = "---",
                         authors = sum(other_ppl$as_author),
                         pcs = sum(other_ppl$as_pc),
                         a_by_affil = paste0(pct(sum(filter(other_ppl, gender == "F")$as_author),
                                                 sum(other_ppl$as_author)), "%"),
                         a_by_conf = NA,
                         pc_by_affil = paste0(pct(sum(filter(other_ppl, gender == "F")$as_pc),
                                                  sum(other_ppl$as_pc)), "%"),
                         pc_by_conf = NA,
                         total_hosted = nrow(sys_confs) - sum(cntry$total_hosted),
                         country = paste0("All ", country_count - nrow(cntry), " others"))

cntry %>%
  left_join(dplyr::select(countries, c("code", "country"))) %>%
#  mutate(country = paste0(country, " (", code, ")")) %>%
  rbind(all_others) %>%
  mutate(a_by_conf = ifelse(is.na(a_by_conf), "---", a_by_conf),
         pc_by_conf = ifelse(is.na(pc_by_conf), "---", pc_by_conf)) %>%
  dplyr::select(country, total_hosted, authors, a_by_affil, a_by_conf, pcs, pc_by_affil, pc_by_conf) %>%
  knitr::kable("latex", booktabs = T,
               align = c("lrrrrrrr"),
               caption = "Representation of women in the top 20 countries by author count. Shown for each country are: the number of the number of conferences it hosted; total authors affiliated with the contry; ratio of these authors that are women (FAR affiliated); ratio of female authors in local conferences (FAR hosted); total number of affiliated PC members, ratio of these that are women (FPR affiliated), and FPR in all locally hosted conferences. All counts include only persons whose email is unambigously affiliated with that country (with repeats). Women's ratios are compared to all other countries with a $\\chi^{2}$ test  (*$p<0.05$; **$p<0.01$; ***$p<0.001$).",
               col.names = NULL) %>%  #c("Country", "Total authors", "Female author ratio by affiliation country", "Female author ratio by conference country", "PC members", "Female PC by affil", "Female PC by conf")) %>%
#    column_spec(column = 2:4, width = "2cm") %>%
  add_header_above(c("Country"=1, "Conferences", "Authors", "FAR affiliated", "FAR hosted", "PCs", "FPR affiliated", "FPR hosted")) %>%
  #   add_header_above(c("Country" = 2, "Total nauthors" = 2, "Female author ratio by affiliation country" = 2, "Female author ratio by conference country" = 2, "PC members" = 2, "Female PC by affil" = 2, "Female PC by conf" = 2)) %>%
  kable_styling(font_size = 8, latex_options = "hold_position")
```

When it comes to geography, gender differences are much larger than experience or sector differences.
Researchers in our dataset hail from `r country_count` different countries that show distinct differences in researcher count and female representation (Table \@ref(tab:country-rep)).
Most of the top countries by author count appear to be more economically developed than the rest, perhaps because systems research can be capital-intensive, requiring state-of-the-art computing equipment.
Female author ratio, however, does not show the same association with a country's economic development, as exemplified by the low FAR of the UK, Singapore, South Korea, Netherlands, and Japan.
This result is consistent with larger gender studies as well [@elsevier20:journey; @holman18:gender; @lariviere13:bibliometrics].
Similarly, FAR does not appear to be strongly associated with a country's gender gap index [@charles06:degrees; @stoet18:gender; @world17:global].

FAR is also not strongly correlated with a country's number of authors
(`r report_test(cor.test(cntry$authors, parse_number(cntry$a_by_affil)))`).
The correlation is even weaker if we omit the US, which comprises most authors
(`r pct(nrow(filter(demographics, country == "US", role == "author")), nrow(filter(demographics, !is.na(country), role == "author")), 2)`%)
and PC members
(`r pct(nrow(filter(demographics, country == "US", role == "pc")), nrow(filter(demographics, !is.na(country), role == "pc")), 2)`%)
for which we have country and gender information.
US-based authors also exhibit higher FAR compared to the rest of the world
(`r us_a <- filter(repeat_authors, country == "US", !is.na(gender)); pct(sum(us_a$gender == "F"), nrow(us_a), 2)`% vs.
`r not_us_a <- filter(repeat_authors, country != "US", !is.na(gender)); pct(sum(not_us_a$gender == "F"), nrow(not_us_a), 2)`%,
`r report_test(chisq.test(data.frame(c(sum(us_a$gender == "F"), sum(us_a$gender == "M")), c(sum(not_us_a$gender == "F"), sum(not_us_a$gender == "M")))), 3, show_df = T)`).
About half of the total US-based CS researchers (and in our data) are likely foreign-born [@frachtenberg20:survey; @national20:science], but this distinction does not appear to explain differences in the gender gap [@frachtenberg20:survey; @goyette99:intersection; @hango13:gender; @tong10:place].

One hypothesis for the higher FAR in the US is that as the host of most systems conferences, the US might be more appealing to researchers who prefer domestic travel, such as parents of young children.
In conferences in all countries except South Korea and Italy, we found significantly higher representation of local-affiliated authors.
<!-- `r a_with_c<-repeat_authors %>% drop_na(country) %>% left_join(sys_confs, by = c("conf" = "key")); code_ <- "JP"; pct(nrow(filter(a_with_c, country.x == code_)), nrow(a_with_c)); pct(nrow(filter(a_with_c, country.x == code_, country.y == code_)), nrow(filter(a_with_c, country.y == code_)))` -->
However, we found no evidence of a gender difference in this preference---not in the US, where there are actually fewer women in US-hosted conferences---and not more generally, where the correlation between a country's FAR by affiliation and by hosted conference is nonexistent
(`r report_test(cor.test(parse_number(cntry$a_by_conf), parse_number(cntry$a_by_affil)))`).

<!---We can reexamine the pair of conferences SC and ISC as an example.
Both cover the same topic (high-performance computing) and take place regularly in the US in the fall and Germany in the summer, respectively.-->

The number of authors affiliated with a country is highly correlated with the number of local PC members
(`r report_test(cor.test(cntry$authors, cntry$pcs))`),
which also implies that most PC members hail from the West.
Note, however, that Western reviewers are not significantly overrepresented compared to authors, as has been observed in journals in other fields [@publons18:peer].

For PC members, the gender-gap differences across countries are even higher than for authors, with women representing
`r us_p <- filter(repeat_pc, country == "US", !is.na(gender)); pct(sum(us_p$gender == "F"), nrow(us_p), 2)`%
of US-based PC members, compared to
`r not_us_p <- filter(repeat_pc, country != "US", !is.na(gender)); pct(sum(not_us_p$gender == "F"), nrow(not_us_p), 2)`%
in the rest of the world
(`r report_test(chisq.test(data.frame(c(sum(us_p$gender == "F"), sum(us_p$gender == "M")), c(sum(not_us_p$gender == "F"), sum(not_us_p$gender == "M")))), 3, show_df = T)`).
Again, the fact that the US attracts many foreign scientists does not appear to explain the higher FPR in the US, since most of the foreign-born authors appear to be students [@frachtenberg20:survey], who are less likely to serve on PCs.
With few exceptions, most countries exhibit significantly higher FPR than FAR, as in the overall statistics.
Moreover, except for the US and Spain, all countries exhibit an even higher FPR for hosted conferences, unlike FAR.
It is also worth noting that for researchers with unknown country affiliation, both FAR and FPR are very similar to the overall statistics, which suggests that any selection bias based on availability of country and gender information is limited.

<!----------------------------------------------------------------------------------------------------->

## Linear model of gender


```{r me-model, echo = F, warning = F, message = F}
all_features <- sys_roles %>%
  filter(role == "author") %>%
  left_join(sys_authors) %>%
  drop_na(gender) %>%
  mutate(conf = gsub("_\\d+", "", conf)) %>%
  left_join(conf_gender, by = c("conf" = "conf")) %>%
  select(c(conf, npapers, double_blind, PC, session, keynote, panel, chair, diversity_effort, acceptance_rate, h5_index, h5_median, submissions, age, past_papers, mean_pages, past_citations, mean_historical_citations, authors_num, mean_authors_per_paper, mean_review_load, pc_paper_ratio, pc_author_ratio, open_access, is_org_IEEE, is_org_ACM, is_org_USENIX, hindex, sector, as_author, gender))

rescale01 <- function(x) {
  rng <- range(x, na.rm = TRUE)
  (x - rng[1]) / (rng[2] - rng[1])
}

scaled_af = mutate_if(all_features, is.numeric, rescale01)

modelLMER <- glmer(data = scaled_af, family = "binomial", gender ~ is_org_ACM + is_org_IEEE + is_org_USENIX + mean_pages + submissions + double_blind + open_access + diversity_effort + age + past_papers + past_citations + mean_historical_citations + h5_index + h5_median + pc_author_ratio + npapers + authors_num + mean_authors_per_paper + acceptance_rate + pc_paper_ratio + mean_review_load + sector + hindex + as_author + (1 | conf))
```


To round up our exploratory data analysis analysis, we built a logistic-regression mixed-effects model to surface the factors most strongly associated with gender.
The model combines the 27 conference-related factors and 3 author factors (work sector, h-index, and number of papers in this set) as predictor variables.
Each data point comprises one author and accepted paper pair, with the author's gender as the outcome variable.
All of the predictors were treated as fixed effects, and each numeric predictor was scaled to the range 0--1.
Because many of these factors may be correlated or confounded by conference, the model also included the conference name for each paper as a random effect.

This model, like the one predicting FAR from conference factors alone, is not very predictive
(AIC: `r round(AIC(modelLMER), 1)`; BIC: `r round(BIC(modelLMER), 1)`; theoretical conditional $R^{2}$: `r round(MuMIn::r.squaredGLMM(modelLMER)["theoretical",][2], 2)`).
Most of the factors have negligible impact or significance on the author's gender.
This null result reaffirms that the underrepresentation of women does not appear to stem from a particular conference, policy, or author demographic.

The most significant predictive factor for an author being male turns out to be how many overall papers they have published in this set of conferences during 2017
($p=`r round(coef(summary(modelLMER))["as_author",][4], 3)`$).
This observation is not particularly insightful because the distribution of published papers skews heavily male on the right tail.
In other words, since most of the prolific outliers were men, they produced an outsize effect on the linear model.

The ratio of papers with a PC member author in a conference is also linked with a higher likelihood of an author being female
($p=`r round(coef(summary(modelLMER))["pc_paper_ratio",][4], 3)`$).
Since conference FPR values are higher that FAR values, it follows that more papers from the PC would be associated with more female authors.
The only other factor with $p<.05$ is for conferences organized by USENIX, where men published at a slightly higher rate than other conferences, but this correlation is not likely to be causal.

<!----------------------------------------------------------------------------------------------------->
<!----------------------------------------------------------------------------------------------------->


# Related work {#sec:related}

A number of prior studies have analyzed the representation of women in various academic fields, including CS.
Fewer studies have looked at specific fields of CS, and in particular, the large and influential field of computer systems.
Here, we review recent studies and compare their data sources, metrics, methodologies, and findings to our own.
We also briefly discuss some possible explanations of this gender gap that have been proposed in the literature for CS and as a whole, and frame them in the context of computer systems.

One of the most expansive studies of gender representation in CS authorship was recently published by Wang et al. [@wang21:trends].
It examined Semantic Scholar authorship data from the 1940s to 2019, and looked at 151M publications, including 11.8M in CS alone.
This study used the Gender API tool to infer genders from given names, omitting any rare or initialed names.
Instead of assigning binary genders, however, the authors derived a gender probability distribution for each name from the accuracy estimates returned by Gender API.
In the 2017 timeframe, FAR in overall CS was around 25%, significantly higher than FAR for systems alone.

A similarly large study looked at all CS submissions on arxiv as of 2016 [@holman18:gender].
For gender assignment, it also used a name-inference service (genderize.io), simply omitting all names where the predicted accuracy was less than 95%.
It computed overall FAR as $\approx{17\%}$, and slightly higher for first authors, agreeing with our observation.
It should be noted, however, that arxiv is a preprint server and these documents do not match exactly the peer-reviewed papers analyzed in most studies, including ours.

A more sophisticated gender inference approach was taken by Mattauch et al., which aimed for higher accuracy by using machine learning algorithms to also infer the cultural context of each name.
Like with the other inference methods, gender could not be accurately inferred for Asian names, so over 20% of the author names were omitted in this study.
Using this approach, the study estimated FAR for 18 CS conferences in the preceding six years, including six of our conferences: ASPLOS, EuroPar, EuroSys, SOSP, ATC, and VEE.
For all but one of these conferences (VEE), the estimated FAR values were within 2% points of the ones we found, which suggests that these values have been fairly stable in recent years.

Another study exploring some of our conferences, but earlier in time (1966--2009), was conducted by Cohoon et al. [@cohoon11:cspapers].
Generally, the FAR values they computed, even for the same conferences, tend to be higher than those we computed, with an overall CS number of $\approx{25\%}$ by 2007.
The discrepancy could be partially explained by the different time periods under observation, although we doubt that a decade would lead to significantly decreased representation of women, based on the trends exhibited in the other studies.
We do note, however, that Cohoon's study used a very different gender-assignment methodology, which could explain most of the difference.
For 70% of papers, it used the same name-inference technique as the previous two studies using genderizer.io.
For the others, it used a statistical approach that assigned a gender of female to authors with ambiguous genders with a probability of 40%--45%.
Based on our experience with inferred and looked-up genders for both systems and non-systems papers, we believe this probability tends to overestimate the actual ratio of women.

In contrast, Way et al. used a hand-curated dataset in their study of tenure-track faculty [@way16:gender].
Their analysis used a list of 5032 tenure-track faculty from 205 CS academic institutes in the US and Canada, and found only about 15% of CS faculty were women.
Note, however, that the study was limited to North America and excluded students, which in our dataset comprised over one third of the authors [@frachtenberg20:survey].

A good source of data on students in our timeframe comes from the Taulbee report [@zweben18:taulbee], which found the ratio of women among fresh CS PhD awardees in 2017 to be about 18%.
Notably, in the discipline of computer engineering---which is perhaps closer in research topics to computer systems---the ratio was only about 11%.

Another complementary statistic also comes from the US-based National Science Board, which recently found women to represent just under 30% of the overall CS workforce [@national20:science].
This estimate is not limited to CS researchers, and in particular, authors, as in most of these studies.

Most of these sources point to a significantly worse gap in systems than the rest of CS.
From the FAR statistics alone it is not immediately clear why this should be the case, but we can look at some of the expansive literature on the gender gap for clues.
Many causes for women's underrepresentation in science and technology have been posited, and we briefly describe a few of these next, in the specific context of our data for systems.

One important factor that was associated with gender differences in publication rate and impact was the possible role of resource requirements [@duch12:possible; @head13:funding].
Many of the subfields of computer systems, such as high-performance computing, do indeed require expensive experimental platforms, which may partially explain their gender gap [@frachtenberg21:whpc; @frantzana19:women].
But high resource requirement cannot fully explain lower FAR metrics, as evident in the data on CS theory conferences we collected.
The lack of association between a country's FAR and its economic development also weakens this explanation for systems as a whole.

Another important source of women's recruitment and retention in a field is the availability of female role models [@bettinger05:faculty; @drury11:female; @herrmann16:effects].
The relative dearth of women in last author position that we observed in systems conferences may therefore have a contributing factor to lower FAR as well.
Recall that our collection of systems papers averages
`r round(mean(sys_papers$nauthors), 2)`
coauthors per paper, which is some 50% higher than the mean $\approx{3.0}$ authors per paper that Wang et al. found in contemporary CS publications [@wang21:trends].
We hypothesize that this difference stems from the large emphasis on systems implementation in this field, requiring larger team efforts.

The difference in collaboration may also offer clues to the larger gender gap in computer systems.
In past studies, women's collaborative research networks have been found to be smaller than mens [@fox06:engineering; @whittington09:networks].
The overall lack of female peers and mentors in systems can make collaboration even harder for women [@abbate12:recoding], leading to fewer or smaller collaborations, which would consequently lower their research output in systems.

Finally, we must take into account that different fields attract women at different rates.
For example, a number of studies posited that women are more likely to work in human-centered fields [@diekman13:navigating; @fisher02:unlocking; @sax19:disciplinary].
The higher FAR we observed in human-computer interaction and CS education appear to confirm this observation for CS fields.
Systems in particular is perhaps most related to the field of electrical engineering.
This field has also historically fared poorly in terms of women's underrepresentation, and exhibits FAR values hovering on 10%, similar to the one we observed for systems [@fox06:engineering; @nelson03:national].

<!----------------------------------------------------------------------------------------------------->
<!----------------------------------------------------------------------------------------------------->


# Conclusion {#sec:conclusion}

This study presents a methodology and dataset to estimate the current percentage of women in systems research.
Unlike most comparable studies that use gender-inference based on names with limited accuracy and coverage, our hand-curated dataset includes genders for nearly all the researchers participating in these conferences, leading to more precise estimates.

Our main finding is that only $\approx{10\%}$ of systems authors are women, a ratio that is significantly lower than the rest of CS.
The percentage of women who serve on PCs is almost twice as high, but the evidence suggests that it is relatively inflated, and not representative of systems as a whole.

The large gender gap is not associated with almost any of the explanatory factors evaluated.
Importantly, variations in female author ratio cannot be explained by multiple conference factors, including policies that are explicitly designed to improve diversity.
These variations are also not fully explained by demographic differences such as research experience or work sector.
The data show larger gender-gap variations by country of affiliation, but these appear unrelated to geographical region, economic development, or gender gap index.
The lack of significant correlations or strongly predictive factors in the linear models suggests that the low representation of women in computer system is endemic to the field, rather than an effect of conference factors or author demographics.


Inviting more women to visible conference roles and implementing diversity-focused policies likely contributes to more inclusive conferences, but is insufficient on its own to add women authors to the field.
Increasing women's participation in systems research will require addressing the systemic causes of their exclusion, which are even more pronounced in this field than in the rest of CS.
The underrepresentation of women in the field may be related to factors such as high resource requirements, fewer female role models and collaboration opportunities, and different gender preferences.
But these factors alone do not completely explain this complex, multifaceted phenomenon.
Identifying the specific, endemic causes for this larger gender gap remains an open research question.


### Acknowledgements {-}
We thank Betsy Bizot, Brooke Cowan, Natalie Enright Jerger, Kathryn McKinley, Heather Metcalf, Anna Ritz, Aspen Russel, Kelly Shaw and Jonathan Wells for their insightful comments on earlier drafts. We also thank Josh Reiss, Alex Richter, and Josh Yamamoto for their assistance with gender data gathering, and Reed College Social Justice Research and Education Fund for their support.

# References {-}
